[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805 - Machine Learning I",
    "section": "",
    "text": "Hello, I’m Daniel—a dedicated educator with a deep enthusiasm for sharing the intricacies of data science. My professional journey revolves around not just teaching these skills but also championing inclusive pedagogical approaches. I thrive on continuous learning and take pride in implementing cutting-edge methods that cater to diverse learning styles. Beyond the classroom, I actively engage with communities that share my passion, creating a collaborative space for growth and exploration. Join me on this exciting journey of knowledge, innovation, and community building!\nOutside of work, I’m an avid hiker, novice (but passionate) meal-maker, and I really love the color green. Poke around my website to learn more!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\nlets run a for loop\n\nfor i in range(1,10):\n  print(i)\n\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/third Post/index.html",
    "href": "posts/third Post/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/First Blog Post/index.html",
    "href": "posts/First Blog Post/index.html",
    "title": "Regression (Linear and Nonlinear)",
    "section": "",
    "text": "Regression is a statistical technique in machine learning and statistics that aims to establish a relationship between a dependent variable (target) and one or more independent variables (features or predictors). The primary goal of regression analysis is to model the relationship between these variables, enabling the prediction or estimation of the dependent variable based on the values of the independent variables.\nIn simpler terms, regression helps us understand how the changes in one or more variables are associated with changes in another variable. It is widely used for prediction and forecasting, making it a fundamental tool in various fields, including finance, economics, biology, and social sciences.\nThere are several types of regression models, with linear regression being one of the most common. In linear regression, the relationship between the variables is modeled as a linear equation, representing a straight line on a graph. However, regression is not limited to linear relationships; non-linear regression models can capture more complex patterns, accommodating scenarios where the relationship between variables is curved or follows a different pattern.\nThe training process in regression involves fitting the model to historical data, allowing the algorithm to learn the underlying patterns. Once trained, the regression model can be used to make predictions on new, unseen data, providing valuable insights and aiding decision-making processes.\nIn this post, we will consider the implementation of linear and nonlinear regression for predicting house prices based on size"
  },
  {
    "objectID": "posts/Second Blog Post/index.html",
    "href": "posts/Second Blog Post/index.html",
    "title": "Classification",
    "section": "",
    "text": "In machine learning, classification is a type of supervised learning where the algorithm is trained to categorize input data into predefined classes or categories. The goal is to learn a mapping between the input features and the corresponding class labels based on a set of labeled training data. Essentially, the algorithm learns to generalize from the provided examples and then applies this knowledge to classify new, unseen instances.\nThe process involves training a model on a labeled dataset, where each data point has input features and an associated class label. The model learns to recognize patterns and relationships within the input data that are indicative of the different classes. Once trained, the model can predict the class labels for new, unseen data.\nSome types of Classification challenges are :\nThere are various classification algorithms, each with its strengths and weaknesses, suited for different types of data and problem domains. Common algorithms include:\nThe choice of algorithm often depends on factors such as the nature of the data, the size of the dataset, and the desired interpretability of the model.\nWe will go over them one by one."
  },
  {
    "objectID": "posts/Third Blog Post/index.html",
    "href": "posts/Third Blog Post/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a type of unsupervised machine learning technique where the goal is to group similar data points together based on certain characteristics or features. The objective is to identify natural patterns or structures within the data without the need for predefined labels.\nIn a clustering algorithm, the algorithm tries to partition the dataset into groups, or clusters, where data points within the same cluster are more similar to each other than to those in other clusters. The idea is that data points in the same cluster share some underlying patterns or properties, and these clusters can provide insights into the inherent structure of the data.\nThere are various clustering algorithms, each with its own approach to defining what constitutes a “similar” data point and how to form clusters. Some popular clustering algorithms include K-means clustering, hierarchical clustering, and DBSCAN (Density-Based Spatial Clustering of Applications with Noise).\nHere’s a brief overview of a couple of commonly used clustering algorithms:\nOther clustering algorithms are: density-based, distribution-based, centroid-based and hierarchical-based clustering.\nClustering is used in various applications, including customer segmentation, image segmentation, anomaly detection, and more. It is particularly valuable when the structure of the data is not well-defined or when there is no labeled training data available for supervised learning."
  },
  {
    "objectID": "posts/First Blog Post/index.html#simple-linear-regression",
    "href": "posts/First Blog Post/index.html#simple-linear-regression",
    "title": "Regression (Linear and Nonlinear)",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nSimple Linear Regression is a fundamental and straightforward form of regression analysis where the relationship between two variables is modeled using a linear equation. In this case, there are two variables: one is considered the independent variable (often denoted as \\(X\\)), and the other is the dependent variable (often denoted as \\(Y\\)).\n\nModel Representation\nThe equation for simple linear regression is represented as:\n\\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\]\nThe \\(\\beta_1\\) is called a scale factor or coefficient and \\(\\beta_0\\) is called bias coefficient. The bias coefficient gives an extra degree of freedom to this model and \\(\\epsilon\\) is the error term accounting for unobserved factors that affect Y but are not accounted for by the model\nThis equation is similar to the line equation \\(y = mx +b\\) with \\(m = \\beta_1\\)(Slope) and \\(b = \\beta_0\\)(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.\nBut how do we find these coefficients? That’s the learning procedure. We can find these using different approaches. One is called Ordinary Least Square Method and other one is called Gradient Descent Approach.\n\n\nOrdinary Least Square Method\nThe Ordinary Least Squares (OLS) method is a common approach used in linear regression to estimate the parameters of the linear equation by minimizing the sum of the squared differences between the observed and predicted values of the dependent variable. In simple terms, OLS aims to find the best-fitting line through the data points.\nLet’s say we have few inputs and outputs plotted in a 2D space with a scatter plot to yield the following image:\n\n\n\n\n\nFor a simple linear regression model with the equation given above, the OLS method seeks to find the values \\(\\beta_0\\), \\(\\beta_1\\) of the linear model that minimize the sum of squared residuals.\nA good model will always have least error and we can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.\n\n\n\n\n\nAnd total error of this model is the sum of all errors of each point. ie.\n\\[\nD = \\sum_{i=1}^{m} d_i^2\n\\]\n\\(d_i\\) - Distance between line and ith point.\n\\(m\\) - Total number of points\nYou may have observed that we are taking the square of each distance. This is done because certain points lie above the line while others lie below it. By minimizing D, we aim to reduce the error in the model.\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n\\]\n\\[\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n\\]\nIn these equations \\(\\bar{x}\\) is the mean value of input variable \\(x\\) and \\(\\bar{y}\\) is the mean value of output variable \\(y\\)\nNow we have the Ordinary Least Square Method which is described with the following equations\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n\\]\n\\[\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n\\]\n\n\nImplementation\nWe are going to use a dataset containing the price of houses as a function of size to implement regression. This data is split into 3 for training, validation and finally testing. Let’s start off by importing and viewing the data.\n\n#Import the needed libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\n\nwith open('assignment1.json', 'r') as json_file:\n    data = json.load(json_file)\n    \nX_train = np.array(data['X_train']).reshape(-1, 1)\ny_train = np.array(data['Y_train']).reshape(-1, 1)\nX_val = np.array(data['X_val']).reshape(-1, 1)\ny_val = np.array(data['Y_val']).reshape(-1, 1)\nX_test = np.array(data['X_test']).reshape(-1, 1)\ny_test = np.array(data['Y_test']).reshape(-1, 1)\n\n\narray1 = data['X_train']\narray2 = data['Y_train']\n\narray3 = data['X_val']\narray4 = data['Y_val']\n\narray5 = data['X_test']\narray6 = data['Y_test']\n\n\n\nprint(\"Training Data\")\ntableData = {\n  'X_train': array1,\n  'Y_train': array2,\n}\ntable = pd.DataFrame(tableData)\nprint(table.head(3))\nprint(f\"number of rows and colums: {table.shape}\")\nprint()\n\nprint(\"Validation Data\")\ntableData2 = {\n  'X_Val': array3,\n  'Y_Val': array4\n}\ntable2 = pd.DataFrame(tableData2)\nprint(table2.head(3))\nprint(f\"number of rows and colums: {table2.shape}\")\nprint()\n\nprint(\"Test Data\")\ntableData3 = {\n  'X_test': array5,\n  'Y_test': array6\n}\ntable3 = pd.DataFrame(tableData3)\nprint(table3.head(3))\nprint(f\"number of rows and colums: {table3.shape}\")\n\nTraining Data\n   X_train  Y_train\n0    661.5     4.25\n1    465.0     2.30\n2   1442.0    68.00\nnumber of rows and colums: (12, 2)\n\nValidation Data\n    X_Val   Y_Val\n0   650.0   4.170\n1   682.5   4.067\n2  1417.0  31.870\nnumber of rows and colums: (21, 2)\n\nTest Data\n   X_test  Y_test\n0   405.0   3.316\n1   330.0   5.400\n2   135.0   1.300\nnumber of rows and colums: (21, 2)\n\n\nAs we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.\nwe need to implement feature scaling.\nFeature scaling is a preprocessing step in machine learning that involves adjusting the scale of the input features to a similar range. The goal is to ensure that all features contribute equally to the model training process, preventing certain features from dominating due to their larger scales.\n\n# Step 2: Preprocess the data (feature scaling)\nmean = np.mean(X_train)\nstd = np.std(X_train)\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\nNext, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first\n\nplt.scatter(array1, array2, label='training data')\nplt.scatter(array3, array4, label='validation data')\nplt.scatter(array5, array6, label='testing data')\n\nplt.ylabel('House Price')\nplt.xlabel('House size')\nplt.legend()\nplt.title('Training, Validation and Testing Data')\n\nText(0.5, 1.0, 'Training, Validation and Testing Data')\n\n\n\n\n\nLets create a function to implement linear regression with L2 regularization.\nLinear regression with regularization is an extension of traditional linear regression that incorporates regularization techniques to prevent overfitting and improve the model’s generalization performance. The two common types of regularization used in linear regression are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization).\n\\[\nJ(\\theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\theta_j^2)\n\\]\n\n# Step 3: Implement regularized linear regression with gradient descent\ndef ridge_regression(X, y, alpha, num_iterations, learning_rate):\n    m, n = X.shape\n    # Initialize theta with the correct shape\n    theta = np.zeros((n, 1))\n    history = []\n\n    for _ in range(num_iterations):\n        gradient = (X.T @ (X @ theta - y) + alpha * theta) / m\n        theta -= learning_rate * gradient\n        cost = np.sum((X @ theta - y) ** 2) / (2 * m) + (alpha / (2 * m)) * np.sum(theta[1:] ** 2)\n        history.append(cost)\n\n    return theta, history, cost\n\nIn the equation above, \\(\\lambda\\) is the regularization parameter which ensures a balance between the trade-off between fitting the training data well and keeping the model simple. This is implemented with the gradient descent algorithm.\n\n\nGradient Descent\nGradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n\nStep 1\nInitialize values \\(\\theta_0, \\theta_1, ..., \\theta_n\\) with some value. In this case we will initialize with 0.\n\n\nStep 2\nIteratively update,\n\\[\n\\theta_j : \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n\\]\nuntil it converges, where:\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \\ j = 0\n\\]\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j \\ j\\ge 1\n\\]\nThis is the procedure. Here \\(\\alpha\\) is the learning rate. This operation \\(\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\\) means we are finding partial derivative of cost with respect to each \\(\\theta_j\\). This is called Gradient.\nIn step 2 we are changing the values of \\(\\theta_j\\). in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don’t want to change values of \\(\\theta_j\\). drastically, because we might miss the minima. That’s why we need learning rate.\n\n\n\nAnimation illustrating the gradient descent method\n\n\nThe above animation illustrates the Gradient Descent method.\nAfter making substitutions, Step 2 becomes:\n\\[\n\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j\n\\]\nWe iteratively change values of \\(\\theta_j\\) according to above equation. This particular method is called Batch Gradient Descent.\nThen we need to implement a function to plot the line of best fit\n\ndef plot_data_and_fit(X_train, y_train, theta):\n    plt.scatter(X_train, y_train, color='blue', label='Training data')\n    \n    x_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\n    x_values_extended = np.column_stack((np.ones_like(x_values), x_values))\n    y_values = x_values_extended @ theta\n    \n    plt.plot(x_values, y_values, color='red', label='Line of best fit')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Training Data and Line of Best Fit')\n    plt.legend()\n    plt.show()\n\nNext we will train the model using the training data and return the cost using the equations below:\n\n# Step 4: Train the model using the training data\nalpha = 0 # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\nX_train_extended = np.column_stack((np.ones_like(X_train), X_train))\ntheta, history, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\nprint(f\"the cost iss: {cost}\")\n\nthe cost iss: 86.7594520168758\n\n\nThen tuning the regularization parameter\n\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    X_val_extended = np.column_stack((np.ones_like(X_val), X_val))\n    theta_val, history_, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_extended @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n\nBest regularization parameter (alpha): 10\n\n\nEvaluating the Model on the Testing data using the root mean square of errors.\nRoot Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,\n\\[\nRMSE = \\sqrt{\\sum_{i=1}^m \\frac{1}{m}(\\hat{y_1}-y_i)^2)}\n\\]\nHere \\(\\hat{y_i}\\) is the \\(i^{th}\\) predicted output values.\n\n# Step 6: Evaluate the model using the testing data\nX_test_extended = np.column_stack((np.ones_like(X_test), X_test))\ntheta_test, _, cost = ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_extended @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\nprint(cost)\n\nMean Squared Error on Test Data: 81.66385694629241\n234.9787954527312\n\n\n\n# Plot the cost history during training\nplt.plot(range(num_iterations), history)\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\nplt.title('Cost vs. Iteration')\nplt.show()\n\n\n\n\nPlotting the training data and line of best fit\n\n# Plot training data and line of best fit\nplot_data_and_fit(X_train, y_train, theta)\nprint(theta)\n\n\n\n\n[[22.10833333]\n [20.73627522]]\n\n\n\\[\nHouse Price = 22.108 + 20.736 \\times House size\n\\]"
  },
  {
    "objectID": "posts/First Blog Post/index.html#multiple-linear-regression",
    "href": "posts/First Blog Post/index.html#multiple-linear-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nMultiple Linear Regression is a type of Linear Regression when the input has multiple features(variables).\n\nModel Representation\nSimilar to Simple Linear Regression, we have input variable(X) and output variable(Y). But the input variable has � features. Therefore, we can represent this linear model as follows;\n\\[\nY = \\beta_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\n\\]\n\\[\nY = \\beta_0x_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\n\\]\n\\[\nx_0 = 1\n\\]\nNow we can convert this eqaution to matrix form.\n\\[\nY = \\beta^TX\n\\]\nWhere,\n\\[\n\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\.\\\\.\\\\\\beta_n\\end{bmatrix}\n\\]\nand,\n\\[\nX = \\begin{bmatrix}x_0\\\\x_1\\\\x_2\\\\.\\\\.\\\\x_n\\end{bmatrix}\n\\]\nWe have to define the cost of the model. Cost bascially gives the error in our model. Y in above equation is the our hypothesis(approximation). We are going to define it as our hypothesis function.\n\\[\nh_\\beta(x) = \\beta^Tx\n\\]\nand the cost is,\n\\[\nJ(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\beta(x^{\\textrm{(i)}}) - y^{\\textrm{(i)}})^2\n\\]\nBy minimizing this cost function, we can get find �. We use Gradient Descent for this.\n\n\nGradient Descent\nGradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n\nStep 1\nInitialize values �0, �1,..., �� with some value. In this case we will initialize with 0.\n\n\nStep 2\nIteratively update,\n\\[\n\\beta_j := \\beta_j - \\alpha\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\n\\]\nuntil it converges.\nThis is the procedure. Here � is the learning rate. This operation \\(\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\\) means we are finding partial derivate of cost with respect to each ��. This is called Gradient.\nRead this if you are unfamiliar with partial derivatives.\nIn step 2 we are changing the values of �� in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don’t want to change values of �� drastically, because we might miss the minima. That’s why we need learning rate.\n\nThe above animation illustrates the Gradient Descent method.\nBut we still didn’t find the value of �����(�). After we applying the mathematics. The step 2 becomes.\n\\[\n\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}\n\\]\nWe iteratively change values of �� according to above equation. This particular method is called Batch Gradient Descent.\n\n\n\nImplementation\nLet’s try to implement this in Python. This looks like a long procedure. But the implementation is comparitively easy since we will vectorize all the equations. If you are unfamiliar with vectorization, read this post\nWe will be using a student score dataset. In this particular dataset, we have math, reading and writing exam scores of 1000 students. We will try to find a predict the score of writing exam from math and reading scores. You can get this dataset from this Github Repo. That’s we have 2 features(input variables). Let’s start by importing our dataset.\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndata = pd.read_csv('student.csv')\nprint(data.shape)\ndata.head()\n\n(1000, 3)\n\n\n\n\n\n\n\n\n\nMath\nReading\nWriting\n\n\n\n\n0\n48\n68\n63\n\n\n1\n62\n81\n72\n\n\n2\n79\n80\n78\n\n\n3\n76\n83\n79\n\n\n4\n59\n64\n62\n\n\n\n\n\n\n\nWe will get scores to an array.\n\nmath = data['Math'].values\nread = data['Reading'].values\nwrite = data['Writing'].values\n\n# Ploting the scores as scatter plot\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(math, read, write, color='#ef1234')\nplt.show()\n\n&lt;Figure size 1920x960 with 0 Axes&gt;\n\n\nNow we will generate our X, Y and \\(\\beta\\)\n\nm = len(math)\nx0 = np.ones(m)\nX = np.array([x0, math, read]).T\n# Initial Coefficients\nB = np.array([0, 0, 0])\nY = np.array(write)\nalpha = 0.0001\n\nWe’ll define our cost function.\n\ndef cost_function(X, Y, B):\n    m = len(Y)\n    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n    return J\n\n\ninital_cost = cost_function(X, Y, B)\nprint(inital_cost)\n\n2470.11\n\n\nAs you can see our initial cost is huge. Now we’ll reduce our cost prediocally using Gradient Descent.\nHypothesis: \\(h_\\beta(x) = \\beta^Tx\\)\nLoss: \\((h_\\beta(x)-y)\\)\nGradient: \\((h_\\beta(x)-y)x_{j}\\)\nGradient Descent Updation: \\(\\beta_j := \\beta_j - \\alpha(h_\\beta(x)-y)x_{j})\\)\n\ndef gradient_descent(X, Y, B, alpha, iterations):\n    cost_history = [0] * iterations\n    m = len(Y)\n    \n    for iteration in range(iterations):\n        # Hypothesis Values\n        h = X.dot(B)\n        # Difference b/w Hypothesis and Actual Y\n        loss = h - Y\n        # Gradient Calculation\n        gradient = X.T.dot(loss) / m\n        # Changing Values of B using Gradient\n        B = B - alpha * gradient\n        # New Cost Value\n        cost = cost_function(X, Y, B)\n        cost_history[iteration] = cost\n        \n    return B, cost_history\n\nNow we will compute final value of \\(\\beta\\)\n\n# 100000 Iterations\nnewB, cost_history = gradient_descent(X, Y, B, alpha, 100000)\n\n# New Values of B\nprint(newB)\n\n# Final Cost of new B\nprint(cost_history[-1])\n\n[-0.47889172  0.09137252  0.90144884]\n10.475123473539167\n\n\nWe can say that in this model,\n\\[\nS_{writing} = -0.47889172 + 0.09137252 * S_{math} + 0.90144884 * S_{reading}\n\\]\nThere we have final hypothesis function of our model. Let’s calculate RMSE and �2 Score of our model to evaluate.\n\n# Model Evaluation - RMSE\ndef rmse(Y, Y_pred):\n    rmse = np.sqrt(sum((Y - Y_pred) ** 2) / len(Y))\n    return rmse\n\n# Model Evaluation - R2 Score\ndef r2_score(Y, Y_pred):\n    mean_y = np.mean(Y)\n    ss_tot = sum((Y - mean_y) ** 2)\n    ss_res = sum((Y - Y_pred) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n    return r2\n\nY_pred = X.dot(newB)\n\nprint(rmse(Y, Y_pred))\nprint(r2_score(Y, Y_pred))\n\n4.577143972727788\n0.9097223273061554\n\n\nWe have very low value of RMSE score and a good �2 score. I guess our model was pretty good.\nNow we will implement this model using scikit-learn.\n\n\nThe scikit-learn Approach\nscikit-learn approach is very similar to Simple Linear Regression Model and simple too. Let’s implement this.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# X and Y\nX = np.array([math, read]).T\nY = np.array(write)\n\n# Model Intialization\nreg = LinearRegression()\n# Data Fitting\nreg = reg.fit(X, Y)\n# Y Prediction\nY_pred = reg.predict(X)\n\n# Model Evaluation\nrmse = np.sqrt(mean_squared_error(Y, Y_pred))\nr2 = reg.score(X, Y)\n\nprint(rmse)\nprint(r2)\n\n4.572887051836439\n0.9098901726717316\n\n\nYou can see that this model is better than one which we have built from scratch by a small margin.\nThat’s it for Linear Regression. I assume, so far you have understood Linear Regression, Ordinary Least Square Method and Gradient Descent."
  },
  {
    "objectID": "posts/Second Blog Post/index.html#binary-classification-for-machine-learning",
    "href": "posts/Second Blog Post/index.html#binary-classification-for-machine-learning",
    "title": "Classification",
    "section": "Binary Classification for Machine Learning",
    "text": "Binary Classification for Machine Learning\nA binary classification refers to those tasks which can give either of any two class labels as the output. Generally, one is considered as the normal state and the other is considered to be the abnormal state.  The following examples will help you to understand them better.\n\nEmail Spam detection: Normal State – Not Spam, Abnormal State – Spam\nConversion prediction: Normal State – Not churned, Abnormal State – Churn\nConversion Prediction: Normal State – Bought an item, Abnormal State – Not bought an item\n\nYou can also add the example of that ” No cancer detected” to be a normal state and ” Cancer detected” to be the abnormal state. The notation mostly followed is that the normal state gets assigned the value of 0 and the class with the abnormal state gets assigned the value of 1. For each example, one can also create a model which predicts the Bernoulli probability for the output. You can read more about the probability here. In short, it returns a discrete value that covers all cases and will give the output as either the outcome will have a value of 1 or 0. Hence after the association to two different states, the model can give an output for either of the values present.\nThe most popular algorithms which are used for binary classification are :\n\nK-Nearest Neighbours\nLogistic Regression\nSupport Vector Machine\nDecision Trees\nNaive Bayes\nNeural Networks\netc."
  },
  {
    "objectID": "posts/Third Blog Post/index.html#types-of-clustering-algorithms",
    "href": "posts/Third Blog Post/index.html#types-of-clustering-algorithms",
    "title": "Clustering",
    "section": "Types of clustering algorithms",
    "text": "Types of clustering algorithms\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\nDensity-based\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\nThe great thing about this is that the clusters can be any shape. You aren’t constrained to expected conditions.\nThe clustering algorithms under this type don’t try to assign outliers to clusters, so they get ignored.\n\n\nDistribution-based\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\nIf you aren’t sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\n\nCentroid-based\nCentroid-based clustering is the one you probably hear about the most. It’s a little sensitive to the initial parameters you give it, but it’s fast and efficient.\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\n\nHierarchical-based\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\nThis is more restrictive than the other clustering types, but it’s perfect for specific kinds of data sets.\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you’ll get invaluable insight on your data. You might find connections you never would have thought of.\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning."
  },
  {
    "objectID": "posts/Third Blog Post/index.html#k-means-clustering-algorithm",
    "href": "posts/Third Blog Post/index.html#k-means-clustering-algorithm",
    "title": "Clustering",
    "section": "K-means clustering algorithm",
    "text": "K-means clustering algorithm\nK-means clustering is the most commonly used clustering algorithm. It’s a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThis algorithm tries to minimize the variance of data points within a cluster. It’s also how most people are introduced to unsupervised machine learning.\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it’ll take more time to classify data points if there are a large amount of them in the data set.\nSince this is how k-means clusters data points, it doesn’t scale well.\n\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html",
    "href": "posts/Fourth Blog Post/index.html",
    "title": "Anomaly/Outlier Detection",
    "section": "",
    "text": "Anomaly detection is a process in data analysis and machine learning that involves identifying patterns, events, or data points that deviate significantly from the expected or normal behavior within a dataset. The goal is to identify anomalies or outliers that may indicate errors, fraud, security threats, or other unexpected events. Anomaly detection is used across various domains, including cybersecurity, finance, healthcare, industrial monitoring, and more.\nWhat is an anomaly?\nAn anomaly, in the context of data analysis or system behavior, refers to an observation or event that deviates significantly from what is considered normal, expected, or typical. Anomalies are also often referred to as outliers, novelties, or deviations. Detecting anomalies is crucial in various fields, including data analysis, cybersecurity, finance, and industrial systems, as anomalies may indicate errors, fraud, security threats, or other unexpected events.\nThere are two main types of anomalies:\nCommon reasons for anomalies are:\nAnomalies can be detected through various methods, including statistical techniques, machine learning algorithms, and rule-based systems. Some common approaches include:\nAnomaly detection is crucial for maintaining the integrity and security of systems, as anomalies may indicate issues that require investigation or intervention. It’s an important aspect of data analysis, monitoring, and maintenance in various industries."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#types-of-clustering-algorithms",
    "href": "posts/Fourth Blog Post/index.html#types-of-clustering-algorithms",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "Types of clustering algorithms",
    "text": "Types of clustering algorithms\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\nDensity-based\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\nThe great thing about this is that the clusters can be any shape. You aren’t constrained to expected conditions.\nThe clustering algorithms under this type don’t try to assign outliers to clusters, so they get ignored.\n\n\nDistribution-based\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\nIf you aren’t sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\n\nCentroid-based\nCentroid-based clustering is the one you probably hear about the most. It’s a little sensitive to the initial parameters you give it, but it’s fast and efficient.\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\n\nHierarchical-based\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\nThis is more restrictive than the other clustering types, but it’s perfect for specific kinds of data sets.\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you’ll get invaluable insight on your data. You might find connections you never would have thought of.\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#k-means-clustering-algorithm",
    "href": "posts/Fourth Blog Post/index.html#k-means-clustering-algorithm",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "K-means clustering algorithm",
    "text": "K-means clustering algorithm\nK-means clustering is the most commonly used clustering algorithm. It’s a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThis algorithm tries to minimize the variance of data points within a cluster. It’s also how most people are introduced to unsupervised machine learning.\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it’ll take more time to classify data points if there are a large amount of them in the data set.\nSince this is how k-means clusters data points, it doesn’t scale well.\n\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=3)\n\n\n\n\n\n\n\nDBSCAN clustering algorithm\nDBSCAN stands for density-based spatial clustering of applications with noise. It’s a density-based clustering algorithm, unlike k-means.\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\nChoosing the right initial parameters is critical for this algorithm to work.\n\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAgglomerative Hierarchy clustering algorithm\nThis is the most common type of hierarchical clustering algorithm. It’s used to group objects in clusters based on how similar they are to each other.\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nSpectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#what-is-an-anomaly",
    "href": "posts/Fourth Blog Post/index.html#what-is-an-anomaly",
    "title": "Anomaly/Outlier Detection",
    "section": "What is an anomaly?",
    "text": "What is an anomaly?\nBefore talking about anomaly detection, we need to understand what an anomaly is.\nGenerally speaking, an anomaly is something that differs from a norm: a deviation, an exception. In software engineering, by anomaly we understand a rare occurrence or event that doesn’t fit into the pattern, and, therefore, seems suspicious. Some examples are:\n\nsudden burst or decrease in activity;\nerror in the text;\nsudden rapid drop or increase in temperature.\n\nCommon reasons for outliers are:\n\ndata preprocessing errors;\nnoise;\nfraud;\nattacks.\n\nNormally, you want to catch them all; a software program must run smoothly and be predictable so every outlier is a potential threat to its robustness and security. Catching and identifying anomalies is what we call anomaly or outlier detection.\nFor example, if large sums of money are spent one after another within one day and it is not your typical behavior, a bank can block your card. They will see an unusual pattern in your daily transactions. This anomaly can typically be connected to fraud since identity thieves try to steal as much money as they can while they can. Once an anomaly is detected, it needs to be investigated, or problems may follow."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#why-do-you-need-machine-learning-for-anomaly-detection",
    "href": "posts/Fourth Blog Post/index.html#why-do-you-need-machine-learning-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection",
    "section": "Why do you need machine learning for anomaly detection?",
    "text": "Why do you need machine learning for anomaly detection?\nThis is a process that is usually conducted with the help of statistics and machine learning tools.\nThe reason is that the majority of companies today that require outlier detection work with huge amounts of data: transactions, text, image, and video content, etc. You would have to spend days going through all the transitions that happen inside a bank every hour, and more and more are generated every second. It is simply impossible to drive any meaningful insights from this amount of data manually.\nMoreover, another difficulty is that the data is often unstructured, which means that the information wasn’t arranged in any specific way for the data analysis. For example, business documents, emails, or images are examples of unstructured data.\nTo be able to collect, clean, structure, analyze, and store data, you need to use tools that aren’t scared of big volumes of data. Machine learning techniques, in fact, show the best results when large data sets are involved. Machine learning algorithms are able to process most types of data. Moreover, you can choose the algorithm based on your problem and even combine different techniques for the best results.\nMachine learning used for real-world applications helps to streamline the process of anomaly detection and save the resources. It can happen not only post-factum but also in real time. Real-time anomaly detection is applied to improve security and robustness, for instance, in fraud discovery and cybersecurity.\nWhat are anomaly detection methods?\n\n\n\n\n\nThere are different kinds of anomaly detection methods with machine learning.\n\nSupervised\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\nNote: While you can label some anomalies and try to classify them (hence it’s a classification task), the underlying goal of anomaly detection is defining “normal data points” rather than “abnormal data points”. So in real world applications with very few anomaly samples labelled, it’s almost never regarded as a supervised task.\n\n\nUnsupervised\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it’s impossible to predict all the anomalies that can occur in the dataset. Think of self-driving cars, for example. They can face a situation on the road that has never happened before. Putting all road situations into a finite number of classes would be impossible. That is why neural networks are priceless when working with real-life data in real-time.\nHowever, ANNs almost rocket science level of complexity. So before you try out those, you might want to experiment with more conventional algorithms like DBSCAN, especially if your project is not that big.\nMoreover, the architecture of neural networks is a black box. We often don’t know what kinds of events neural networks will label as anomalies, moreover, it can easily learn wrong rules that are not so easy to fix. That is why unsupervised anomaly detection techniques are often less trustworthy than supervised ones.\n\n\nSemi-supervised\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model’s predictions more accurate.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n\n&lt;Axes: xlabel='Item_MRP', ylabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n\n\n\n\n\n\n\n\nItem_MRP\nItem_Outlet_Sales\n\n\n\n\n0\n0.927507\n0.283587\n\n\n1\n0.072068\n0.031419\n\n\n2\n0.468288\n0.158115\n\n\n3\n0.640093\n0.053555\n\n\n4\n0.095805\n0.073651\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(base_estimator=KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Fit the models and predict outliers\nfor clf_name, clf in classifiers.items():\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores &gt; threshold).astype(int)\n        y_test_pred = (y_test_scores &gt; threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Test Set Predictions')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Training Set Predictions')\n    \n    plt.show()\n\n    # Evaluate the model performance\n    print(f\"------ {clf_name} ------\")\n    print(f\"Train Accuracy: {np.sum(y_train_pred == 0) / len(y_train_pred):.2%}\")\n    print(f\"Test Accuracy: {np.sum(y_test_pred == 0) / len(y_test_pred):.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test_pred, np.zeros_like(y_test_pred), zero_division=1))  # Set zero_division to 1\n    print(\"\\n\")\n\n\n\n\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOption 2\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Set up subplots for visualization\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\naxs = axs.flatten()\n\n# Fit the models and predict outliers\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores &gt; threshold).astype(int)\n        y_test_pred = (y_test_scores &gt; threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    inliers_train = X_train[y_train_pred == 0]\n    outliers_train = X_train[y_train_pred == 1]\n    inliers_test = X_test[y_test_pred == 0]\n    outliers_test = X_test[y_test_pred == 1]\n\n    axs[i].scatter(inliers_train[:, 0], inliers_train[:, 1], color='blue', label='Inliers (Train)')\n    axs[i].scatter(outliers_train[:, 0], outliers_train[:, 1], color='red', label='Outliers (Train)')\n    axs[i].scatter(inliers_test[:, 0], inliers_test[:, 1], color='green', label='Inliers (Test)')\n    axs[i].scatter(outliers_test[:, 0], outliers_test[:, 1], color='orange', label='Outliers (Test)')\n    axs[i].set_title(clf_name)\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = [\n    ('Angle-based Outlier Detector (ABOD)', ABOD(contamination=outliers_fraction)),\n    ('Cluster-based Local Outlier Factor (CBLOF)', CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state)),\n    ('Feature Bagging', FeatureBagging(base_estimator=KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state)),\n    ('Histogram-based Outlier Detection (HBOS)', HBOS(contamination=outliers_fraction)),\n    ('Isolation Forest', IsolationForest(contamination=outliers_fraction, random_state=random_state)),\n    ('K Nearest Neighbors (KNN)', KNN(contamination=outliers_fraction)),\n    ('Average KNN', KNN(method='mean', contamination=outliers_fraction))\n]\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Fit the models and predict outliers\nindex = 0\nwhile index &lt; len(classifiers):\n    clf_name, clf = classifiers[index]\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores &gt; threshold).astype(int)\n        y_test_pred = (y_test_scores &gt; threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Test Set Predictions')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Training Set Predictions')\n\n    # Evaluate the model performance\n    print(f\"------ {clf_name} ------\")\n    print(f\"Train Accuracy: {np.sum(y_train_pred == 0) / len(y_train_pred):.2%}\")\n    print(f\"Test Accuracy: {np.sum(y_test_pred == 0) / len(y_test_pred):.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test_pred, np.zeros_like(y_test_pred), zero_division=1))  # Set zero_division to 1\n    plt.show()\n    \n    # Increment the index for the next iteration\n    index += 1\n\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html",
    "href": "posts/Fifth Blog Post/index.html",
    "title": "Probability Theory and Random Variables",
    "section": "",
    "text": "The subject of probability theory is the foundation upon which all of statistics is built, providing a means for modelling population, experiments or almost anything else that could be considered a random phenomenon. Through these models, statisticians are able to draw inferences about populations, inferences based on examination of only a part of the whole."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#sample-space",
    "href": "posts/Fifth Blog Post/index.html#sample-space",
    "title": "Probability Theory and Random Variables",
    "section": "Sample Space",
    "text": "Sample Space"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#events",
    "href": "posts/Fifth Blog Post/index.html#events",
    "title": "Probability Theory and Random Variables",
    "section": "Events",
    "text": "Events\nAn event is any collection of possible outcomes of an experiment that is, any subset of \\(S\\) (including \\(S\\) itself)\nLet A be an event, a subset of S. We say the event A occurs if the outcome of the experiment is the set A. When speaking of probabilities, we generally speak of the probability of an event, rather than a set. But we may use the terms interchangeably.\nGiven any two events (or sets) A and B, we have the following elementary set operations.\nUnion: The union of A and B, written \\(A\\cup B\\) is the set of elements that belong to either A or B or both\n\\[\nA \\cup B = \\{x:x \\in A \\ or \\ x \\in B \\}\n\\]\nIntersection: The intersection of A and B, written \\(A\\cap B\\) is the set of elements that belong to both A and B\n\\[\nA \\cap B = \\{x:x \\in A \\ and \\ x \\in B\\}\n\\]\nComplementation: The complement of A, written \\(A^c\\), is the set of all elements that are not in A\n\\[\nA^c = \\{x:x \\not\\in A\\}\n\\]\nElementary set operations can be combined, somewhat akin to the way addition and multiplication can be combined. As long as we are careful, we can treat sets as if they were numbers. We can now state the following useful properties of set operations.\n\nCommutativity.\na. \\(A\\cup B = B \\cup B\\),\nb. \\(A \\cap B = B \\cap A\\)\nAssocaiativity\na. \\(A \\cup (B\\cup C) = (A \\cup B) \\cup C\\)\nb. \\(A \\cap (B\\cap C) = (A \\cap B) \\cap C\\)\nDistribution Laws\na. \\(A \\cap (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\\)\nb. \\(A \\cup (B\\cap C) = (A \\cup B) \\cap (A \\cup C)\\)\nDeMorgan’s Laws\na. \\((A \\cup B )^C = A^C \\cap B^C\\)\nb. \\((A \\cap B )^C = A^C \\cup B^C\\)"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#probability",
    "href": "posts/Fifth Blog Post/index.html#probability",
    "title": "Probability Theory and Random Variables",
    "section": "Probability",
    "text": "Probability\nConsider the simple experiment of tossing a fair coin, so \\(S = \\{H, T\\}\\). By a “fair” coin we mean a balanced coin that is equally as likely to land heads up as tails up, and hence the reasonable probability function is the one that assigns equal probabilities to heads and tails, that is\n\\[\nP(\\{H\\}) = P(\\{T\\})\n\\]\nSince \\(S = \\{H\\} \\cup \\{T\\}\\), we have \\(P(\\{H\\} \\cup \\{T\\}) = P(\\{H\\}) + P(\\{T\\})\\) and\n\\[\nP(\\{H\\}) + P(\\{T\\}) = 1\n\\]\nSolving the equations simultaneously shows that \\(P(\\{H\\}) = P(\\{T\\}) = 1/2\\)\nIn machine learning, we often deal with uncertainty and stochastic quantities, due to one of the reasons being incomplete observability — therefore, we most likely work with sampled data.\nNow, suppose we want to draw reliable conclusions about the behavior of a random variable, despite the fact that we only have limited data and we simply do not know the entire population.\nHence, we need some kind of way to generalize from the sampled data to the population, or in other words — we need to estimate the true data-generating process.\n\n\n\nEstimating the data-generating process\n\n\nUnderstanding the probability distribution, allows us to compute the probability of a certain outcome by also accounting for the variability in the results. Thus, it enables us to generalize from the sample to the population, estimate the data-generating function and predict the behavior of a random variable more accurately."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#set-theory",
    "href": "posts/Fifth Blog Post/index.html#set-theory",
    "title": "Probability Theory and Random Variables",
    "section": "Set Theory",
    "text": "Set Theory\nThe set \\(S\\) of all possible outcomes of a particular experiment is called the sample space for the experiment. If the experiment consists of tossing a coin, the sample space contains two outcomes, heads and tails: thus,\n\\[\nS = \\{H, T\\}\n\\]\nIf, on the other hand, the experiment consists of observing the reported SAT scores of randomly selected students at a certain university, the sample space would be the set of positive integers between 200 and 800 that are multiples of ten — that is, \\(S = \\{200, 210, 220, ..., 780, 790, 800 \\}\\)\nWe can classify sample spaces into two types according to the number of elements they contain. Sample spaces can be either countable or uncountable; If the elements of a sample space can be put into 1-1 correspondence with a subset of the integers, the sample space is countable. Of course, if the sample space contains only a finite number of elements, it is countable. Thus, the coin toss and SAT score sample is uncountable, since the positive real numbers cannot be put into 1-1 correspondence with the integers. If, however, we measured the reaction time to the nearest second, then the sample space would be (in seconds) \\(S = \\{0, 1, 2, 3, ... \\}\\), which is then countable."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#probability-distribution",
    "href": "posts/Fifth Blog Post/index.html#probability-distribution",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Distribution",
    "text": "Probability Distribution\nThe description of how likely a random variable takes one of its possible states can be given by a probability distribution. Thus, the probability distribution is a mathematical function that gives the probabilities of different outcomes for an experiment.\nMore generally it can be described as the function\n\\[\nP:A \\rightarrow R\n\\]\nwhich maps an input space A — related to the sample space — to a real number, namely the probability.\nFor the above function to characterize a probability distribution, it must follow all of the Kolmogorov axioms:\n\nNon-negativity\nNo probability exceeds 1\nAdditivity of any countable disjoint (mutually exclusive) events\n\nThe way we describe a probability distribution depends on whether the random variable is discrete or continuous, which will result in a probability mass or density function respectively."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#bayesian-inference",
    "href": "posts/Fifth Blog Post/index.html#bayesian-inference",
    "title": "Probability Theory and Random Variables",
    "section": "Bayesian Inference",
    "text": "Bayesian Inference\nBayesian inference is a way of making statistical inferences in which the statistician assigns subjective probabilities to the distributions that could generate the data. These subjective probabilities form the so-called prior distribution.\nAfter the data is observed, Bayes’ rule is used to update the prior, that is, to revise the probabilities assigned to the possible data generating distributions. These revised probabilities form the so-called posterior distribution.\nThis lecture provides an introduction to Bayesian inference and discusses a simple example of inference about the mean of a normal distribution.\n\nThe likelihood\nThe first building block of a parametric Bayesian model is the likelihood\nThe likelihood is equal to the probability density of x when the parameter of the data generating distribution is equal to \\(\\theta\\)\nFor the time being, we assume that and are continuous. Later, we will discuss how to relax this assumption.\n\n\nThe prior\nThe second building block of a Bayesian model is the prior\n\\[\np(\\theta)\n\\]\nThe prior is the subjective probability density assigned to the parameter\n\n\nThe posterior\nAfter observing the data , we use Bayes’ rule to update the prior about the parameter :\nThe conditional density is called posterior distribution of the parameter.\nBy using the formula for the marginal density derived above, we obtain\nThus, the posterior depends on the two distributions specified by the statistician, the prior and the likelihood .\nExample\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, binom\n\n# Set up prior distribution\na = 0.5  # alpha parameter\nb = 0.5  # beta parameter\nprior = beta(a, b)  # create Beta distribution object\n\n# Generate some fake data\nn = 100  # number of trials\nk = 40   # number of successes\ndata = binom.rvs(1, k/n, size=n)  # generate binary data from binomial distribution\n\n# Compute posterior distribution\na_post = a + np.sum(data)  # update alpha parameter\nb_post = b + n - np.sum(data)  # update beta parameter\nposterior = beta(a_post, b_post)  # create updated Beta distribution object\n\n# Compute credible intervals\nCI_95 = beta.ppf([0.025, 0.975], a_post, b_post)  # compute 95% credible interval\n\n# Plot prior and posterior distributions\nx = np.linspace(0, 1, 1000)\nprior_pdf = prior.pdf(x)\nposterior_pdf = posterior.pdf(x)\n\nplt.plot(x, prior_pdf, 'b--', linewidth=2)\nplt.plot(x, posterior_pdf, 'r-', linewidth=2)\nplt.ylim([0, max(posterior_pdf)*1.1])\nplt.legend(['Prior', 'Posterior'])\nplt.xlabel('p/(1-p)')\nplt.ylabel('Density')\nplt.title('Bayesian Analysis of p/(1-p)')\nplt.show()"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#the-likelihood",
    "href": "posts/Fifth Blog Post/index.html#the-likelihood",
    "title": "Probability Theory and Random Variables",
    "section": "The likelihood",
    "text": "The likelihood\nThe first building block of a parametric Bayesian model is the likelihood\nThe likelihood is equal to the probability density of x when the parameter of the data generating distribution is equal to \\(\\theta\\)\nFor the time being, we assume that and are continuous. Later, we will discuss how to relax this assumption."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#the-prior",
    "href": "posts/Fifth Blog Post/index.html#the-prior",
    "title": "Probability Theory and Random Variables",
    "section": "The prior",
    "text": "The prior\nThe second building block of a Bayesian model is the prior\n\\[\np(\\theta)\n\\]\nThe prior is the subjective probability density assigned to the parameter"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#the-posterior",
    "href": "posts/Fifth Blog Post/index.html#the-posterior",
    "title": "Probability Theory and Random Variables",
    "section": "The posterior",
    "text": "The posterior\nAfter observing the data , we use Bayes’ rule to update the prior about the parameter :\nThe conditional density is called posterior distribution of the parameter.\nBy using the formula for the marginal density derived above, we obtain\nThus, the posterior depends on the two distributions specified by the statistician, the prior and the likelihood .\nExample\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, binom\n\n# Set up prior distribution\na = 0.5  # alpha parameter\nb = 0.5  # beta parameter\nprior = beta(a, b)  # create Beta distribution object\n\n# Generate some fake data\nn = 100  # number of trials\nk = 40   # number of successes\ndata = binom.rvs(1, k/n, size=n)  # generate binary data from binomial distribution\n\n# Compute posterior distribution\na_post = a + np.sum(data)  # update alpha parameter\nb_post = b + n - np.sum(data)  # update beta parameter\nposterior = beta(a_post, b_post)  # create updated Beta distribution object\n\n# Compute credible intervals\nCI_95 = beta.ppf([0.025, 0.975], a_post, b_post)  # compute 95% credible interval\n\n# Plot prior and posterior distributions\nx = np.linspace(0, 1, 1000)\nprior_pdf = prior.pdf(x)\nposterior_pdf = posterior.pdf(x)\n\nplt.plot(x, prior_pdf, 'b--', linewidth=2)\nplt.plot(x, posterior_pdf, 'r-', linewidth=2)\nplt.ylim([0, max(posterior_pdf)*1.1])\nplt.legend(['Prior', 'Posterior'])\nplt.xlabel('p/(1-p)')\nplt.ylabel('Density')\nplt.title('Bayesian Analysis of p/(1-p)')\nplt.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#how-does-anomaly-detection-work",
    "href": "posts/Fourth Blog Post/index.html#how-does-anomaly-detection-work",
    "title": "Anomaly/Outlier Detection",
    "section": "How does anomaly detection work?",
    "text": "How does anomaly detection work?\nThere are several ways of training machine learning algorithms to detect anomalies. Supervised machine learning techniques are used when you have a labeled data set indicating normal vs. abnormal conditions. For example, a bank or credit card company can develop a process for labeling fraudulent credit card transactions after those transactions have been reported. Medical researchers might similarly label images or data sets indicative of future disease diagnosis. In such instances, supervised machine learning models can be trained to detect these known anomalies.\nResearchers might start with some previously discovered outliers but suspect that other anomalies also exist. In the scenario of fraudulent credit card transactions, consumers might fail to report suspicious transactions with innocuous-sounding names and of a small value. A data scientist might use reports that include these types of fraudulent transactions to automatically label other like transactions as fraud, using semi-supervised machine learning techniques.\nThe supervised and semi-supervised techniques can only detect known anomalies. However, the vast majority of data is unlabeled. In these cases, data scientists might use unsupervised anomaly detection techniques, which can automatically identify exceptional or rare events.\nFor example, a cloud cost estimator might look for unusual upticks in data egress charges or processing costs that could be caused by a poorly written algorithm. Similarly, an intrusion detection algorithm might look for novel network traffic patterns or a rise in authentication requests. In both cases, unsupervised machine learning techniques might be used to identify data points indicating things that are well outside the range of normal behavior. In contrast, supervised techniques would have to be explicitly trained using examples of previously known deviant behavior."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#different-types-of-anomalies",
    "href": "posts/Fourth Blog Post/index.html#different-types-of-anomalies",
    "title": "Anomaly/Outlier Detection",
    "section": "Different types of anomalies",
    "text": "Different types of anomalies\nBroadly speaking, there are three different types of anomalies.\n\nGlobal outliers, or point anomalies, occur far outside the range of the rest of a data set.\nContextual outliers deviate from other points in the same context, e.g., holiday or weekend sales.\nCollective outliers occur when a range of different types of data vary when considered together, for example, ice cream sales and temperature spikes."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#anomaly-detection-techniques",
    "href": "posts/Fourth Blog Post/index.html#anomaly-detection-techniques",
    "title": "Anomaly/Outlier Detection",
    "section": "Anomaly detection techniques",
    "text": "Anomaly detection techniques\nMany different kinds of machine learning algorithms can be trained to detect anomalies. Some of the most popular anomaly detection methods include the following:\n\nDensity-based algorithms determine when an outlier differs from a larger, hence denser normal data set, using algorithms like K-nearest neighbor and Isolation Forest.\nCluster-based algorithms evaluate how any point differs from clusters of related data using techniques like K-means cluster analysis.\nBayesian-network algorithms develop models for estimating the probability that events will occur based on related data and then identifying significant deviations from these predictions.\nNeural network algorithms train a neural network to predict an expected time series and then flag deviations."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Regression (Linear and Nonlinear)\n\n\n\n\n\n\n\ndata\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier Detection\n\n\n\n\n\n\n\ndata\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/First Blog Post/index.html#nonlinear-regression",
    "href": "posts/First Blog Post/index.html#nonlinear-regression",
    "title": "Regression (Linear and Nonlinear)",
    "section": "Nonlinear Regression",
    "text": "Nonlinear Regression\nNonlinear regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as a nonlinear function. In contrast to linear regression, which assumes a linear relationship between variables, nonlinear regression allows for more complex and curved relationships to be captured.\nThe general form of nonlinear regression is expressed as:\n\\[\nY = f(X, \\theta) + \\epsilon\n\\]\nWhere:\n\n\\(Y\\) is the dependent variable\n\\(X\\) is the independent variable(s)\n\\(\\theta\\) represents the parameters of the nonlinear function \\(f\\)\n\\(\\epsilon\\) is the error term\n\nThe goal of nonlinear regression is to estimate the parameters \\((\\theta)\\) of the chosen nonlinear function in a way that minimizes the sum of squared differences between the predicted values and the actual observed values. This is typically done using optimization techniques, such as gradient descent or other numerical optimization algorithms.\n\nImplementation\n\n# Step 4: Train the model using the training data\nalpha = 10  # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\ndegree = 3  # Degree of the polynomial features\n\n# Create polynomial features\nX_train_poly = np.column_stack([X_train ** i for i in range(1, degree + 1)])\n\n# Train the model using polynomial features\nX_train_poly = np.column_stack((np.ones_like(X_train_poly), X_train_poly))\ntheta, history_train, cost_train = ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)\nprint(f\"Training cost: {cost_train}\")\nprint(theta)\n\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    # Create polynomial features for validation data\n    X_val_poly = np.column_stack([X_val ** i for i in range(1, degree + 1)])\n    X_val_poly = np.column_stack((np.ones_like(X_val_poly), X_val_poly))\n    \n    theta_val, history_val, cost_val = ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_poly @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\"\"\"\n    # Plot the cost history for both training and validation\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()\n\"\"\"\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n\n# Step 6: Evaluate the model using the testing data\n# Create polynomial features for test data\nX_test_poly = np.column_stack([X_test ** i for i in range(1, degree + 1)])\nX_test_poly = np.column_stack((np.ones_like(X_test_poly), X_test_poly))\n\ntheta_test, _, cost_test = ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_poly @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\n\n# Plot training data and the polynomial fit\nplt.scatter(X_train, y_train, color='blue', label='Training data')\nx_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\nx_values_poly = np.column_stack([x_values ** i for i in range(1, degree + 1)])\nx_values_poly = np.column_stack((np.ones_like(x_values_poly), x_values_poly))\ny_values = x_values_poly @ theta\nplt.plot(x_values, y_values, color='red', label=f'Polynomial Fit (Degree {degree})')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Training Data and Polynomial Fit')\nplt.legend()\nplt.show()\n\nTraining cost: 104.19514186498827\n[[3.51715242]\n [3.51715242]\n [3.51715242]\n [5.00411734]\n [8.66812878]\n [6.78627004]]\nBest regularization parameter (alpha): 0.01\nMean Squared Error on Test Data: 19.60464293191099\n\n\n\n\n\n\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#implementation",
    "href": "posts/Fourth Blog Post/index.html#implementation",
    "title": "Anomaly/Outlier Detection",
    "section": "Implementation",
    "text": "Implementation\nLet’s implement anomaly detection in python using a dataset which contains sales data. First, we need to import the necessary libraries and view the data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n\n&lt;Axes: xlabel='Item_MRP', ylabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n\n\n\n\n\n\n\n\nItem_MRP\nItem_Outlet_Sales\n\n\n\n\n0\n0.927507\n0.283587\n\n\n1\n0.072068\n0.031419\n\n\n2\n0.468288\n0.158115\n\n\n3\n0.640093\n0.053555\n\n\n4\n0.095805\n0.073651\n\n\n\n\n\n\n\nThe next bit of code defines a Python dictionary named classifiers that contains instances of various outlier detection algorithms from the scikit-learn library. Each key-value pair in the dictionary represents a different outlier detection algorithm. Hence, the following algorithms were employed for outlier detection:\n\nAngle based Outlier Detector (ABOD)\nCluster based Local Outlier Factor (CBLOF)\nFeature Bagging\nHistogram-based Outlier Detection (HBOS)\nIsolation Forest\nK Nearest Neighbors (KNN)\nAverage KNN\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Angle-based Outlier Detector (ABOD)\nclf_abod = ABOD(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_abod.fit(X_train)\ny_train_pred_abod = clf_abod.labels_\ny_test_pred_abod = clf_abod.predict(X_test)\nprint(\"------ Angle-based Outlier Detector (ABOD) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_abod == 0) / len(y_train_pred_abod):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_abod == 0) / len(y_test_pred_abod):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_abod, np.zeros_like(y_test_pred_abod), zero_division=1))\ninliers_train_abod = X_train[y_train_pred_abod == 0]\noutliers_train_abod = X_train[y_train_pred_abod == 1]\ninliers_test_abod = X_test[y_test_pred_abod == 0]\noutliers_test_abod = X_test[y_test_pred_abod == 1]\nplt.scatter(inliers_train_abod[:, 0], inliers_train_abod[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_abod[:, 0], outliers_train_abod[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_abod[:, 0], inliers_test_abod[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_abod[:, 0], outliers_test_abod[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Angle-based Outlier Detector (ABOD)\")\nplt.legend()\nplt.show()\n\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n\n\n\n\n\n\n# Cluster-based Local Outlier Factor (CBLOF)\nclf_cblof = CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_cblof.fit(X_train)\ny_train_pred_cblof = clf_cblof.labels_\ny_test_pred_cblof = clf_cblof.predict(X_test)\nprint(\"------ Cluster-based Local Outlier Factor (CBLOF) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_cblof == 0) / len(y_train_pred_cblof):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_cblof == 0) / len(y_test_pred_cblof):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_cblof, np.zeros_like(y_test_pred_cblof), zero_division=1))\ninliers_train_cblof = X_train[y_train_pred_cblof == 0]\noutliers_train_cblof = X_train[y_train_pred_cblof == 1]\ninliers_test_cblof = X_test[y_test_pred_cblof == 0]\noutliers_test_cblof = X_test[y_test_pred_cblof == 1]\nplt.scatter(inliers_train_cblof[:, 0], inliers_train_cblof[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_cblof[:, 0], outliers_train_cblof[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_cblof[:, 0], inliers_test_cblof[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_cblof[:, 0], outliers_test_cblof[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Cluster-based Local Outlier Factor (CBLOF)\")\nplt.legend()\nplt.show()\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n\n\n\n\n# Feature Bagging\nclf_feature_bagging = FeatureBagging(KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_feature_bagging.fit(X_train)\ny_train_pred_feature_bagging = clf_feature_bagging.labels_\ny_test_pred_feature_bagging = clf_feature_bagging.predict(X_test)\nprint(\"------ Feature Bagging ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_feature_bagging == 0) / len(y_train_pred_feature_bagging):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_feature_bagging == 0) / len(y_test_pred_feature_bagging):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_feature_bagging, np.zeros_like(y_test_pred_feature_bagging), zero_division=1))\ninliers_train_feature_bagging = X_train[y_train_pred_feature_bagging == 0]\noutliers_train_feature_bagging = X_train[y_train_pred_feature_bagging == 1]\ninliers_test_feature_bagging = X_test[y_test_pred_feature_bagging == 0]\noutliers_test_feature_bagging = X_test[y_test_pred_feature_bagging == 1]\nplt.scatter(inliers_train_feature_bagging[:, 0], inliers_train_feature_bagging[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_feature_bagging[:, 0], outliers_train_feature_bagging[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_feature_bagging[:, 0], inliers_test_feature_bagging[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_feature_bagging[:, 0], outliers_test_feature_bagging[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Feature Bagging\")\nplt.legend()\nplt.show()\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n\n\n\n\n\n\n# Histogram-based Outlier Detection (HBOS)\nclf_hbos = HBOS(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_hbos.fit(X_train)\ny_train_pred_hbos = clf_hbos.labels_\ny_test_pred_hbos = clf_hbos.predict(X_test)\nprint(\"------ Histogram-based Outlier Detection (HBOS) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_hbos == 0) / len(y_train_pred_hbos):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_hbos == 0) / len(y_test_pred_hbos):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_hbos, np.zeros_like(y_test_pred_hbos), zero_division=1))\ninliers_train_hbos = X_train[y_train_pred_hbos == 0]\noutliers_train_hbos = X_train[y_train_pred_hbos == 1]\ninliers_test_hbos = X_test[y_test_pred_hbos == 0]\noutliers_test_hbos = X_test[y_test_pred_hbos == 1]\nplt.scatter(inliers_train_hbos[:, 0], inliers_train_hbos[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_hbos[:, 0], outliers_train_hbos[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_hbos[:, 0], inliers_test_hbos[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_hbos[:, 0], outliers_test_hbos[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Histogram-based Outlier Detection (HBOS)\")\nplt.legend()\nplt.show()\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n\n\n\n\n# Isolation Forest\nclf_isolation_forest = IsolationForest(contamination=outliers_fraction, random_state=random_state)\nplt.figure(figsize=(8, 6))\nclf_isolation_forest.fit(X_train)\ny_train_scores_if = clf_isolation_forest.decision_function(X_train)\ny_test_scores_if = clf_isolation_forest.decision_function(X_test)\nthreshold_if = np.percentile(y_train_scores_if, 100 * outliers_fraction)\ny_train_pred_if = (y_train_scores_if &gt; threshold_if).astype(int)\ny_test_pred_if = (y_test_scores_if &gt; threshold_if).astype(int)\nprint(\"------ Isolation Forest ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_if == 0) / len(y_train_pred_if):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_if == 0) / len(y_test_pred_if):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_if, np.zeros_like(y_test_pred_if), zero_division=1))\ninliers_train_if = X_train[y_train_pred_if == 0]\noutliers_train_if = X_train[y_train_pred_if == 1]\ninliers_test_if = X_test[y_test_pred_if == 0]\noutliers_test_if = X_test[y_test_pred_if == 1]\nplt.scatter(inliers_train_if[:, 0], inliers_train_if[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_if[:, 0], outliers_train_if[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_if[:, 0], inliers_test_if[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_if[:, 0], outliers_test_if[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Isolation Forest\")\nplt.legend()\nplt.show()\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n\n\n\n\n\n\n# K Nearest Neighbors (KNN)\nclf_knn = KNN(contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_knn.fit(X_train)\ny_train_pred_knn = clf_knn.labels_\ny_test_pred_knn = clf_knn.predict(X_test)\nprint(\"------ K Nearest Neighbors (KNN) ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_knn == 0) / len(y_train_pred_knn):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_knn == 0) / len(y_test_pred_knn):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_knn, np.zeros_like(y_test_pred_knn), zero_division=1))\ninliers_train_knn = X_train[y_train_pred_knn == 0]\noutliers_train_knn = X_train[y_train_pred_knn == 1]\ninliers_test_knn = X_test[y_test_pred_knn == 0]\noutliers_test_knn = X_test[y_test_pred_knn == 1]\nplt.scatter(inliers_train_knn[:, 0], inliers_train_knn[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_knn[:, 0], outliers_train_knn[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_knn[:, 0], inliers_test_knn[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_knn[:, 0], outliers_test_knn[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"K Nearest Neighbors (KNN)\")\nplt.legend()\nplt.show()\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n\n\n\n\n\n\n# Average KNN\nclf_avg_knn = KNN(method='mean', contamination=outliers_fraction)\nplt.figure(figsize=(8, 6))\nclf_avg_knn.fit(X_train)\ny_train_pred_avg_knn = clf_avg_knn.labels_\ny_test_pred_avg_knn = clf_avg_knn.predict(X_test)\nprint(\"------ Average KNN ------\")\nprint(f\"Train Accuracy: {np.sum(y_train_pred_avg_knn == 0) / len(y_train_pred_avg_knn):.2%}\")\nprint(f\"Test Accuracy: {np.sum(y_test_pred_avg_knn == 0) / len(y_test_pred_avg_knn):.2%}\")\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test_pred_avg_knn, np.zeros_like(y_test_pred_avg_knn), zero_division=1))\ninliers_train_avg_knn = X_train[y_train_pred_avg_knn == 0]\noutliers_train_avg_knn = X_train[y_train_pred_avg_knn == 1]\ninliers_test_avg_knn = X_test[y_test_pred_avg_knn == 0]\noutliers_test_avg_knn = X_test[y_test_pred_avg_knn == 1]\nplt.scatter(inliers_train_avg_knn[:, 0], inliers_train_avg_knn[:, 1], color='blue', label='Inliers (Train)')\nplt.scatter(outliers_train_avg_knn[:, 0], outliers_train_avg_knn[:, 1], color='red', label='Outliers (Train)')\nplt.scatter(inliers_test_avg_knn[:, 0], inliers_test_avg_knn[:, 1], color='green', label='Inliers (Test)')\nplt.scatter(outliers_test_avg_knn[:, 0], outliers_test_avg_knn[:, 1], color='orange', label='Outliers (Test)')\nplt.title(\"Average KNN\")\nplt.legend()\nplt.show()\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#k-nearest-neighbours",
    "href": "posts/Second Blog Post/index.html#k-nearest-neighbours",
    "title": "Classification",
    "section": "K-Nearest Neighbours",
    "text": "K-Nearest Neighbours\nK-Nearest Neighbors (KNN) is a simple and intuitive supervised machine learning algorithm used for classification and regression tasks. It is a type of instance-based learning, also known as lazy learning, where the algorithm makes predictions based on the entire training dataset rather than learning a specific model during the training phase.\nHere’s a basic overview of how the KNN algorithm works:\n\nTraining Phase:\n\nThe algorithm stores all the training examples in memory.\nEach example in the training set consists of a set of features and a corresponding class label.\n\nPrediction Phase:\n\nWhen a prediction is needed for a new, unseen data point, the algorithm calculates the distances between that point and all the points in the training set. Common distance metrics include Euclidean distance, Manhattan distance, or others depending on the problem.\nThe “k” nearest neighbors to the new data point are identified based on the calculated distances. “K” is a user-defined parameter representing the number of neighbors to consider.\nFor a classification task, the algorithm assigns the class label that is most frequent among the k neighbors. In regression tasks, the algorithm may return the average or weighted average of the target values of the k neighbors.\n\n\nKNN is a versatile algorithm with some key characteristics:\n\nNon-parametric: KNN doesn’t make any assumptions about the underlying data distribution. It adapts to the data during the training phase.\nInstance-based: Instead of building an explicit model during training, KNN stores the entire dataset and makes predictions based on the similarities between instances.\nSimple and interpretable: KNN is easy to understand and implement, making it a good choice for quick prototyping and baseline models.\n\nHowever, KNN has some limitations, such as being sensitive to irrelevant or redundant features, computation complexity (especially for large datasets), and a lack of interpretability for the decision-making process.\nChoosing the appropriate value for “k” is crucial, as a small k may lead to overfitting, and a large k may introduce bias. The optimal value of “k” often depends on the specific dataset and problem at hand.\n\nImplementation\nNow, let’s implement K-Nearest neighbours on the scikit learn breast cancer dataset to classify malignant and benign cancers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the KNN classifier\nk = 5  # You can choose your desired value for k\nknn_classifier = KNeighborsClassifier(n_neighbors=k)\nknn_classifier.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = knn_classifier.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries on the training set\nplot_decision_boundary(X_train_2d, y_train, knn_classifier, 'KNN Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries on the test set\nplot_decision_boundary(X_test_2d, y_test, knn_classifier, 'KNN Classification (Test Set)')\nplt.show()\n\nAccuracy: 97.37%"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#logistic-regression",
    "href": "posts/Second Blog Post/index.html#logistic-regression",
    "title": "Classification",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nLogistic Regression is a statistical method and a popular machine learning algorithm used for binary classification problems. Despite its name, it is used for classification rather than regression. Logistic Regression models the probability that a given input belongs to a particular category. It’s widely employed in various fields, such as medicine (disease prediction), marketing (customer churn analysis), and finance (credit scoring).\nHere’s a brief overview of how logistic regression works:\n\nSigmoid Function:\n\nLogistic Regression uses the logistic function, also called the sigmoid function, to model the probability.\nThe sigmoid function has an S-shaped curve and maps any real-valued number to the range [0, 1]. The formula for the sigmoid function is: \\(\\sigma(z) = \\frac{1}{1+ e^{-z}}\\) where \\(\\sigma(z)\\) is the linear combination of input features and weights.\n\nLinear Combination:\n\nLogistic Regression establishes a linear relationship between the input features and the log-odds (logit) of the probability of belonging to the positive class.\nThe linear combination is given by: \\(z = b_0 + b_1\\times x_1 + b_2\\times x_2 + ... + b_n \\times x_n\\) where \\(b_0, b_1, ... , b_n\\) are the coefficients (weights) and \\(x_1, x_2, ... , x_n\\) are the input features.\n\nProbability Prediction:\n\nThe output of the sigmoid function is interpreted as the probability that the given input belongs to the positive class. If \\(\\sigma(z)\\) is close to 1, the model predicts a high probability of belonging to the positive class; if close to 0, it predicts a low probability.\n\nDecision Boundary:\n\nA decision boundary is established by the model based on a threshold probability (commonly 0.5). If the predicted probability is above the threshold, the instance is classified as the positive class; otherwise, it’s classified as the negative class.\n\n\nTraining a logistic regression model involves finding the optimal weights that maximize the likelihood of the observed data given the model. This is typically done using optimization algorithms like gradient descent.\nLogistic Regression is advantageous for its simplicity, interpretability, and efficiency. However, it assumes a linear relationship between the features and the log-odds, which may not hold in all situations. Extensions like polynomial logistic regression can be used to capture non-linear relationships.\n\nImplementation\nNow, let’s implement logistic regression on the breast cancer dataset to classify malignant and benign cancers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\nlogreg_model.fit(X_train_2d, y_train)  # Use the reduced dimensionality for training\n\n# Make predictions on the test set\ny_pred = logreg_model.predict(X_test_2d)  # Use the same dimensionality for testing\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\n\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for logistic regression on the training set\nplot_decision_boundary(X_train_2d, y_train, logreg_model, 'Logistic Regression (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for logistic regression on the test set\nplot_decision_boundary(X_test_2d, y_test, logreg_model, 'Logistic Regression (Test Set)')\nplt.show()\n\nAccuracy: 99.12%"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#support-vector-machine",
    "href": "posts/Second Blog Post/index.html#support-vector-machine",
    "title": "Classification",
    "section": "Support Vector Machine",
    "text": "Support Vector Machine\nA Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its primary objective is to find a hyperplane in a high-dimensional space that best separates data points into different classes. In the context of classification, the SVM aims to create a decision boundary that maximizes the margin between classes.\nHere are key concepts and features of Support Vector Machines:\n\nHyperplane:\n\nIn a two-dimensional space, a hyperplane is a simple line. In higher dimensions, it becomes a hyperplane, which is a subspace of one dimension less than the ambient space. For a binary classification problem, the SVM seeks the hyperplane that best separates data points of different classes.\n\nMargin:\n\nThe margin is the distance between the hyperplane and the nearest data point from either class. SVMs strive to maximize this margin because a larger margin generally leads to better generalization performance on unseen data.\n\nSupport Vectors:\n\nSupport vectors are the data points that are closest to the hyperplane and have the most influence on determining its position. These are the critical elements for defining the margin and decision boundary.\n\nKernel Trick:\n\nSVMs can handle non-linear decision boundaries by using a kernel trick. The kernel function transforms the input features into a higher-dimensional space, making it possible to find a hyperplane in this transformed space. Common kernels include polynomial kernels and radial basis function (RBF) kernels.\n\nC Parameter:\n\nThe C parameter in SVM is a regularization parameter that controls the trade-off between achieving a smooth decision boundary and classifying the training points correctly. A smaller C value allows for a more flexible decision boundary (potentially with some misclassifications), while a larger C value enforces a stricter boundary.\n\n\nSVMs have several advantages:\n\nEffective in high-dimensional spaces.\nVersatile due to the kernel trick, enabling them to handle complex relationships in the data.\nResistant to overfitting, especially in high-dimensional spaces.\n\n\nImplementation\nNow, lets use support vector machines to classify malignant and benign cancers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction separately on the training and testing sets\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the SVM model\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = svm_model.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\n\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for SVM on the training set\nplot_decision_boundary(X_train_2d, y_train, svm_model, 'SVM Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for SVM on the test set\nplot_decision_boundary(X_test_2d, y_test, svm_model, 'SVM Classification (Test Set)')\nplt.show()\n\nAccuracy: 99.12%"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#naive-bayes",
    "href": "posts/Second Blog Post/index.html#naive-bayes",
    "title": "Classification",
    "section": "Naive Bayes",
    "text": "Naive Bayes\nNaive Bayes is a family of probabilistic algorithms used for classification and, in some cases, regression tasks. It is based on Bayes’ theorem, which is a mathematical formula that describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\nHere are the key concepts of Naive Bayes:\n\nBayes’ Theorem:\n\nBayes’ theorem relates the conditional and marginal probabilities of random events. For a classification problem, it can be expressed as: \\[P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}\\] where:\n\n\\(P(y|X)\\) is the posterior probability of class \\(y\\) given the features \\(X\\),\n\\(P(X|y)\\) is the likelihood of the features given the class,\n\\(P(y)\\) is the prior probability of class \\(y\\),\n\\(P(X)\\) is the probability of the features.\n\n\nNaive Assumption:\n\nThe “naive” in Naive Bayes comes from the assumption that features are conditionally independent given the class label. This means that the presence of one feature is considered independent of the presence of any other feature, given the class label. While this assumption simplifies the model, it may not always hold in real-world scenarios.\n\nTypes of Naive Bayes:\n\nThere are different variants of Naive Bayes, depending on the distributional assumptions made about the data. The three most common types are:\n\nGaussian Naive Bayes: Assumes that the features follow a normal distribution.\nMultinomial Naive Bayes: Used for discrete data, often for text classification with word frequencies.\nBernoulli Naive Bayes: Assumes binary (0 or 1) features, often used for text classification with binary term presence/absence.\n\n\nText Classification:\n\nNaive Bayes is particularly popular in text classification tasks, such as spam filtering and sentiment analysis. It works well with high-dimensional data like word counts in documents.\n\nTraining and Prediction:\n\nDuring training, Naive Bayes estimates the parameters (probabilities) from the training dataset.\nDuring prediction, it calculates the posterior probability for each class and assigns the class with the highest probability to the input instance.\n\n\nNaive Bayes is computationally efficient, simple to implement, and often performs surprisingly well, especially in text and document classification tasks. However, its performance may degrade when the independence assumption is strongly violated or when dealing with highly correlated features.\n\nImplementation\nNow, lets implement Naive Bayes to classify malignant and benign cancers using Gaussian Naive Bayes\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train the Naive Bayes model\nnb_model = GaussianNB()\nnb_model.fit(X_train_scaled, y_train)\n\n# Make predictions on the test set\ny_pred = nb_model.predict(X_test_scaled)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n# Plot Classification Results\nplt.figure(figsize=(8, 6))\ncorrectly_classified = (y_test == y_pred)\nincorrectly_classified = (y_test != y_pred)\n\n# Plot correctly classified points\nsns.scatterplot(x=X_test_scaled[correctly_classified, 0], y=X_test_scaled[correctly_classified, 1], color='green', label='Correctly Classified', marker='o')\n\n# Plot incorrectly classified points\nsns.scatterplot(x=X_test_scaled[incorrectly_classified, 0], y=X_test_scaled[incorrectly_classified, 1], color='red', label='Incorrectly Classified', marker='x')\n\nplt.title('Classification Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n\nAccuracy: 96.49%"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#neural-networks",
    "href": "posts/Second Blog Post/index.html#neural-networks",
    "title": "Classification",
    "section": "Neural Networks",
    "text": "Neural Networks\nNeural networks, or artificial neural networks (ANNs), are a class of machine learning models inspired by the structure and functioning of the human brain. They consist of interconnected nodes, known as neurons or artificial neurons, organized into layers. Neural networks are a fundamental component of deep learning, a subfield of machine learning that focuses on models with multiple layers, also known as deep neural networks.\nHere are the key components and concepts related to neural networks:\n\nNeurons:\n\nNeurons are the basic units of a neural network. They receive inputs, perform a weighted sum of those inputs, apply an activation function, and produce an output. The output is then passed to the next layer of neurons.\n\nLayers:\n\nNeural networks are organized into layers, typically divided into three types:\n\nInput Layer: Neurons that receive the initial input data.\nHidden Layers: Neurons that process the input data. Deep neural networks have multiple hidden layers.\nOutput Layer: Neurons that produce the final output of the network.\n\n\nConnections and Weights:\n\nNeurons in one layer are connected to neurons in the next layer by connections. Each connection has an associated weight that determines the strength of the connection. During training, these weights are adjusted to optimize the network’s performance.\n\nActivation Function:\n\nThe activation function introduces non-linearity to the network, allowing it to learn complex relationships in the data. Common activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).\n\nFeedforward and Backpropagation:\n\nDuring the feedforward phase, input data is passed through the network to generate predictions. The predictions are compared to the actual targets, and the error is calculated.\nBackpropagation is the process of iteratively adjusting the weights of the connections based on the calculated error. This is done using optimization algorithms like gradient descent to minimize the error and improve the model’s performance.\n\nDeep Learning:\n\nNeural networks with multiple hidden layers are referred to as deep neural networks. The depth of the network allows it to learn hierarchical features and representations, making it capable of handling complex tasks.\n\nTypes of Neural Networks:\n\nDifferent types of neural networks are designed for specific tasks. For example:\n\nFeedforward Neural Networks (FNN): Standard neural networks where information flows in one direction, from input to output.\nConvolutional Neural Networks (CNN): Effective for image-related tasks, with specialized layers for feature extraction.\nRecurrent Neural Networks (RNN): Suitable for sequential data, with connections that form cycles to capture temporal dependencies.\n\n\n\nNeural networks have achieved remarkable success in various domains, including image and speech recognition, natural language processing, and playing games. Their power lies in their ability to automatically learn complex patterns and representations from data, enabling them to excel in tasks that traditional algorithms may struggle with.\n\nImplementation\nNow, let’s use a neural network to classify malignant or benign cancers\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_breast_cancer\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the neural network model using Keras\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification, so use 'sigmoid' activation\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Visualize the model architecture\nplot_model(model, to_file='neural_network.png', show_shapes=True, show_layer_names=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot training history\nplt.figure(figsize=(12, 5))\n\n# Plot training & validation accuracy values\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n1/4 [======&gt;.......................] - ETA: 0s - loss: 0.1449 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b4/4 [==============================] - 0s 960us/step - loss: 0.0669 - accuracy: 0.9825\nTest Loss: 0.0669\nTest Accuracy: 0.9825"
  },
  {
    "objectID": "posts/Second Blog Post/index.html#decision-trees",
    "href": "posts/Second Blog Post/index.html#decision-trees",
    "title": "Classification",
    "section": "Decision Trees",
    "text": "Decision Trees\nA Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It’s a tree-like structure where each internal node represents a decision based on the value of a particular feature, each branch represents the outcome of that decision, and each leaf node represents the final decision or prediction.\nHere are the key concepts of decision trees:\n\nNode Types:\n\nRoot Node: The topmost node that makes the initial decision.\nInternal Nodes: Nodes that represent decisions based on feature values.\nLeaf Nodes: Terminal nodes that provide the final prediction or decision.\n\nDecision Criteria:\n\nAt each internal node, a decision is made based on the value of a specific feature. The goal is to make decisions that result in the most accurate predictions.\n\nSplitting:\n\nThe process of dividing a node into two or more child nodes based on a chosen feature and a threshold value. The goal is to increase the homogeneity of the target variable within each resulting node.\n\nHomogeneity and Impurity:\n\nDecision trees aim to create nodes that are as pure as possible. Impurity measures, such as Gini impurity or entropy, are used to quantify the homogeneity within a node. The goal is to minimize impurity during the tree-building process.\n\nTree Pruning:\n\nDecision trees can become too complex and overfit the training data. Pruning involves removing some branches (subtrees) from the tree to prevent overfitting and improve generalization to new data.\n\nCategorical and Continuous Variables:\n\nDecision trees can handle both categorical and continuous features. For categorical features, the tree performs a split for each category, while for continuous features, the tree finds an optimal threshold to split the data.\n\nEnsemble Methods:\n\nDecision trees are often used in ensemble methods, such as Random Forests and Gradient Boosting, to enhance predictive performance. Ensemble methods combine the predictions of multiple decision trees to achieve more robust and accurate results.\n\nInterpretability:\n\nDecision trees are known for their interpretability. The structure of the tree provides a clear and intuitive representation of the decision-making process, making it easy to understand how the model arrives at its predictions.\n\n\nDecision trees are used in various applications, including finance, healthcare, and marketing. They are particularly useful when dealing with a mix of categorical and numerical features and are valued for their simplicity, interpretability, and ability to handle non-linear relationships in the data.\n\nImplementation\nNow lets use a Decision Tree to classify malignant and benign cancers\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot the confusion matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nplt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_mat[i, j]), ha='center', va='center')\n\nplt.show()\n\n# Plot the decision tree\nplt.figure(figsize=(15, 10))\ntree.plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)\nplt.show()\n\nTest Accuracy: 0.9474"
  },
  {
    "objectID": "posts/Third Blog Post/index.html#implementation",
    "href": "posts/Third Blog Post/index.html#implementation",
    "title": "Clustering",
    "section": "Implementation",
    "text": "Implementation\nWe will use the make_classification() function of the scikit learn library to create a test binary classification dataset. The dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how “well” the clusters were identified.\nIt is worth mentionining that the clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters.\nAn example of creating and summarizing the synthetic clustering dataset is listed below.\n\n# synthetic classification dataset\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# create scatter plot for samples from each class\nfor class_value in range(2):\n    # get row indexes for samples with this class\n    row_ix = where(y == class_value)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\nRunning the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters).\nWe can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings."
  },
  {
    "objectID": "posts/Third Blog Post/index.html#dbscan-clustering-algorithm",
    "href": "posts/Third Blog Post/index.html#dbscan-clustering-algorithm",
    "title": "Clustering",
    "section": "DBSCAN clustering algorithm",
    "text": "DBSCAN clustering algorithm\nDBSCAN stands for density-based spatial clustering of applications with noise. It’s a density-based clustering algorithm, unlike k-means.\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\nChoosing the right initial parameters is critical for this algorithm to work.\n\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Third Blog Post/index.html#agglomerative-hierarchy-clustering-algorithm",
    "href": "posts/Third Blog Post/index.html#agglomerative-hierarchy-clustering-algorithm",
    "title": "Clustering",
    "section": "Agglomerative Hierarchy clustering algorithm",
    "text": "Agglomerative Hierarchy clustering algorithm\nThis is the most common type of hierarchical clustering algorithm. It’s used to group objects in clusters based on how similar they are to each other.\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nSpectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#probability-density-function",
    "href": "posts/Fifth Blog Post/index.html#probability-density-function",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nIn the earlier sections, we learned that a random variable can either be discrete or continuous. If it is discrete, we can describe the probability distribution with a probability mass function.\nNow, we are dealing with continuous variables — hence, we need to describe the probability distribution with a probability density function (PDF).\nThe PDF, contrary to the PMF, does not give the probability of a random variable taking a specific state directly. Instead, it describes the probability of landing inside an infinitesimal region. In other terms, the PDF describes the probability of a random variable lying between a particular range of values.\nIn order to find the actual probability mass, we need to integrate, which yields the area under the density function but above the x-axis.\n\n\n\nAn example of a probability density function\n\n\nThe probability density function must be non-negative and its integral needs to be 1.\n\\[\n(1) \\ \\ p(x) \\ge 0\n\\]\n\\[\n(2) \\ \\ \\int p(x) \\delta x = 1\n\\]\nOne of the most common continuous probability distributions is the gaussian or normal distribution."
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#probability-mass-function",
    "href": "posts/Fifth Blog Post/index.html#probability-mass-function",
    "title": "Probability Theory and Random Variables",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nThe probability mass function (PMF) describes the probability distribution over a discrete random variable. In other terms, it is a function that returns the probability of a random variable being exactly equal to a specific value.\nThe returned probability lies in the range [0, 1] and the sum of all probabilities for every state equals one.\nLet’s imagine a plot where the x-axis describes the states and the y-axis shows the probability of a certain state. Thinking this way allows us to envision the probability or the PMF as a barplot sitting on top of a state.\n\n\n\nAn example of a uniform PMF\n\n\nIn the following, we will learn about three common discrete probability distributions: The Bernoulli, binomial and geometric distribution.\n\nBernoulli Distribution\nNamed after the Swiss mathematician Jacob Bernoulli, the Bernoulli distribution is a discrete probability distribution of a single binary random variable, which either takes the value 1 or 0.\nLoosely speaking, we can think of the Bernoulli distribution as a model giving the set of possible outcomes for a single experiment, that can be answered with a simple yes-no question.\nMore formally the function can be stated as the following equation\n\\[\nf(k;p) =     \\begin{dcases}\n        q = 1-p & if \\ k = 0 \\\\\n        q & if \\ k = 1 \\\\\n    \\end{dcases}\n\\]\n\\[\nf(k;p) = p^k(1-p)^{1-k} \\ for \\ k \\in \\{0,1\\}\n\\]\nwhich basically evaluates to p if k=1 or to (1-p) if k=0. Thus, the Bernoulli distribution is parametrized by just a single parameter p.\nSuppose, we toss a fair coin once. The probability of obtaining heads is P(Heads) = 0.5. Visualizing the PMF we get the following plot:\n\nSince the Bernoulli Distribution models only a single trial, it can also be viewed as a special case of the binomial distribution\n\n\nBinomial Distribution\nThe binomial distribution describes the discrete probability distribution of the number of successes in a sequence of n independent trials, each with a binary outcome. The success or failure is given by the probability p or (1-p) respectively.\nThus, the binomial distribution is parametrized by the parameters\n\\[\nn \\in N, \\ p \\in [0,1]\n\\]\nMore formally the binomial distribution can be expressed with the following equation:\n\\[\nf(k;n,p) = {n \\choose k} p^k(1-p)^{n-k}\n\\]\nThe success of k is given by the probability p to the power of k, whereas the probability of failure is defined by (1-p) to the power of n minus k, which is basically the number of trials minus the one trial where we get k.\nSince the event of success k can occur anywhere in n trials, we have “n choose k” ways to distribute the success.\nLet’s pick up our coin-tossing example from before and build on it.\nNow, we are going to flip the fair coin three times, while being interested in the random variable describing the number of heads obtained.\n\n\n\nNumber of heads in three coin flips\n\n\nIf we want to compute the probability of the coin coming up as heads two times, we can simply use the equation from before and pluck in the values.\n\\[\nP(2) = {3 \\choose 2} p^2 (1-p)^{3-2}\n\\]\n\\[\nP(2) = 3(0.5)^2(0.5)^1\n\\]\n\\[\nP(2) = 0.375\n\\]\nwhich results in a probability P(2) = 0.375. If we proceed in the same way for the remaining probabilities, we get the following distribution:\n\n\n\nGeometric Distribution\nSuppose, we are interested in the number of times we have to flip a coin until it comes up heads for the first time.\nThe geometric distribution gives the probability of the first success occurrence, requiring n independent trials, with a success probability of p.\nMore formally it can be stated as\nwhich computes the probability of the number of trials needed up to and including the success event.\nThe following assumptions need to be true, in order to calculate the geometric distribution:\n\nIndependence\nFor each trial, there are only two possible outcomes\nThe probability of success is the same for every trial\n\nLet’s visualize the geometric distribution by answering the question for the probability of the number of trials needed for the coin to come up heads for the first time.\n\n\n\nThe geometric distribution until first head\n\n\n\n\nGaussian Distribution\nThe Gaussian distribution is often considered a sensible choice to represent a real-valued random variable, whose distribution is unknown.\nThis is mainly due to the central limit theorem, which, loosely speaking, states that the average of many independent random variables with finite mean and variance is itself a random variable — which is normally distributed as the number of observations increases.\nThis is especially useful since it allows us to model complicated systems as Gaussian distributed, even if the individual parts follow a more complicated structure or distribution.\nAnother reason it is a common choice for modeling a distribution over a continuous variable is the fact that it inserts the least amount of prior knowledge.\nMore formally, the Gaussian distribution can be stated as\n\\[\nN (x: \\mu, \\sigma^2) = \\sqrt\\frac{1}{2\\pi\\sigma^2}exp(-\\frac{1}{2\\sigma^2}(x-\\mu)^2)\n\\]\nwhere the parameter µ is the mean and σ² describes the variance.\nIn simple terms, the mean will be responsible for defining the central peak of the bell-shaped distribution, whereas the variance or the standard deviation defines its width.\nWe can visualize the normal distribution as the following:\n\n\n\nAn example of a Gaussian distribution"
  },
  {
    "objectID": "posts/Fifth Blog Post/index.html#markov-chain-monte-carlo-mcmc",
    "href": "posts/Fifth Blog Post/index.html#markov-chain-monte-carlo-mcmc",
    "title": "Probability Theory and Random Variables",
    "section": "Markov Chain Monte Carlo (MCMC)",
    "text": "Markov Chain Monte Carlo (MCMC)\nThere are several Bayesian models that allow us to compute the posterior distribution of the parameters analytically. However, this is often not possible.\nWhen an analytical solution is not available, Markov Chain Monte Carlo (MCMC) methods are commonly employed to derive the posterior distribution numerically.\nMCMC methods are Monte Carlo methods that allow us to generate large samples of correlated draws from the posterior distribution of the parameter vector by simply using the proportionality\nThe empirical distribution of the generated sample can then be used to produce plug-in estimates of the quantities of interest.\nSee the lecture on MCMC methods for more details.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Define the target distribution\ndef target_dist(x):\n    return np.exp(-x**2 / 2) / np.sqrt(2 * np.pi)\n\n# Define the proposal distribution (normal distribution)\ndef prop_dist(x, sigma):\n    return np.random.normal(x, sigma)\n\n# Set the initial state and other parameters\nx0 = 0\nnum_samples = 10000\nburn_in = 1000\nsigma = 1\n\n# Initialize the Markov chain\nx = x0\n\n# Generate samples using the Metropolis-Hastings algorithm\nsamples = np.zeros(num_samples)\nfor i in range(num_samples + burn_in):\n    # Generate a proposal\n    x_prop = prop_dist(x, sigma)\n\n    # Compute the acceptance probability\n    alpha = min(1, target_dist(x_prop) / target_dist(x))\n\n    # Decide whether to accept the proposal\n    if np.random.rand() &lt; alpha:\n        x = x_prop\n\n    # Save the sample after the burn-in period\n    if i &gt; burn_in:\n        samples[i - burn_in] = x\n\n# Plot the histogram of the samples and the target distribution\nplt.hist(samples, bins=30, density=True, alpha=0.5)\nx_vals = np.linspace(-5, 5, 100)\nplt.plot(x_vals, target_dist(x_vals), 'r-', linewidth=2)\nplt.xlabel('x')\nplt.ylabel('Probability density')\nplt.legend(['Samples', 'Target distribution'])\nplt.show()"
  }
]