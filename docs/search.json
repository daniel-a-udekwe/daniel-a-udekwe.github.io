[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning I Blog Posts",
    "section": "",
    "text": "Regression (Linear and Nonlinear)\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection and Treatment Techniques\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nProbability Theory and Random Variables\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nDaniel A. Udekwe\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2\n\n\nlets run a for loop\n\nfor i in range(1,10):\n  print(i)\n\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/third Post/index.html",
    "href": "posts/third Post/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/First Blog Post/index.html",
    "href": "posts/First Blog Post/index.html",
    "title": "Regression (Linear and Nonlinear)",
    "section": "",
    "text": "Regression is one of the most common algorithms in machine learning. In this post we will explore this algorithm and we will implement it using Python from scratch.\nAs the name suggests this algorithm is applicable for Regression problems. Linear Regression is a Linear Model. Which means, we will establish a linear relationship between the input variables(X) and single output variable(Y). When the input(X) is a single variable this model is called Simple Linear Regression and when there are mutiple input variables(X), it is called Multiple Linear Regression. Additionally, non-linear regression seeks to establish a non-linear relationship between the input variable (x) and the output (Y)"
  },
  {
    "objectID": "posts/Second Blog Post/index.html",
    "href": "posts/Second Blog Post/index.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is a task of Machine Learning which assigns a label value to a specific class and then can identify a particular type to be of one kind or another. The most basic example can be of the mail spam filtration system where one can classify a mail as either \"spam\" or \"not spam\".\nClassification usually refers to any kind of problem where a specific type of class label is the result to be predicted from the given input field of data. Some types of Classification challenges are :\nFor any model, you will require a training dataset with many examples of inputs and outputs from which the model will train itself. The training data must include all the possible scenarios of the problem and must have sufficient data for each label for the model to be trained correctly. Class labels are often returned as string values and hence needs to be encoded into an integer like either representing 0 for \"spam\" and 1 for \"no-spam\".\nThere are mainly 4 different types of classification tasks that you might encounter in your day to day challenges. Generally, the different types of predictive models in machine learning are as follows :\nWe will go over them one by one."
  },
  {
    "objectID": "posts/Third Blog Post/index.html",
    "href": "posts/Third Blog Post/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and i"
  },
  {
    "objectID": "posts/First Blog Post/index.html#simple-linear-regression",
    "href": "posts/First Blog Post/index.html#simple-linear-regression",
    "title": "Regression (Linear and Nonlinear)",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nWe discussed that Linear Regression is a simple model. Simple Linear Regression is the simplest model in machine learning.\n\nModel Representation\nIn this problem we have an input variable - X and one output variable - Y. And we want to build linear relationship between these variables. Here the input variable is called Independent Variable and the output variable is called Dependent Variable. We can define this linear relationship as follows:\\[Y = \\beta_0 + \\beta_1 X\\]\nThe \\(\\beta_1\\) is called a scale factor or coefficient and \\(\\beta_0\\) is called bias coefficient. The bias coeffient gives an extra degree of freedom to this model. This equation is similar to the line equation \\(y = mx +b\\) with \\(m = \\beta_1\\)(Slope) and \\(b = \\beta_0\\)(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.\nBut how do we find these coefficients? That’s the learning procedure. We can find these using different approaches. One is called Ordinary Least Square Method and other one is called Gradient Descent Approach. We will use Ordinary Least Square Method in Simple Linear Regression and Gradient Descent Approach in Multiple Linear Regression in post.\n\n\nOrdinary Least Square Method\nEarlier in this post we discussed that we are going to approximate the relationship between X and Y to a line. Let’s say we have few inputs and outputs. And we plot these scatter points in 2D space, we will get something like the following image.\n\nAnd you can see a line in the image. That’s what we are going to accomplish. And we want to minimize the error of out model. A good model will always have least error. We can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.\n\nAnd total error of this model is the sum of all errors of each point. ie.\n\\[\nD = \\sum_{i=1}^{m} d_i^2\n\\]\n\\(d_i\\)Distance between line and ith point.\n\\(m\\)- Total number of points\nYou might have noticed that we are squaring each of the distances. This is because, some points will be above the line and some points will be below the line. We can minimize the error in the model by minimizing \\(D\\) And after the mathematics of minimizing \\(D_i\\), we will get;\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n\\]\n\\[\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n\\]\nIn these equations �¯ is the mean value of input variable X and �¯ is the mean value of output variable Y.\nNow we have the model. This method is called Ordinary Least Square Method. Now we will implement this model in Python.\n\\[\nY = \\beta_0 + \\beta_1X\n\\]\n\\[\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n\\]\n\\[\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n\\]\n\n\nImplementation\nWe are going to use a dataset containing the price of houses as a function of size. This data is split into 3 for training, validation and finally testing. Let’s start off by importing and viewing the data.\n\n#Import the needed libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\n\nwith open('assignment1.json', 'r') as json_file:\n    data = json.load(json_file)\n    \nX_train = np.array(data['X_train']).reshape(-1, 1)\ny_train = np.array(data['Y_train']).reshape(-1, 1)\nX_val = np.array(data['X_val']).reshape(-1, 1)\ny_val = np.array(data['Y_val']).reshape(-1, 1)\nX_test = np.array(data['X_test']).reshape(-1, 1)\ny_test = np.array(data['Y_test']).reshape(-1, 1)\n\n\narray1 = data['X_train']\narray2 = data['Y_train']\n\narray3 = data['X_val']\narray4 = data['Y_val']\n\narray5 = data['X_test']\narray6 = data['Y_test']\n\n\n\nprint(\"Training Data\")\ntableData = {\n  'X_train': array1,\n  'Y_train': array2,\n}\ntable = pd.DataFrame(tableData)\nprint(table.head(3))\nprint(f\"number of rows and colums: {table.shape}\")\nprint()\n\nprint(\"Validation Data\")\ntableData2 = {\n  'X_Val': array3,\n  'Y_Val': array4\n}\ntable2 = pd.DataFrame(tableData2)\nprint(table2.head(3))\nprint(f\"number of rows and colums: {table2.shape}\")\nprint()\n\nprint(\"Test Data\")\ntableData3 = {\n  'X_test': array5,\n  'Y_test': array6\n}\ntable3 = pd.DataFrame(tableData3)\nprint(table3.head(3))\nprint(f\"number of rows and colums: {table3.shape}\")\n\nTraining Data\n   X_train  Y_train\n0    661.5     4.25\n1    465.0     2.30\n2   1442.0    68.00\nnumber of rows and colums: (12, 2)\n\nValidation Data\n    X_Val   Y_Val\n0   650.0   4.170\n1   682.5   4.067\n2  1417.0  31.870\nnumber of rows and colums: (21, 2)\n\nTest Data\n   X_test  Y_test\n0   405.0   3.316\n1   330.0   5.400\n2   135.0   1.300\nnumber of rows and colums: (21, 2)\n\n\nAs we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.\nwe need to implement feature scaling\n\n# Step 2: Preprocess the data (feature scaling)\nmean = np.mean(X_train)\nstd = np.std(X_train)\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n\nNext, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first\n\nplt.scatter(array1, array2, label='training data')\nplt.scatter(array3, array4, label='validation data')\nplt.scatter(array5, array6, label='testing data')\n\nplt.ylabel('House Price')\nplt.xlabel('House size')\nplt.legend()\nplt.title('Training, Validation and Testing Data')\n\nText(0.5, 1.0, 'Training, Validation and Testing Data')\n\n\n\n\n\nLets create a function to implement linear regression with regularization.\n\\[\nJ(\\theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\theta_j^2)\n\\]\n\n# Step 3: Implement regularized linear regression with gradient descent\ndef ridge_regression(X, y, alpha, num_iterations, learning_rate):\n    m, n = X.shape\n    # Initialize theta with the correct shape\n    theta = np.zeros((n, 1))\n    history = []\n\n    for _ in range(num_iterations):\n        gradient = (X.T @ (X @ theta - y) + alpha * theta) / m\n        theta -= learning_rate * gradient\n        cost = np.sum((X @ theta - y) ** 2) / (2 * m) + (alpha / (2 * m)) * np.sum(theta[1:] ** 2)\n        history.append(cost)\n\n    return theta, history, cost\n\nIn the equation above, \\(\\lambda\\) is the regularization parameter which ensures … This is implemented with the gradient descent algorithm.\n\n\nGradient Descent\nGradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n\nStep 1\nInitialize values \\(\\theta_0, \\theta_1, ..., \\theta_n\\) with some value. In this case we will initialize with 0.\n\n\nStep 2\nIteratively update,\n\\[\n\\theta_j : \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n\\]\nuntil it converges.\nwhere:\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \\ j = 0\n\\]\n\\[\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j \\ j\\ge 1\n\\]\nThis is the procedure. Here \\(\\alpha\\) is the learning rate. This operation \\(\\frac{\\partial}{\\partial \\theta_j} J(\\theta)\\) means we are finding partial derivate of cost with respect to each \\(\\theta_j\\). This is called Gradient.\nRead this if you are unfamiliar with partial derivatives.\nIn step 2 we are changing the values of \\(\\theta_j\\). in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don’t want to change values of \\(\\theta_j\\). drastically, because we might miss the minima. That’s why we need learning rate.\n\n\n\nAnimation illustrating the gradient descent method\n\n\nThe above animation illustrates the Gradient Descent method.\nBut we still didn’t find the value of \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\) . After we applying the mathematics. The step 2 becomes.\n\\[\n\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j\n\\]\nWe iteratively change values of \\(\\theta_j\\) according to above equation. This particular method is called Batch Gradient Descent.\nThen we need to implement a function to plot the line of best fit\n\ndef plot_data_and_fit(X_train, y_train, theta):\n    plt.scatter(X_train, y_train, color='blue', label='Training data')\n    \n    x_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\n    x_values_extended = np.column_stack((np.ones_like(x_values), x_values))\n    y_values = x_values_extended @ theta\n    \n    plt.plot(x_values, y_values, color='red', label='Line of best fit')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Training Data and Line of Best Fit')\n    plt.legend()\n    plt.show()\n\nNext we will train the model using the training data and return the cost using the equations below:\n\n# Step 4: Train the model using the training data\nalpha = 0 # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\nX_train_extended = np.column_stack((np.ones_like(X_train), X_train))\ntheta, history, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\nprint(f\"the cost iss: {cost}\")\n\nthe cost iss: 86.7594520168758\n\n\nThen tuning the regularization parameter\n\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    X_val_extended = np.column_stack((np.ones_like(X_val), X_val))\n    theta_val, history_, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_extended @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n\nBest regularization parameter (alpha): 10\n\n\nEvaluating the Model on the Testing data using the root mean square of errors.\nRoot Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,\n\\[\nRMSE = \\sqrt{\\sum_{i=1}^m \\frac{1}{m}(\\hat{y_1}-y_i)^2)}\n\\]\nHere \\(\\hat{y_i}\\) is the \\(i^{th}\\) predicted output values.\n\n# Step 6: Evaluate the model using the testing data\nX_test_extended = np.column_stack((np.ones_like(X_test), X_test))\ntheta_test, _, cost = ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_extended @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\nprint(cost)\n\nMean Squared Error on Test Data: 81.66385694629241\n234.9787954527312\n\n\n\n# Plot the cost history during training\nplt.plot(range(num_iterations), history)\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\nplt.title('Cost vs. Iteration')\nplt.show()\n\n\n\n\nPlotting the training data and line of best fit\n\n# Plot training data and line of best fit\nplot_data_and_fit(X_train, y_train, theta)\nprint(theta)\n\n\n\n\n[[22.10833333]\n [20.73627522]]\n\n\n\\[\nHouse Price = 22.108 + 20.736 \\times House size\n\\]\n\nNonlinear Regression\n\n# Step 4: Train the model using the training data\nalpha = 10  # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\ndegree = 3  # Degree of the polynomial features\n\n# Create polynomial features\nX_train_poly = np.column_stack([X_train ** i for i in range(1, degree + 1)])\n\n# Train the model using polynomial features\nX_train_poly = np.column_stack((np.ones_like(X_train_poly), X_train_poly))\ntheta, history_train, cost_train = ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)\nprint(f\"Training cost: {cost_train}\")\nprint(theta)\n\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    # Create polynomial features for validation data\n    X_val_poly = np.column_stack([X_val ** i for i in range(1, degree + 1)])\n    X_val_poly = np.column_stack((np.ones_like(X_val_poly), X_val_poly))\n    \n    theta_val, history_val, cost_val = ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_poly @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\"\"\"\n    # Plot the cost history for both training and validation\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()\n\"\"\"\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n\n# Step 6: Evaluate the model using the testing data\n# Create polynomial features for test data\nX_test_poly = np.column_stack([X_test ** i for i in range(1, degree + 1)])\nX_test_poly = np.column_stack((np.ones_like(X_test_poly), X_test_poly))\n\ntheta_test, _, cost_test = ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_poly @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\n\n# Plot training data and the polynomial fit\nplt.scatter(X_train, y_train, color='blue', label='Training data')\nx_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\nx_values_poly = np.column_stack([x_values ** i for i in range(1, degree + 1)])\nx_values_poly = np.column_stack((np.ones_like(x_values_poly), x_values_poly))\ny_values = x_values_poly @ theta\nplt.plot(x_values, y_values, color='red', label=f'Polynomial Fit (Degree {degree})')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Training Data and Polynomial Fit')\nplt.legend()\nplt.show()\n\nTraining cost: 104.19514186498827\n[[3.51715242]\n [3.51715242]\n [3.51715242]\n [5.00411734]\n [8.66812878]\n [6.78627004]]\nBest regularization parameter (alpha): 0.01\nMean Squared Error on Test Data: 19.60464293191099\n\n\n\n\n\n\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()"
  },
  {
    "objectID": "posts/First Blog Post/index.html#multiple-linear-regression",
    "href": "posts/First Blog Post/index.html#multiple-linear-regression",
    "title": "Linear and Non-Linear Regression",
    "section": "Multiple Linear Regression",
    "text": "Multiple Linear Regression\nMultiple Linear Regression is a type of Linear Regression when the input has multiple features(variables).\n\nModel Representation\nSimilar to Simple Linear Regression, we have input variable(X) and output variable(Y). But the input variable has � features. Therefore, we can represent this linear model as follows;\n\\[\nY = \\beta_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\n\\]\n\\[\nY = \\beta_0x_0 + \\beta_1x_1 + \\beta_1x_2 + ... + \\beta_nx_n\n\\]\n\\[\nx_0 = 1\n\\]\nNow we can convert this eqaution to matrix form.\n\\[\nY = \\beta^TX\n\\]\nWhere,\n\\[\n\\beta = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\\\.\\\\.\\\\\\beta_n\\end{bmatrix}\n\\]\nand,\n\\[\nX = \\begin{bmatrix}x_0\\\\x_1\\\\x_2\\\\.\\\\.\\\\x_n\\end{bmatrix}\n\\]\nWe have to define the cost of the model. Cost bascially gives the error in our model. Y in above equation is the our hypothesis(approximation). We are going to define it as our hypothesis function.\n\\[\nh_\\beta(x) = \\beta^Tx\n\\]\nand the cost is,\n\\[\nJ(\\beta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\beta(x^{\\textrm{(i)}}) - y^{\\textrm{(i)}})^2\n\\]\nBy minimizing this cost function, we can get find �. We use Gradient Descent for this.\n\n\nGradient Descent\nGradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n\nStep 1\nInitialize values �0, �1,..., �� with some value. In this case we will initialize with 0.\n\n\nStep 2\nIteratively update,\n\\[\n\\beta_j := \\beta_j - \\alpha\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\n\\]\nuntil it converges.\nThis is the procedure. Here � is the learning rate. This operation \\(\\frac{\\partial}{\\partial \\beta_j} J(\\beta)\\) means we are finding partial derivate of cost with respect to each ��. This is called Gradient.\nRead this if you are unfamiliar with partial derivatives.\nIn step 2 we are changing the values of �� in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don’t want to change values of �� drastically, because we might miss the minima. That’s why we need learning rate.\n\nThe above animation illustrates the Gradient Descent method.\nBut we still didn’t find the value of �����(�). After we applying the mathematics. The step 2 becomes.\n\\[\n\\beta_j := \\beta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m (h_\\beta(x^{(i)})-y^{(i)})x_{j}^{(i)}\n\\]\nWe iteratively change values of �� according to above equation. This particular method is called Batch Gradient Descent.\n\n\n\nImplementation\nLet’s try to implement this in Python. This looks like a long procedure. But the implementation is comparitively easy since we will vectorize all the equations. If you are unfamiliar with vectorization, read this post\nWe will be using a student score dataset. In this particular dataset, we have math, reading and writing exam scores of 1000 students. We will try to find a predict the score of writing exam from math and reading scores. You can get this dataset from this Github Repo. That’s we have 2 features(input variables). Let’s start by importing our dataset.\n\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndata = pd.read_csv('student.csv')\nprint(data.shape)\ndata.head()\n\n(1000, 3)\n\n\n\n\n\n\n\n\n\nMath\nReading\nWriting\n\n\n\n\n0\n48\n68\n63\n\n\n1\n62\n81\n72\n\n\n2\n79\n80\n78\n\n\n3\n76\n83\n79\n\n\n4\n59\n64\n62\n\n\n\n\n\n\n\nWe will get scores to an array.\n\nmath = data['Math'].values\nread = data['Reading'].values\nwrite = data['Writing'].values\n\n# Ploting the scores as scatter plot\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(math, read, write, color='#ef1234')\nplt.show()\n\n&lt;Figure size 1920x960 with 0 Axes&gt;\n\n\nNow we will generate our X, Y and \\(\\beta\\)\n\nm = len(math)\nx0 = np.ones(m)\nX = np.array([x0, math, read]).T\n# Initial Coefficients\nB = np.array([0, 0, 0])\nY = np.array(write)\nalpha = 0.0001\n\nWe’ll define our cost function.\n\ndef cost_function(X, Y, B):\n    m = len(Y)\n    J = np.sum((X.dot(B) - Y) ** 2)/(2 * m)\n    return J\n\n\ninital_cost = cost_function(X, Y, B)\nprint(inital_cost)\n\n2470.11\n\n\nAs you can see our initial cost is huge. Now we’ll reduce our cost prediocally using Gradient Descent.\nHypothesis: \\(h_\\beta(x) = \\beta^Tx\\)\nLoss: \\((h_\\beta(x)-y)\\)\nGradient: \\((h_\\beta(x)-y)x_{j}\\)\nGradient Descent Updation: \\(\\beta_j := \\beta_j - \\alpha(h_\\beta(x)-y)x_{j})\\)\n\ndef gradient_descent(X, Y, B, alpha, iterations):\n    cost_history = [0] * iterations\n    m = len(Y)\n    \n    for iteration in range(iterations):\n        # Hypothesis Values\n        h = X.dot(B)\n        # Difference b/w Hypothesis and Actual Y\n        loss = h - Y\n        # Gradient Calculation\n        gradient = X.T.dot(loss) / m\n        # Changing Values of B using Gradient\n        B = B - alpha * gradient\n        # New Cost Value\n        cost = cost_function(X, Y, B)\n        cost_history[iteration] = cost\n        \n    return B, cost_history\n\nNow we will compute final value of \\(\\beta\\)\n\n# 100000 Iterations\nnewB, cost_history = gradient_descent(X, Y, B, alpha, 100000)\n\n# New Values of B\nprint(newB)\n\n# Final Cost of new B\nprint(cost_history[-1])\n\n[-0.47889172  0.09137252  0.90144884]\n10.475123473539167\n\n\nWe can say that in this model,\n\\[\nS_{writing} = -0.47889172 + 0.09137252 * S_{math} + 0.90144884 * S_{reading}\n\\]\nThere we have final hypothesis function of our model. Let’s calculate RMSE and �2 Score of our model to evaluate.\n\n# Model Evaluation - RMSE\ndef rmse(Y, Y_pred):\n    rmse = np.sqrt(sum((Y - Y_pred) ** 2) / len(Y))\n    return rmse\n\n# Model Evaluation - R2 Score\ndef r2_score(Y, Y_pred):\n    mean_y = np.mean(Y)\n    ss_tot = sum((Y - mean_y) ** 2)\n    ss_res = sum((Y - Y_pred) ** 2)\n    r2 = 1 - (ss_res / ss_tot)\n    return r2\n\nY_pred = X.dot(newB)\n\nprint(rmse(Y, Y_pred))\nprint(r2_score(Y, Y_pred))\n\n4.577143972727788\n0.9097223273061554\n\n\nWe have very low value of RMSE score and a good �2 score. I guess our model was pretty good.\nNow we will implement this model using scikit-learn.\n\n\nThe scikit-learn Approach\nscikit-learn approach is very similar to Simple Linear Regression Model and simple too. Let’s implement this.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# X and Y\nX = np.array([math, read]).T\nY = np.array(write)\n\n# Model Intialization\nreg = LinearRegression()\n# Data Fitting\nreg = reg.fit(X, Y)\n# Y Prediction\nY_pred = reg.predict(X)\n\n# Model Evaluation\nrmse = np.sqrt(mean_squared_error(Y, Y_pred))\nr2 = reg.score(X, Y)\n\nprint(rmse)\nprint(r2)\n\n4.572887051836439\n0.9098901726717316\n\n\nYou can see that this model is better than one which we have built from scratch by a small margin.\nThat’s it for Linear Regression. I assume, so far you have understood Linear Regression, Ordinary Least Square Method and Gradient Descent."
  },
  {
    "objectID": "posts/Second Blog Post/index.html#binary-classification-for-machine-learning",
    "href": "posts/Second Blog Post/index.html#binary-classification-for-machine-learning",
    "title": "Classification",
    "section": "Binary Classification for Machine Learning",
    "text": "Binary Classification for Machine Learning\nA binary classification refers to those tasks which can give either of any two class labels as the output. Generally, one is considered as the normal state and the other is considered to be the abnormal state.  The following examples will help you to understand them better.\n\nEmail Spam detection: Normal State – Not Spam, Abnormal State – Spam\nConversion prediction: Normal State – Not churned, Abnormal State – Churn\nConversion Prediction: Normal State – Bought an item, Abnormal State – Not bought an item\n\nYou can also add the example of that \" No cancer detected\" to be a normal state and \" Cancer detected\" to be the abnormal state. The notation mostly followed is that the normal state gets assigned the value of 0 and the class with the abnormal state gets assigned the value of 1. For each example, one can also create a model which predicts the Bernoulli probability for the output. You can read more about the probability here. In short, it returns a discrete value that covers all cases and will give the output as either the outcome will have a value of 1 or 0. Hence after the association to two different states, the model can give an output for either of the values present."
  },
  {
    "objectID": "posts/Third Blog Post/index.html#types-of-clustering-algorithms",
    "href": "posts/Third Blog Post/index.html#types-of-clustering-algorithms",
    "title": "Clustering",
    "section": "Types of clustering algorithms",
    "text": "Types of clustering algorithms\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\nDensity-based\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\nThe great thing about this is that the clusters can be any shape. You aren’t constrained to expected conditions.\nThe clustering algorithms under this type don’t try to assign outliers to clusters, so they get ignored.\n\n\nDistribution-based\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\nIf you aren’t sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\n\nCentroid-based\nCentroid-based clustering is the one you probably hear about the most. It’s a little sensitive to the initial parameters you give it, but it’s fast and efficient.\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\n\nHierarchical-based\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\nThis is more restrictive than the other clustering types, but it’s perfect for specific kinds of data sets.\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you’ll get invaluable insight on your data. You might find connections you never would have thought of.\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning."
  },
  {
    "objectID": "posts/Third Blog Post/index.html#k-means-clustering-algorithm",
    "href": "posts/Third Blog Post/index.html#k-means-clustering-algorithm",
    "title": "Clustering",
    "section": "K-means clustering algorithm",
    "text": "K-means clustering algorithm\nK-means clustering is the most commonly used clustering algorithm. It’s a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThis algorithm tries to minimize the variance of data points within a cluster. It’s also how most people are introduced to unsupervised machine learning.\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it’ll take more time to classify data points if there are a large amount of them in the data set.\nSince this is how k-means clusters data points, it doesn’t scale well.\n\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=3)\n\n\n\n\n\n\n\nDBSCAN clustering algorithm\nDBSCAN stands for density-based spatial clustering of applications with noise. It’s a density-based clustering algorithm, unlike k-means.\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\nChoosing the right initial parameters is critical for this algorithm to work.\n\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAgglomerative Hierarchy clustering algorithm\nThis is the most common type of hierarchical clustering algorithm. It’s used to group objects in clusters based on how similar they are to each other.\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nSpectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html",
    "href": "posts/Fourth Blog Post/index.html",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "",
    "text": "Anomaly detection is one of the most common use cases of machine learning. Finding and identifying outliers helps to prevent fraud, adversary attacks, and network intrusions that can compromise your company's future.\nIn this post, we will talk about how anomaly detection works, what machine learning techniques you can use for it, and what benefits anomaly detection with ML brings to a business."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#types-of-clustering-algorithms",
    "href": "posts/Fourth Blog Post/index.html#types-of-clustering-algorithms",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "Types of clustering algorithms",
    "text": "Types of clustering algorithms\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\nDensity-based\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\nThe great thing about this is that the clusters can be any shape. You aren’t constrained to expected conditions.\nThe clustering algorithms under this type don’t try to assign outliers to clusters, so they get ignored.\n\n\nDistribution-based\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\nIf you aren’t sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\n\nCentroid-based\nCentroid-based clustering is the one you probably hear about the most. It’s a little sensitive to the initial parameters you give it, but it’s fast and efficient.\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\n\nHierarchical-based\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\nThis is more restrictive than the other clustering types, but it’s perfect for specific kinds of data sets.\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you’ll get invaluable insight on your data. You might find connections you never would have thought of.\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#k-means-clustering-algorithm",
    "href": "posts/Fourth Blog Post/index.html#k-means-clustering-algorithm",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "K-means clustering algorithm",
    "text": "K-means clustering algorithm\nK-means clustering is the most commonly used clustering algorithm. It’s a centroid-based algorithm and the simplest unsupervised learning algorithm.\nThis algorithm tries to minimize the variance of data points within a cluster. It’s also how most people are introduced to unsupervised machine learning.\nK-means is best used on smaller data sets because it iterates over all of the data points. That means it’ll take more time to classify data points if there are a large amount of them in the data set.\nSince this is how k-means clusters data points, it doesn’t scale well.\n\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n\n\n\n\n\n\nMini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=3)\n\n\n\n\n\n\n\nDBSCAN clustering algorithm\nDBSCAN stands for density-based spatial clustering of applications with noise. It’s a density-based clustering algorithm, unlike k-means.\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\nDBSCAN uses two parameters to determine how clusters are defined: minPts (the minimum number of data points that need to be clustered together for an area to be considered high-density) and eps (the distance used to determine if a data point is in the same area as other data points).\nChoosing the right initial parameters is critical for this algorithm to work.\n\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAgglomerative Hierarchy clustering algorithm\nThis is the most common type of hierarchical clustering algorithm. It’s used to group objects in clusters based on how similar they are to each other.\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nBIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n\n\n\n\n\n\nSpectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n    # get row indexes for samples with this cluster\n    row_ix = where(yhat == cluster)\n    # create scatter of these samples\n    pyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#what-is-an-anomaly",
    "href": "posts/Fourth Blog Post/index.html#what-is-an-anomaly",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "What is an anomaly?",
    "text": "What is an anomaly?\nBefore talking about anomaly detection, we need to understand what an anomaly is.\nGenerally speaking, an anomaly is something that differs from a norm: a deviation, an exception. In software engineering, by anomaly we understand a rare occurrence or event that doesn't fit into the pattern, and, therefore, seems suspicious. Some examples are:\n\nsudden burst or decrease in activity;\nerror in the text;\nsudden rapid drop or increase in temperature.\n\nCommon reasons for outliers are:\n\ndata preprocessing errors;\nnoise;\nfraud;\nattacks.\n\nNormally, you want to catch them all; a software program must run smoothly and be predictable so every outlier is a potential threat to its robustness and security. Catching and identifying anomalies is what we call anomaly or outlier detection.\nFor example, if large sums of money are spent one after another within one day and it is not your typical behavior, a bank can block your card. They will see an unusual pattern in your daily transactions. This anomaly can typically be connected to fraud since identity thieves try to steal as much money as they can while they can. Once an anomaly is detected, it needs to be investigated, or problems may follow."
  },
  {
    "objectID": "posts/Fourth Blog Post/index.html#why-do-you-need-machine-learning-for-anomaly-detection",
    "href": "posts/Fourth Blog Post/index.html#why-do-you-need-machine-learning-for-anomaly-detection",
    "title": "Anomaly Detection and Treatment Techniques",
    "section": "Why do you need machine learning for anomaly detection?",
    "text": "Why do you need machine learning for anomaly detection?\nThis is a process that is usually conducted with the help of statistics and machine learning tools.\nThe reason is that the majority of companies today that require outlier detection work with huge amounts of data: transactions, text, image, and video content, etc. You would have to spend days going through all the transitions that happen inside a bank every hour, and more and more are generated every second. It is simply impossible to drive any meaningful insights from this amount of data manually.\nMoreover, another difficulty is that the data is often unstructured, which means that the information wasn't arranged in any specific way for the data analysis. For example, business documents, emails, or images are examples of unstructured data.\nTo be able to collect, clean, structure, analyze, and store data, you need to use tools that aren't scared of big volumes of data. Machine learning techniques, in fact, show the best results when large data sets are involved. Machine learning algorithms are able to process most types of data. Moreover, you can choose the algorithm based on your problem and even combine different techniques for the best results.\nMachine learning used for real-world applications helps to streamline the process of anomaly detection and save the resources. It can happen not only post-factum but also in real time. Real-time anomaly detection is applied to improve security and robustness, for instance, in fraud discovery and cybersecurity.\nWhat are anomaly detection methods?\n\n\n\n\n\nThere are different kinds of anomaly detection methods with machine learning.\n\nSupervised\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\nNote: While you can label some anomalies and try to classify them (hence it's a classification task), the underlying goal of anomaly detection is defining \"normal data points\" rather than \"abnormal data points\". So in real world applications with very few anomaly samples labelled, it's almost never regarded as a supervised task.\n\n\nUnsupervised\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it's impossible to predict all the anomalies that can occur in the dataset. Think of self-driving cars, for example. They can face a situation on the road that has never happened before. Putting all road situations into a finite number of classes would be impossible. That is why neural networks are priceless when working with real-life data in real-time.\nHowever, ANNs almost rocket science level of complexity. So before you try out those, you might want to experiment with more conventional algorithms like DBSCAN, especially if your project is not that big.\nMoreover, the architecture of neural networks is a black box. We often don't know what kinds of events neural networks will label as anomalies, moreover, it can easily learn wrong rules that are not so easy to fix. That is why unsupervised anomaly detection techniques are often less trustworthy than supervised ones.\n\n\nSemi-supervised\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model's predictions more accurate.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n\n&lt;Axes: xlabel='Item_MRP', ylabel='Item_Outlet_Sales'&gt;\n\n\n\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n\n\n\n\n\n\n\n\nItem_MRP\nItem_Outlet_Sales\n\n\n\n\n0\n0.927507\n0.283587\n\n\n1\n0.072068\n0.031419\n\n\n2\n0.468288\n0.158115\n\n\n3\n0.640093\n0.053555\n\n\n4\n0.095805\n0.073651\n\n\n\n\n\n\n\n\nX1 = df['Item_MRP'].values.reshape(-1,1)\nX2 = df['Item_Outlet_Sales'].values.reshape(-1,1)\n\nX = np.concatenate((X1,X2),axis=1)\n\n\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n# Define seven outlier detection tools to be compared\nclassifiers = {\n        'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n        'Cluster-based Local Outlier Factor (CBLOF)':CBLOF(contamination=outliers_fraction,check_estimator=False, random_state=random_state),\n        'Feature Bagging':FeatureBagging(LOF(n_neighbors=35),contamination=outliers_fraction,check_estimator=False,random_state=random_state),\n        'Histogram-base Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n        'Isolation Forest': IForest(contamination=outliers_fraction,random_state=random_state),\n        'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n        'Average KNN': KNN(method='mean',contamination=outliers_fraction)\n}"
  }
]