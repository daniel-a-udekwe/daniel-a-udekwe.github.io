<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel A. Udekwe">
<meta name="dcterms.date" content="2023-11-26">

<title>About Me - Regression (Linear and Nonlinear)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">About Me</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">Blog Posts</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/daniel-udekwe" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/daniel-a-udekwe-19a2bbb2" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Regression (Linear and Nonlinear)</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">data</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Daniel A. Udekwe </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 26, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link active" data-scroll-target="#simple-linear-regression">Simple Linear Regression</a>
  <ul class="collapse">
  <li><a href="#model-representation" id="toc-model-representation" class="nav-link" data-scroll-target="#model-representation">Model Representation</a></li>
  <li><a href="#ordinary-least-square-method" id="toc-ordinary-least-square-method" class="nav-link" data-scroll-target="#ordinary-least-square-method">Ordinary Least Square Method</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent">Gradient Descent</a></li>
  </ul></li>
  <li><a href="#nonlinear-regression" id="toc-nonlinear-regression" class="nav-link" data-scroll-target="#nonlinear-regression">Nonlinear Regression</a>
  <ul class="collapse">
  <li><a href="#implementation-1" id="toc-implementation-1" class="nav-link" data-scroll-target="#implementation-1">Implementation</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/daniel-a-udekwe/daniel-a-udekwe.github.io/blob/main/posts/First Blog Post/index.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<p>Regression is a statistical technique in machine learning and statistics that aims to establish a relationship between a <strong>dependent variable (target)</strong> and one or more <strong>independent variables (features or predictors)</strong>. The primary goal of regression analysis is to model the relationship between these variables, enabling the prediction or estimation of the dependent variable based on the values of the independent variables.</p>
<p>In simpler terms, regression helps us understand how the changes in one or more variables are associated with changes in another variable. It is widely used for prediction and forecasting, making it a fundamental tool in various fields, including finance, economics, biology, and social sciences.</p>
<p>There are several types of regression models, with linear regression being one of the most common. In linear regression, the relationship between the variables is modeled as a linear equation, representing a straight line on a graph. However, regression is not limited to linear relationships; non-linear regression models can capture more complex patterns, accommodating scenarios where the relationship between variables is curved or follows a different pattern.</p>
<p>The training process in regression involves fitting the model to historical data, allowing the algorithm to learn the underlying patterns. Once trained, the regression model can be used to make predictions on new, unseen data, providing valuable insights and aiding decision-making processes.</p>
<p>In this post, we will consider the implementation of linear and nonlinear regression for predicting house prices based on size</p>
<section id="simple-linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="simple-linear-regression">Simple Linear Regression</h2>
<p>Simple Linear Regression is a fundamental and straightforward form of regression analysis where the relationship between two variables is modeled using a linear equation. In this case, there are two variables: one is considered the independent variable (often denoted as <span class="math inline">\(X\)</span>), and the other is the dependent variable (often denoted as <span class="math inline">\(Y\)</span>).</p>
<section id="model-representation" class="level3">
<h3 class="anchored" data-anchor-id="model-representation">Model Representation</h3>
<p>The equation for simple linear regression is represented as:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \epsilon\]</span></p>
<p>The <span class="math inline">\(\beta_1\)</span> is called a scale factor or <strong>coefficient</strong> and <span class="math inline">\(\beta_0\)</span> is called <strong>bias coefficient</strong>. The bias coefficient gives an extra degree of freedom to this model and <span class="math inline">\(\epsilon\)</span> is the error term accounting for unobserved factors that affect Y but are not accounted for by the model</p>
<p>This equation is similar to the line equation <span class="math inline">\(y = mx +b\)</span> with <span class="math inline">\(m = \beta_1\)</span>(Slope) and <span class="math inline">\(b = \beta_0\)</span>(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.</p>
<p>But how do we find these coefficients? That’s the learning procedure. We can find these using different approaches. One is called <strong>Ordinary Least Square Method</strong> and other one is called <strong>Gradient Descent Approach</strong>.</p>
</section>
<section id="ordinary-least-square-method" class="level3">
<h3 class="anchored" data-anchor-id="ordinary-least-square-method">Ordinary Least Square Method</h3>
<p>The Ordinary Least Squares (OLS) method is a common approach used in linear regression to estimate the parameters of the linear equation by minimizing the sum of the squared differences between the observed and predicted values of the dependent variable. In simple terms, OLS aims to find the best-fitting line through the data points.</p>
<p>Let’s say we have few inputs and outputs plotted in a 2D space with a scatter plot to yield the following image:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regression_1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>For a simple linear regression model with the equation given above, the OLS method seeks to find the values <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> of the linear model that minimize the sum of squared residuals.</p>
<p>A good model will always have least error and we can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regression_2.jpeg" class="img-fluid figure-img"></p>
</figure>
</div>
<p>And total error of this model is the sum of all errors of each point. ie.</p>
<p><span class="math display">\[
D = \sum_{i=1}^{m} d_i^2
\]</span></p>
<p><span class="math inline">\(d_i\)</span> - Distance between line and i<sup>th</sup> point.</p>
<p><span class="math inline">\(m\)</span> - Total number of points</p>
<p>You may have observed that we are taking the square of each distance. This is done because certain points lie above the line while others lie below it. By minimizing D, we aim to reduce the error in the model.</p>
<p><span class="math display">\[
\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}
\]</span></p>
<p><span class="math display">\[
\beta_0 = \bar{y} - \beta_1\bar{x}
\]</span></p>
<p>In these equations <span class="math inline">\(\bar{x}\)</span> is the mean value of input variable <span class="math inline">\(x\)</span> and <span class="math inline">\(\bar{y}\)</span> is the mean value of output variable <span class="math inline">\(y\)</span></p>
<p>Now we have the <em>Ordinary Least Square Method</em> which is described with the following equations</p>
<p><span class="math display">\[
Y = \beta_0 + \beta_1X
\]</span></p>
<p><span class="math display">\[
\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}
\]</span></p>
<p><span class="math display">\[
\beta_0 = \bar{y} - \beta_1\bar{x}
\]</span></p>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>We are going to use a dataset containing the price of houses as a function of size to implement regression. This data is split into 3 for training, validation and finally testing. Let’s start off by importing and viewing the data.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Import the needed libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'assignment1.json'</span>, <span class="st">'r'</span>) <span class="im">as</span> json_file:</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.load(json_file)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array(data[<span class="st">'X_train'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array(data[<span class="st">'Y_train'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> np.array(data[<span class="st">'X_val'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> np.array(data[<span class="st">'Y_val'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array(data[<span class="st">'X_test'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.array(data[<span class="st">'Y_test'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>array1 <span class="op">=</span> data[<span class="st">'X_train'</span>]</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>array2 <span class="op">=</span> data[<span class="st">'Y_train'</span>]</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>array3 <span class="op">=</span> data[<span class="st">'X_val'</span>]</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>array4 <span class="op">=</span> data[<span class="st">'Y_val'</span>]</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>array5 <span class="op">=</span> data[<span class="st">'X_test'</span>]</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>array6 <span class="op">=</span> data[<span class="st">'Y_test'</span>]</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training Data"</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>tableData <span class="op">=</span> {</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_train'</span>: array1,</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_train'</span>: array2,</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> pd.DataFrame(tableData)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table.head(<span class="dv">3</span>))</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Validation Data"</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>tableData2 <span class="op">=</span> {</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_Val'</span>: array3,</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_Val'</span>: array4</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>table2 <span class="op">=</span> pd.DataFrame(tableData2)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table2.head(<span class="dv">3</span>))</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table2<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test Data"</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>tableData3 <span class="op">=</span> {</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_test'</span>: array5,</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_test'</span>: array6</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>table3 <span class="op">=</span> pd.DataFrame(tableData3)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table3.head(<span class="dv">3</span>))</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table3<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training Data
   X_train  Y_train
0    661.5     4.25
1    465.0     2.30
2   1442.0    68.00
number of rows and colums: (12, 2)

Validation Data
    X_Val   Y_Val
0   650.0   4.170
1   682.5   4.067
2  1417.0  31.870
number of rows and colums: (21, 2)

Test Data
   X_test  Y_test
0   405.0   3.316
1   330.0   5.400
2   135.0   1.300
number of rows and colums: (21, 2)</code></pre>
</div>
</div>
<p>As we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.</p>
<p>we need to implement feature scaling.</p>
<p>Feature scaling is a preprocessing step in machine learning that involves adjusting the scale of the input features to a similar range. The goal is to ensure that all features contribute equally to the model training process, preventing certain features from dominating due to their larger scales.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Preprocess the data (feature scaling)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(X_train)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.std(X_train)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> (X_train <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> (X_val <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> (X_test <span class="op">-</span> mean) <span class="op">/</span> std</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(array1, array2, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.scatter(array3, array4, label<span class="op">=</span><span class="st">'validation data'</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.scatter(array5, array6, label<span class="op">=</span><span class="st">'testing data'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'House Price'</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'House size'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training, Validation and Testing Data'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>Text(0.5, 1.0, 'Training, Validation and Testing Data')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-2.png" width="585" height="449"></p>
</div>
</div>
<p>Lets create a function to implement linear regression with L2 regularization.</p>
<p>Linear regression with regularization is an extension of traditional linear regression that incorporates regularization techniques to prevent overfitting and improve the model’s generalization performance. The two common types of regularization used in linear regression are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization).</p>
<p><span class="math display">\[
J(\theta) = \frac{1}{2m} (\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2) + \frac{\lambda}{2m}(\sum_{j=1}^n \theta_j^2)
\]</span></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Implement regularized linear regression with gradient descent</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ridge_regression(X, y, alpha, num_iterations, learning_rate):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> X.shape</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize theta with the correct shape</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n, <span class="dv">1</span>))</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> []</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> (X.T <span class="op">@</span> (X <span class="op">@</span> theta <span class="op">-</span> y) <span class="op">+</span> alpha <span class="op">*</span> theta) <span class="op">/</span> m</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> learning_rate <span class="op">*</span> gradient</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> np.<span class="bu">sum</span>((X <span class="op">@</span> theta <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m) <span class="op">+</span> (alpha <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m)) <span class="op">*</span> np.<span class="bu">sum</span>(theta[<span class="dv">1</span>:] <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        history.append(cost)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, history, cost</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the equation above, <span class="math inline">\(\lambda\)</span> is the regularization parameter which ensures a balance between the trade-off between fitting the training data well and keeping the model simple. This is implemented with the gradient descent algorithm.</p>
</section>
<section id="gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="gradient-descent">Gradient Descent</h3>
<p>Gradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.</p>
<section id="step-1" class="level4">
<h4 class="anchored" data-anchor-id="step-1">Step 1</h4>
<p>Initialize values <span class="math inline">\(\theta_0, \theta_1, ..., \theta_n\)</span> with some value. In this case we will initialize with 0.</p>
</section>
<section id="step-2" class="level4">
<h4 class="anchored" data-anchor-id="step-2">Step 2</h4>
<p>Iteratively update,</p>
<p><span class="math display">\[
\theta_j : \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
\]</span></p>
<p>until it converges, where:</p>
<p><span class="math display">\[
\frac{\partial J(\theta)}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \ j = 0
\]</span></p>
<p><span class="math display">\[
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j \ j\ge 1
\]</span></p>
<p>This is the procedure. Here <span class="math inline">\(\alpha\)</span> is the learning rate. This operation <span class="math inline">\(\frac{\partial}{\partial \theta_j} J(\theta)\)</span> means we are finding partial derivative of cost with respect to each <span class="math inline">\(\theta_j\)</span>. This is called Gradient.</p>
<p>In step 2 we are changing the values of <span class="math inline">\(\theta_j\)</span>. in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don’t want to change values of <span class="math inline">\(\theta_j\)</span>. drastically, because we might miss the minima. That’s why we need learning rate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="regression _4.gif" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Animation illustrating the gradient descent method</figcaption>
</figure>
</div>
<p>The above animation illustrates the Gradient Descent method.</p>
<p>After making substitutions, Step 2 becomes:</p>
<p><span class="math display">\[
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j
\]</span></p>
<p>We iteratively change values of <span class="math inline">\(\theta_j\)</span> according to above equation. This particular method is called <strong>Batch Gradient Descent</strong>.</p>
<p>Then we need to implement a function to plot the line of best fit</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_data_and_fit(X_train, y_train, theta):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_train, y_train, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> np.linspace(<span class="bu">min</span>(X_train), <span class="bu">max</span>(X_train), <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    x_values_extended <span class="op">=</span> np.column_stack((np.ones_like(x_values), x_values))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    y_values <span class="op">=</span> x_values_extended <span class="op">@</span> theta</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Line of best fit'</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Training Data and Line of Best Fit'</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next we will train the model using the training data and return the cost using the equations below:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Train the model using the training data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">0</span> <span class="co"># Regularization strength (adjust as needed)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X_train_extended <span class="op">=</span> np.column_stack((np.ones_like(X_train), X_train))</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>theta, history, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"the cost iss: </span><span class="sc">{</span>cost<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>the cost iss: 86.7594520168758</code></pre>
</div>
</div>
<p>Then tuning the regularization parameter</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Tune the regularization parameter using the validation data</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>mse_val <span class="op">=</span> []</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    X_val_extended <span class="op">=</span> np.column_stack((np.ones_like(X_val), X_val))</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    theta_val, history_, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> X_val_extended <span class="op">@</span> theta_val</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((y_val <span class="op">-</span> y_val_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    mse_val.append(mse)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> alphas[np.argmin(mse_val)]</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best regularization parameter (alpha): </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best regularization parameter (alpha): 10</code></pre>
</div>
</div>
<p>Evaluating the Model on the Testing data using the root mean square of errors.</p>
<p>Root Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,</p>
<p><span class="math display">\[
RMSE = \sqrt{\sum_{i=1}^m \frac{1}{m}(\hat{y_1}-y_i)^2)}
\]</span></p>
<p>Here <span class="math inline">\(\hat{y_i}\)</span> is the <span class="math inline">\(i^{th}\)</span> predicted output values.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Evaluate the model using the testing data</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>X_test_extended <span class="op">=</span> np.column_stack((np.ones_like(X_test), X_test))</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>theta_test, _, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> X_test_extended <span class="op">@</span> theta_test</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>mse_test <span class="op">=</span> np.mean((y_test <span class="op">-</span> y_test_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error on Test Data: </span><span class="sc">{</span>mse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cost)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean Squared Error on Test Data: 81.66385694629241
234.9787954527312</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the cost history during training</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(num_iterations), history)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Cost vs. Iteration'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="593" height="449"></p>
</div>
</div>
<p>Plotting the training data and line of best fit</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data and line of best fit</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plot_data_and_fit(X_train, y_train, theta)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-1.png" width="596" height="449"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[[22.10833333]
 [20.73627522]]</code></pre>
</div>
</div>
<p><span class="math display">\[
House Price = 22.108 + 20.736 \times House size
\]</span></p>
<hr>
</section>
</section>
</section>
<section id="nonlinear-regression" class="level2">
<h2 class="anchored" data-anchor-id="nonlinear-regression">Nonlinear Regression</h2>
<p>Nonlinear regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as a nonlinear function. In contrast to linear regression, which assumes a linear relationship between variables, nonlinear regression allows for more complex and curved relationships to be captured.</p>
<p>The general form of nonlinear regression is expressed as:</p>
<p><span class="math display">\[
Y = f(X, \theta) + \epsilon
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is the dependent variable</p></li>
<li><p><span class="math inline">\(X\)</span> is the independent variable(s)</p></li>
<li><p><span class="math inline">\(\theta\)</span> represents the parameters of the nonlinear function <span class="math inline">\(f\)</span></p></li>
<li><p><span class="math inline">\(\epsilon\)</span> is the error term</p></li>
</ul>
<p>The goal of nonlinear regression is to estimate the parameters <span class="math inline">\((\theta)\)</span> of the chosen nonlinear function in a way that minimizes the sum of squared differences between the predicted values and the actual observed values. This is typically done using optimization techniques, such as gradient descent or other numerical optimization algorithms.</p>
<section id="implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-1">Implementation</h3>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Train the model using the training data</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Regularization strength (adjust as needed)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>degree <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Degree of the polynomial features</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create polynomial features</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> np.column_stack([X_train <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using polynomial features</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> np.column_stack((np.ones_like(X_train_poly), X_train_poly))</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>theta, history_train, cost_train <span class="op">=</span> ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training cost: </span><span class="sc">{</span>cost_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Tune the regularization parameter using the validation data</span></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>mse_val <span class="op">=</span> []</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create polynomial features for validation data</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    X_val_poly <span class="op">=</span> np.column_stack([X_val <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    X_val_poly <span class="op">=</span> np.column_stack((np.ones_like(X_val_poly), X_val_poly))</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>    theta_val, history_val, cost_val <span class="op">=</span> ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)</span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> X_val_poly <span class="op">@</span> theta_val</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((y_val <span class="op">-</span> y_val_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>    mse_val.append(mse)</span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a><span class="co">    # Plot the cost history for both training and validation</span></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')</span></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.xlabel('Iteration')</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.ylabel('Cost')</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.title('Cost vs. Iteration')</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.legend()</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.show()</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> alphas[np.argmin(mse_val)]</span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best regularization parameter (alpha): </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Evaluate the model using the testing data</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a><span class="co"># Create polynomial features for test data</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> np.column_stack([X_test <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> np.column_stack((np.ones_like(X_test_poly), X_test_poly))</span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>theta_test, _, cost_test <span class="op">=</span> ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)</span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> X_test_poly <span class="op">@</span> theta_test</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>mse_test <span class="op">=</span> np.mean((y_test <span class="op">-</span> y_test_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error on Test Data: </span><span class="sc">{</span>mse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data and the polynomial fit</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_train, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.linspace(<span class="bu">min</span>(X_train), <span class="bu">max</span>(X_train), <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>x_values_poly <span class="op">=</span> np.column_stack([x_values <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>x_values_poly <span class="op">=</span> np.column_stack((np.ones_like(x_values_poly), x_values_poly))</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>y_values <span class="op">=</span> x_values_poly <span class="op">@</span> theta</span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="ss">f'Polynomial Fit (Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb17-61"><a href="#cb17-61" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Data and Polynomial Fit'</span>)</span>
<span id="cb17-62"><a href="#cb17-62" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb17-63"><a href="#cb17-63" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Training cost: 104.19514186498827
[[3.51715242]
 [3.51715242]
 [3.51715242]
 [5.00411734]
 [8.66812878]
 [6.78627004]]
Best regularization parameter (alpha): 0.01
Mean Squared Error on Test Data: 19.60464293191099</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-2.png" width="585" height="449"></p>
</div>
</div>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(num_iterations), history_train, label<span class="op">=</span><span class="st">'Training Cost'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(num_iterations), history_val, label<span class="op">=</span><span class="st">'Validation Cost'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Cost vs. Iteration'</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" width="593" height="449"></p>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb20" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Regression (Linear and Nonlinear)"</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Daniel A. Udekwe"</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-26"</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [data, code, analysis]</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "regression.png"</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>Regression is a statistical technique in machine learning and statistics that aims to establish a relationship between a **dependent variable (target)** and one or more **independent variables (features or predictors)**. The primary goal of regression analysis is to model the relationship between these variables, enabling the prediction or estimation of the dependent variable based on the values of the independent variables.</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>In simpler terms, regression helps us understand how the changes in one or more variables are associated with changes in another variable. It is widely used for prediction and forecasting, making it a fundamental tool in various fields, including finance, economics, biology, and social sciences.</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>There are several types of regression models, with linear regression being one of the most common. In linear regression, the relationship between the variables is modeled as a linear equation, representing a straight line on a graph. However, regression is not limited to linear relationships; non-linear regression models can capture more complex patterns, accommodating scenarios where the relationship between variables is curved or follows a different pattern.</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>The training process in regression involves fitting the model to historical data, allowing the algorithm to learn the underlying patterns. Once trained, the regression model can be used to make predictions on new, unseen data, providing valuable insights and aiding decision-making processes.</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>In this post, we will consider the implementation of linear and nonlinear regression for predicting house prices based on size</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="fu">## Simple Linear Regression</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>Simple Linear Regression is a fundamental and straightforward form of regression analysis where the relationship between two variables is modeled using a linear equation. In this case, there are two variables: one is considered the independent variable (often denoted as $X$), and the other is the dependent variable (often denoted as $Y$).</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### Model Representation</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>The equation for simple linear regression is represented as:</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>$$Y = \beta_0 + \beta_1 X + \epsilon$$</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>The $\beta_1$ is called a scale factor or **coefficient** and $\beta_0$ is called **bias coefficient**. The bias coefficient gives an extra degree of freedom to this model and $\epsilon$ is the error term accounting for unobserved factors that affect Y but are not accounted for by the model</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>This equation is similar to the line equation $y = mx +b$ with $m = \beta_1$(Slope) and $b = \beta_0$(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>But how do we find these coefficients? That's the learning procedure. We can find these using different approaches. One is called **Ordinary Least Square Method** and other one is called **Gradient Descent Approach**.</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="fu">### Ordinary Least Square Method</span></span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>The Ordinary Least Squares (OLS) method is a common approach used in linear regression to estimate the parameters of the linear equation by minimizing the sum of the squared differences between the observed and predicted values of the dependent variable. In simple terms, OLS aims to find the best-fitting line through the data points.</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>Let's say we have few inputs and outputs plotted in a 2D space with a scatter plot to yield the following image:</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a><span class="al">![](regression_1.png)</span>{fig-align="center"}</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>For a simple linear regression model with the equation given above, the OLS method seeks to find the values $\beta_0$, $\beta_1$ of the linear model that minimize the sum of squared residuals.</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>A good model will always have least error and we can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="al">![](regression_2.jpeg)</span>{fig-align="center"}</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>And total error of this model is the sum of all errors of each point. ie.</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>D = \sum_{i=1}^{m} d_i^2</span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>$d_i$ - Distance between line and i^th^ point.</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>$m$ - Total number of points</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>You may have observed that we are taking the square of each distance. This is done because certain points lie above the line while others lie below it. By minimizing D, we aim to reduce the error in the model.</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>\beta_0 = \bar{y} - \beta_1\bar{x}</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>In these equations $\bar{x}$ is the mean value of input variable $x$ and $\bar{y}$ is the mean value of output variable $y$</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>Now we have the *Ordinary Least Square Method* which is described with the following equations</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>Y = \beta_0 + \beta_1X</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>\beta_0 = \bar{y} - \beta_1\bar{x}</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementation</span></span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>We are going to use a dataset containing the price of houses as a function of size to implement regression. This data is split into 3 for training, validation and finally testing. Let's start off by importing and viewing the data.</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a><span class="co">#Import the needed libraries</span></span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'assignment1.json'</span>, <span class="st">'r'</span>) <span class="im">as</span> json_file:</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> json.load(json_file)</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-101"><a href="#cb20-101" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.array(data[<span class="st">'X_train'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-102"><a href="#cb20-102" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> np.array(data[<span class="st">'Y_train'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-103"><a href="#cb20-103" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> np.array(data[<span class="st">'X_val'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-104"><a href="#cb20-104" aria-hidden="true" tabindex="-1"></a>y_val <span class="op">=</span> np.array(data[<span class="st">'Y_val'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-105"><a href="#cb20-105" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.array(data[<span class="st">'X_test'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-106"><a href="#cb20-106" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> np.array(data[<span class="st">'Y_test'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-107"><a href="#cb20-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-108"><a href="#cb20-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-109"><a href="#cb20-109" aria-hidden="true" tabindex="-1"></a>array1 <span class="op">=</span> data[<span class="st">'X_train'</span>]</span>
<span id="cb20-110"><a href="#cb20-110" aria-hidden="true" tabindex="-1"></a>array2 <span class="op">=</span> data[<span class="st">'Y_train'</span>]</span>
<span id="cb20-111"><a href="#cb20-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-112"><a href="#cb20-112" aria-hidden="true" tabindex="-1"></a>array3 <span class="op">=</span> data[<span class="st">'X_val'</span>]</span>
<span id="cb20-113"><a href="#cb20-113" aria-hidden="true" tabindex="-1"></a>array4 <span class="op">=</span> data[<span class="st">'Y_val'</span>]</span>
<span id="cb20-114"><a href="#cb20-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-115"><a href="#cb20-115" aria-hidden="true" tabindex="-1"></a>array5 <span class="op">=</span> data[<span class="st">'X_test'</span>]</span>
<span id="cb20-116"><a href="#cb20-116" aria-hidden="true" tabindex="-1"></a>array6 <span class="op">=</span> data[<span class="st">'Y_test'</span>]</span>
<span id="cb20-117"><a href="#cb20-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-118"><a href="#cb20-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-119"><a href="#cb20-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-120"><a href="#cb20-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Training Data"</span>)</span>
<span id="cb20-121"><a href="#cb20-121" aria-hidden="true" tabindex="-1"></a>tableData <span class="op">=</span> {</span>
<span id="cb20-122"><a href="#cb20-122" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_train'</span>: array1,</span>
<span id="cb20-123"><a href="#cb20-123" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_train'</span>: array2,</span>
<span id="cb20-124"><a href="#cb20-124" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-125"><a href="#cb20-125" aria-hidden="true" tabindex="-1"></a>table <span class="op">=</span> pd.DataFrame(tableData)</span>
<span id="cb20-126"><a href="#cb20-126" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table.head(<span class="dv">3</span>))</span>
<span id="cb20-127"><a href="#cb20-127" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-128"><a href="#cb20-128" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb20-129"><a href="#cb20-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-130"><a href="#cb20-130" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Validation Data"</span>)</span>
<span id="cb20-131"><a href="#cb20-131" aria-hidden="true" tabindex="-1"></a>tableData2 <span class="op">=</span> {</span>
<span id="cb20-132"><a href="#cb20-132" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_Val'</span>: array3,</span>
<span id="cb20-133"><a href="#cb20-133" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_Val'</span>: array4</span>
<span id="cb20-134"><a href="#cb20-134" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-135"><a href="#cb20-135" aria-hidden="true" tabindex="-1"></a>table2 <span class="op">=</span> pd.DataFrame(tableData2)</span>
<span id="cb20-136"><a href="#cb20-136" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table2.head(<span class="dv">3</span>))</span>
<span id="cb20-137"><a href="#cb20-137" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table2<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-138"><a href="#cb20-138" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb20-139"><a href="#cb20-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-140"><a href="#cb20-140" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test Data"</span>)</span>
<span id="cb20-141"><a href="#cb20-141" aria-hidden="true" tabindex="-1"></a>tableData3 <span class="op">=</span> {</span>
<span id="cb20-142"><a href="#cb20-142" aria-hidden="true" tabindex="-1"></a>  <span class="st">'X_test'</span>: array5,</span>
<span id="cb20-143"><a href="#cb20-143" aria-hidden="true" tabindex="-1"></a>  <span class="st">'Y_test'</span>: array6</span>
<span id="cb20-144"><a href="#cb20-144" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb20-145"><a href="#cb20-145" aria-hidden="true" tabindex="-1"></a>table3 <span class="op">=</span> pd.DataFrame(tableData3)</span>
<span id="cb20-146"><a href="#cb20-146" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(table3.head(<span class="dv">3</span>))</span>
<span id="cb20-147"><a href="#cb20-147" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"number of rows and colums: </span><span class="sc">{</span>table3<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-148"><a href="#cb20-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-149"><a href="#cb20-149" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-150"><a href="#cb20-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-151"><a href="#cb20-151" aria-hidden="true" tabindex="-1"></a>As we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.</span>
<span id="cb20-152"><a href="#cb20-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-153"><a href="#cb20-153" aria-hidden="true" tabindex="-1"></a>we need to implement feature scaling.</span>
<span id="cb20-154"><a href="#cb20-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-155"><a href="#cb20-155" aria-hidden="true" tabindex="-1"></a>Feature scaling is a preprocessing step in machine learning that involves adjusting the scale of the input features to a similar range. The goal is to ensure that all features contribute equally to the model training process, preventing certain features from dominating due to their larger scales.</span>
<span id="cb20-156"><a href="#cb20-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-159"><a href="#cb20-159" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-160"><a href="#cb20-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 2: Preprocess the data (feature scaling)</span></span>
<span id="cb20-161"><a href="#cb20-161" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> np.mean(X_train)</span>
<span id="cb20-162"><a href="#cb20-162" aria-hidden="true" tabindex="-1"></a>std <span class="op">=</span> np.std(X_train)</span>
<span id="cb20-163"><a href="#cb20-163" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> (X_train <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb20-164"><a href="#cb20-164" aria-hidden="true" tabindex="-1"></a>X_val <span class="op">=</span> (X_val <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb20-165"><a href="#cb20-165" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> (X_test <span class="op">-</span> mean) <span class="op">/</span> std</span>
<span id="cb20-166"><a href="#cb20-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-167"><a href="#cb20-167" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-168"><a href="#cb20-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-169"><a href="#cb20-169" aria-hidden="true" tabindex="-1"></a>Next, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first</span>
<span id="cb20-170"><a href="#cb20-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-173"><a href="#cb20-173" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-174"><a href="#cb20-174" aria-hidden="true" tabindex="-1"></a>plt.scatter(array1, array2, label<span class="op">=</span><span class="st">'training data'</span>)</span>
<span id="cb20-175"><a href="#cb20-175" aria-hidden="true" tabindex="-1"></a>plt.scatter(array3, array4, label<span class="op">=</span><span class="st">'validation data'</span>)</span>
<span id="cb20-176"><a href="#cb20-176" aria-hidden="true" tabindex="-1"></a>plt.scatter(array5, array6, label<span class="op">=</span><span class="st">'testing data'</span>)</span>
<span id="cb20-177"><a href="#cb20-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-178"><a href="#cb20-178" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'House Price'</span>)</span>
<span id="cb20-179"><a href="#cb20-179" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'House size'</span>)</span>
<span id="cb20-180"><a href="#cb20-180" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-181"><a href="#cb20-181" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training, Validation and Testing Data'</span>)</span>
<span id="cb20-182"><a href="#cb20-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-183"><a href="#cb20-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-184"><a href="#cb20-184" aria-hidden="true" tabindex="-1"></a>Lets create a function to implement linear regression with L2 regularization.</span>
<span id="cb20-185"><a href="#cb20-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-186"><a href="#cb20-186" aria-hidden="true" tabindex="-1"></a>Linear regression with regularization is an extension of traditional linear regression that incorporates regularization techniques to prevent overfitting and improve the model's generalization performance. The two common types of regularization used in linear regression are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization).</span>
<span id="cb20-187"><a href="#cb20-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-188"><a href="#cb20-188" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-189"><a href="#cb20-189" aria-hidden="true" tabindex="-1"></a>J(\theta) = \frac{1}{2m} (\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2) + \frac{\lambda}{2m}(\sum_{j=1}^n \theta_j^2)</span>
<span id="cb20-190"><a href="#cb20-190" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-191"><a href="#cb20-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-194"><a href="#cb20-194" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-195"><a href="#cb20-195" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 3: Implement regularized linear regression with gradient descent</span></span>
<span id="cb20-196"><a href="#cb20-196" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ridge_regression(X, y, alpha, num_iterations, learning_rate):</span>
<span id="cb20-197"><a href="#cb20-197" aria-hidden="true" tabindex="-1"></a>    m, n <span class="op">=</span> X.shape</span>
<span id="cb20-198"><a href="#cb20-198" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize theta with the correct shape</span></span>
<span id="cb20-199"><a href="#cb20-199" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros((n, <span class="dv">1</span>))</span>
<span id="cb20-200"><a href="#cb20-200" aria-hidden="true" tabindex="-1"></a>    history <span class="op">=</span> []</span>
<span id="cb20-201"><a href="#cb20-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-202"><a href="#cb20-202" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb20-203"><a href="#cb20-203" aria-hidden="true" tabindex="-1"></a>        gradient <span class="op">=</span> (X.T <span class="op">@</span> (X <span class="op">@</span> theta <span class="op">-</span> y) <span class="op">+</span> alpha <span class="op">*</span> theta) <span class="op">/</span> m</span>
<span id="cb20-204"><a href="#cb20-204" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">-=</span> learning_rate <span class="op">*</span> gradient</span>
<span id="cb20-205"><a href="#cb20-205" aria-hidden="true" tabindex="-1"></a>        cost <span class="op">=</span> np.<span class="bu">sum</span>((X <span class="op">@</span> theta <span class="op">-</span> y) <span class="op">**</span> <span class="dv">2</span>) <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m) <span class="op">+</span> (alpha <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> m)) <span class="op">*</span> np.<span class="bu">sum</span>(theta[<span class="dv">1</span>:] <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-206"><a href="#cb20-206" aria-hidden="true" tabindex="-1"></a>        history.append(cost)</span>
<span id="cb20-207"><a href="#cb20-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-208"><a href="#cb20-208" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta, history, cost</span>
<span id="cb20-209"><a href="#cb20-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-210"><a href="#cb20-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-211"><a href="#cb20-211" aria-hidden="true" tabindex="-1"></a>In the equation above, $\lambda$ is the regularization parameter which ensures a balance between the trade-off between fitting the training data well and keeping the model simple. This is implemented with the gradient descent algorithm.</span>
<span id="cb20-212"><a href="#cb20-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-213"><a href="#cb20-213" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gradient Descent</span></span>
<span id="cb20-214"><a href="#cb20-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-215"><a href="#cb20-215" aria-hidden="true" tabindex="-1"></a>Gradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.</span>
<span id="cb20-216"><a href="#cb20-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-217"><a href="#cb20-217" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 1</span></span>
<span id="cb20-218"><a href="#cb20-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-219"><a href="#cb20-219" aria-hidden="true" tabindex="-1"></a>Initialize values $\theta_0, \theta_1, ..., \theta_n$ with some value. In this case we will initialize with 0.</span>
<span id="cb20-220"><a href="#cb20-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-221"><a href="#cb20-221" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Step 2</span></span>
<span id="cb20-222"><a href="#cb20-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-223"><a href="#cb20-223" aria-hidden="true" tabindex="-1"></a>Iteratively update,</span>
<span id="cb20-224"><a href="#cb20-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-225"><a href="#cb20-225" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-226"><a href="#cb20-226" aria-hidden="true" tabindex="-1"></a>\theta_j : \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)</span>
<span id="cb20-227"><a href="#cb20-227" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-228"><a href="#cb20-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-229"><a href="#cb20-229" aria-hidden="true" tabindex="-1"></a>until it converges, where:</span>
<span id="cb20-230"><a href="#cb20-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-231"><a href="#cb20-231" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-232"><a href="#cb20-232" aria-hidden="true" tabindex="-1"></a>\frac{\partial J(\theta)}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \ j = 0</span>
<span id="cb20-233"><a href="#cb20-233" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-234"><a href="#cb20-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-235"><a href="#cb20-235" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-236"><a href="#cb20-236" aria-hidden="true" tabindex="-1"></a>\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j \ j\ge 1 </span>
<span id="cb20-237"><a href="#cb20-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-238"><a href="#cb20-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-239"><a href="#cb20-239" aria-hidden="true" tabindex="-1"></a>This is the procedure. Here $\alpha$ is the learning rate. This operation $\frac{\partial}{\partial \theta_j} J(\theta)$ means we are finding partial derivative of cost with respect to each $\theta_j$. This is called Gradient.</span>
<span id="cb20-240"><a href="#cb20-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-241"><a href="#cb20-241" aria-hidden="true" tabindex="-1"></a>In step 2 we are changing the values of $\theta_j$. in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don't want to change values of $\theta_j$. drastically, because we might miss the minima. That's why we need learning rate.</span>
<span id="cb20-242"><a href="#cb20-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-243"><a href="#cb20-243" aria-hidden="true" tabindex="-1"></a><span class="al">![Animation illustrating the gradient descent method](regression%20_4.gif)</span>{fig-align="center"}</span>
<span id="cb20-244"><a href="#cb20-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-245"><a href="#cb20-245" aria-hidden="true" tabindex="-1"></a>The above animation illustrates the Gradient Descent method.</span>
<span id="cb20-246"><a href="#cb20-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-247"><a href="#cb20-247" aria-hidden="true" tabindex="-1"></a>After making substitutions, Step 2 becomes:</span>
<span id="cb20-248"><a href="#cb20-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-249"><a href="#cb20-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-250"><a href="#cb20-250" aria-hidden="true" tabindex="-1"></a>\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j</span>
<span id="cb20-251"><a href="#cb20-251" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-252"><a href="#cb20-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-253"><a href="#cb20-253" aria-hidden="true" tabindex="-1"></a>We iteratively change values of $\theta_j$ according to above equation. This particular method is called **Batch Gradient Descent**.</span>
<span id="cb20-254"><a href="#cb20-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-255"><a href="#cb20-255" aria-hidden="true" tabindex="-1"></a>Then we need to implement a function to plot the line of best fit</span>
<span id="cb20-256"><a href="#cb20-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-259"><a href="#cb20-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-260"><a href="#cb20-260" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_data_and_fit(X_train, y_train, theta):</span>
<span id="cb20-261"><a href="#cb20-261" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X_train, y_train, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb20-262"><a href="#cb20-262" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-263"><a href="#cb20-263" aria-hidden="true" tabindex="-1"></a>    x_values <span class="op">=</span> np.linspace(<span class="bu">min</span>(X_train), <span class="bu">max</span>(X_train), <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-264"><a href="#cb20-264" aria-hidden="true" tabindex="-1"></a>    x_values_extended <span class="op">=</span> np.column_stack((np.ones_like(x_values), x_values))</span>
<span id="cb20-265"><a href="#cb20-265" aria-hidden="true" tabindex="-1"></a>    y_values <span class="op">=</span> x_values_extended <span class="op">@</span> theta</span>
<span id="cb20-266"><a href="#cb20-266" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-267"><a href="#cb20-267" aria-hidden="true" tabindex="-1"></a>    plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Line of best fit'</span>)</span>
<span id="cb20-268"><a href="#cb20-268" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb20-269"><a href="#cb20-269" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb20-270"><a href="#cb20-270" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Training Data and Line of Best Fit'</span>)</span>
<span id="cb20-271"><a href="#cb20-271" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb20-272"><a href="#cb20-272" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb20-273"><a href="#cb20-273" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-274"><a href="#cb20-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-275"><a href="#cb20-275" aria-hidden="true" tabindex="-1"></a>Next we will train the model using the training data and return the cost using the equations below:</span>
<span id="cb20-276"><a href="#cb20-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-279"><a href="#cb20-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-280"><a href="#cb20-280" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Train the model using the training data</span></span>
<span id="cb20-281"><a href="#cb20-281" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">0</span> <span class="co"># Regularization strength (adjust as needed)</span></span>
<span id="cb20-282"><a href="#cb20-282" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb20-283"><a href="#cb20-283" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-284"><a href="#cb20-284" aria-hidden="true" tabindex="-1"></a>X_train_extended <span class="op">=</span> np.column_stack((np.ones_like(X_train), X_train))</span>
<span id="cb20-285"><a href="#cb20-285" aria-hidden="true" tabindex="-1"></a>theta, history, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb20-286"><a href="#cb20-286" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"the cost iss: </span><span class="sc">{</span>cost<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-287"><a href="#cb20-287" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-288"><a href="#cb20-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-289"><a href="#cb20-289" aria-hidden="true" tabindex="-1"></a>Then tuning the regularization parameter</span>
<span id="cb20-290"><a href="#cb20-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-293"><a href="#cb20-293" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-294"><a href="#cb20-294" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Tune the regularization parameter using the validation data</span></span>
<span id="cb20-295"><a href="#cb20-295" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb20-296"><a href="#cb20-296" aria-hidden="true" tabindex="-1"></a>mse_val <span class="op">=</span> []</span>
<span id="cb20-297"><a href="#cb20-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-298"><a href="#cb20-298" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb20-299"><a href="#cb20-299" aria-hidden="true" tabindex="-1"></a>    X_val_extended <span class="op">=</span> np.column_stack((np.ones_like(X_val), X_val))</span>
<span id="cb20-300"><a href="#cb20-300" aria-hidden="true" tabindex="-1"></a>    theta_val, history_, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb20-301"><a href="#cb20-301" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> X_val_extended <span class="op">@</span> theta_val</span>
<span id="cb20-302"><a href="#cb20-302" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((y_val <span class="op">-</span> y_val_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-303"><a href="#cb20-303" aria-hidden="true" tabindex="-1"></a>    mse_val.append(mse)</span>
<span id="cb20-304"><a href="#cb20-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-305"><a href="#cb20-305" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> alphas[np.argmin(mse_val)]</span>
<span id="cb20-306"><a href="#cb20-306" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best regularization parameter (alpha): </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-307"><a href="#cb20-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-308"><a href="#cb20-308" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-309"><a href="#cb20-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-310"><a href="#cb20-310" aria-hidden="true" tabindex="-1"></a>Evaluating the Model on the Testing data using the root mean square of errors.</span>
<span id="cb20-311"><a href="#cb20-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-312"><a href="#cb20-312" aria-hidden="true" tabindex="-1"></a>Root Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,</span>
<span id="cb20-313"><a href="#cb20-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-314"><a href="#cb20-314" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-315"><a href="#cb20-315" aria-hidden="true" tabindex="-1"></a>RMSE = \sqrt{\sum_{i=1}^m \frac{1}{m}(\hat{y_1}-y_i)^2)}</span>
<span id="cb20-316"><a href="#cb20-316" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-317"><a href="#cb20-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-318"><a href="#cb20-318" aria-hidden="true" tabindex="-1"></a>Here $\hat{y_i}$ is the $i^{th}$ predicted output values.</span>
<span id="cb20-319"><a href="#cb20-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-322"><a href="#cb20-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-323"><a href="#cb20-323" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Evaluate the model using the testing data</span></span>
<span id="cb20-324"><a href="#cb20-324" aria-hidden="true" tabindex="-1"></a>X_test_extended <span class="op">=</span> np.column_stack((np.ones_like(X_test), X_test))</span>
<span id="cb20-325"><a href="#cb20-325" aria-hidden="true" tabindex="-1"></a>theta_test, _, cost <span class="op">=</span> ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)</span>
<span id="cb20-326"><a href="#cb20-326" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> X_test_extended <span class="op">@</span> theta_test</span>
<span id="cb20-327"><a href="#cb20-327" aria-hidden="true" tabindex="-1"></a>mse_test <span class="op">=</span> np.mean((y_test <span class="op">-</span> y_test_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-328"><a href="#cb20-328" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error on Test Data: </span><span class="sc">{</span>mse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-329"><a href="#cb20-329" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cost)</span>
<span id="cb20-330"><a href="#cb20-330" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-331"><a href="#cb20-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-334"><a href="#cb20-334" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-335"><a href="#cb20-335" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the cost history during training</span></span>
<span id="cb20-336"><a href="#cb20-336" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(num_iterations), history)</span>
<span id="cb20-337"><a href="#cb20-337" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb20-338"><a href="#cb20-338" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb20-339"><a href="#cb20-339" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Cost vs. Iteration'</span>)</span>
<span id="cb20-340"><a href="#cb20-340" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-341"><a href="#cb20-341" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-342"><a href="#cb20-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-343"><a href="#cb20-343" aria-hidden="true" tabindex="-1"></a>Plotting the training data and line of best fit</span>
<span id="cb20-344"><a href="#cb20-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-347"><a href="#cb20-347" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-348"><a href="#cb20-348" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data and line of best fit</span></span>
<span id="cb20-349"><a href="#cb20-349" aria-hidden="true" tabindex="-1"></a>plot_data_and_fit(X_train, y_train, theta)</span>
<span id="cb20-350"><a href="#cb20-350" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb20-351"><a href="#cb20-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-352"><a href="#cb20-352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-353"><a href="#cb20-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-354"><a href="#cb20-354" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-355"><a href="#cb20-355" aria-hidden="true" tabindex="-1"></a>House Price = 22.108 + 20.736 \times House size</span>
<span id="cb20-356"><a href="#cb20-356" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-357"><a href="#cb20-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-358"><a href="#cb20-358" aria-hidden="true" tabindex="-1"></a>------------------------------------------------------------------------</span>
<span id="cb20-359"><a href="#cb20-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-360"><a href="#cb20-360" aria-hidden="true" tabindex="-1"></a><span class="fu">## Nonlinear Regression</span></span>
<span id="cb20-361"><a href="#cb20-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-362"><a href="#cb20-362" aria-hidden="true" tabindex="-1"></a>Nonlinear regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as a nonlinear function. In contrast to linear regression, which assumes a linear relationship between variables, nonlinear regression allows for more complex and curved relationships to be captured.</span>
<span id="cb20-363"><a href="#cb20-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-364"><a href="#cb20-364" aria-hidden="true" tabindex="-1"></a>The general form of nonlinear regression is expressed as:</span>
<span id="cb20-365"><a href="#cb20-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-366"><a href="#cb20-366" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-367"><a href="#cb20-367" aria-hidden="true" tabindex="-1"></a>Y = f(X, \theta) + \epsilon</span>
<span id="cb20-368"><a href="#cb20-368" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb20-369"><a href="#cb20-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-370"><a href="#cb20-370" aria-hidden="true" tabindex="-1"></a>Where:</span>
<span id="cb20-371"><a href="#cb20-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-372"><a href="#cb20-372" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$Y$ is the dependent variable</span>
<span id="cb20-373"><a href="#cb20-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-374"><a href="#cb20-374" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$X$ is the independent variable(s)</span>
<span id="cb20-375"><a href="#cb20-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-376"><a href="#cb20-376" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\theta$ represents the parameters of the nonlinear function $f$</span>
<span id="cb20-377"><a href="#cb20-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-378"><a href="#cb20-378" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>$\epsilon$ is the error term</span>
<span id="cb20-379"><a href="#cb20-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-380"><a href="#cb20-380" aria-hidden="true" tabindex="-1"></a>The goal of nonlinear regression is to estimate the parameters $(\theta)$ of the chosen nonlinear function in a way that minimizes the sum of squared differences between the predicted values and the actual observed values. This is typically done using optimization techniques, such as gradient descent or other numerical optimization algorithms.</span>
<span id="cb20-381"><a href="#cb20-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-382"><a href="#cb20-382" aria-hidden="true" tabindex="-1"></a><span class="fu">### Implementation</span></span>
<span id="cb20-383"><a href="#cb20-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-386"><a href="#cb20-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-387"><a href="#cb20-387" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 4: Train the model using the training data</span></span>
<span id="cb20-388"><a href="#cb20-388" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="dv">10</span>  <span class="co"># Regularization strength (adjust as needed)</span></span>
<span id="cb20-389"><a href="#cb20-389" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb20-390"><a href="#cb20-390" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-391"><a href="#cb20-391" aria-hidden="true" tabindex="-1"></a>degree <span class="op">=</span> <span class="dv">3</span>  <span class="co"># Degree of the polynomial features</span></span>
<span id="cb20-392"><a href="#cb20-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-393"><a href="#cb20-393" aria-hidden="true" tabindex="-1"></a><span class="co"># Create polynomial features</span></span>
<span id="cb20-394"><a href="#cb20-394" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> np.column_stack([X_train <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb20-395"><a href="#cb20-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-396"><a href="#cb20-396" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model using polynomial features</span></span>
<span id="cb20-397"><a href="#cb20-397" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> np.column_stack((np.ones_like(X_train_poly), X_train_poly))</span>
<span id="cb20-398"><a href="#cb20-398" aria-hidden="true" tabindex="-1"></a>theta, history_train, cost_train <span class="op">=</span> ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)</span>
<span id="cb20-399"><a href="#cb20-399" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Training cost: </span><span class="sc">{</span>cost_train<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-400"><a href="#cb20-400" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(theta)</span>
<span id="cb20-401"><a href="#cb20-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-402"><a href="#cb20-402" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 5: Tune the regularization parameter using the validation data</span></span>
<span id="cb20-403"><a href="#cb20-403" aria-hidden="true" tabindex="-1"></a>alphas <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</span>
<span id="cb20-404"><a href="#cb20-404" aria-hidden="true" tabindex="-1"></a>mse_val <span class="op">=</span> []</span>
<span id="cb20-405"><a href="#cb20-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-406"><a href="#cb20-406" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> alphas:</span>
<span id="cb20-407"><a href="#cb20-407" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create polynomial features for validation data</span></span>
<span id="cb20-408"><a href="#cb20-408" aria-hidden="true" tabindex="-1"></a>    X_val_poly <span class="op">=</span> np.column_stack([X_val <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb20-409"><a href="#cb20-409" aria-hidden="true" tabindex="-1"></a>    X_val_poly <span class="op">=</span> np.column_stack((np.ones_like(X_val_poly), X_val_poly))</span>
<span id="cb20-410"><a href="#cb20-410" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-411"><a href="#cb20-411" aria-hidden="true" tabindex="-1"></a>    theta_val, history_val, cost_val <span class="op">=</span> ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)</span>
<span id="cb20-412"><a href="#cb20-412" aria-hidden="true" tabindex="-1"></a>    y_val_pred <span class="op">=</span> X_val_poly <span class="op">@</span> theta_val</span>
<span id="cb20-413"><a href="#cb20-413" aria-hidden="true" tabindex="-1"></a>    mse <span class="op">=</span> np.mean((y_val <span class="op">-</span> y_val_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-414"><a href="#cb20-414" aria-hidden="true" tabindex="-1"></a>    mse_val.append(mse)</span>
<span id="cb20-415"><a href="#cb20-415" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb20-416"><a href="#cb20-416" aria-hidden="true" tabindex="-1"></a><span class="co">    # Plot the cost history for both training and validation</span></span>
<span id="cb20-417"><a href="#cb20-417" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')</span></span>
<span id="cb20-418"><a href="#cb20-418" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')</span></span>
<span id="cb20-419"><a href="#cb20-419" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.xlabel('Iteration')</span></span>
<span id="cb20-420"><a href="#cb20-420" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.ylabel('Cost')</span></span>
<span id="cb20-421"><a href="#cb20-421" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.title('Cost vs. Iteration')</span></span>
<span id="cb20-422"><a href="#cb20-422" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.legend()</span></span>
<span id="cb20-423"><a href="#cb20-423" aria-hidden="true" tabindex="-1"></a><span class="co">    plt.show()</span></span>
<span id="cb20-424"><a href="#cb20-424" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb20-425"><a href="#cb20-425" aria-hidden="true" tabindex="-1"></a>best_alpha <span class="op">=</span> alphas[np.argmin(mse_val)]</span>
<span id="cb20-426"><a href="#cb20-426" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best regularization parameter (alpha): </span><span class="sc">{</span>best_alpha<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-427"><a href="#cb20-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-428"><a href="#cb20-428" aria-hidden="true" tabindex="-1"></a><span class="co"># Step 6: Evaluate the model using the testing data</span></span>
<span id="cb20-429"><a href="#cb20-429" aria-hidden="true" tabindex="-1"></a><span class="co"># Create polynomial features for test data</span></span>
<span id="cb20-430"><a href="#cb20-430" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> np.column_stack([X_test <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb20-431"><a href="#cb20-431" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> np.column_stack((np.ones_like(X_test_poly), X_test_poly))</span>
<span id="cb20-432"><a href="#cb20-432" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-433"><a href="#cb20-433" aria-hidden="true" tabindex="-1"></a>theta_test, _, cost_test <span class="op">=</span> ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)</span>
<span id="cb20-434"><a href="#cb20-434" aria-hidden="true" tabindex="-1"></a>y_test_pred <span class="op">=</span> X_test_poly <span class="op">@</span> theta_test</span>
<span id="cb20-435"><a href="#cb20-435" aria-hidden="true" tabindex="-1"></a>mse_test <span class="op">=</span> np.mean((y_test <span class="op">-</span> y_test_pred) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb20-436"><a href="#cb20-436" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean Squared Error on Test Data: </span><span class="sc">{</span>mse_test<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb20-437"><a href="#cb20-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-438"><a href="#cb20-438" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot training data and the polynomial fit</span></span>
<span id="cb20-439"><a href="#cb20-439" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_train, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Training data'</span>)</span>
<span id="cb20-440"><a href="#cb20-440" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.linspace(<span class="bu">min</span>(X_train), <span class="bu">max</span>(X_train), <span class="dv">100</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb20-441"><a href="#cb20-441" aria-hidden="true" tabindex="-1"></a>x_values_poly <span class="op">=</span> np.column_stack([x_values <span class="op">**</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>)])</span>
<span id="cb20-442"><a href="#cb20-442" aria-hidden="true" tabindex="-1"></a>x_values_poly <span class="op">=</span> np.column_stack((np.ones_like(x_values_poly), x_values_poly))</span>
<span id="cb20-443"><a href="#cb20-443" aria-hidden="true" tabindex="-1"></a>y_values <span class="op">=</span> x_values_poly <span class="op">@</span> theta</span>
<span id="cb20-444"><a href="#cb20-444" aria-hidden="true" tabindex="-1"></a>plt.plot(x_values, y_values, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="ss">f'Polynomial Fit (Degree </span><span class="sc">{</span>degree<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb20-445"><a href="#cb20-445" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb20-446"><a href="#cb20-446" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb20-447"><a href="#cb20-447" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Training Data and Polynomial Fit'</span>)</span>
<span id="cb20-448"><a href="#cb20-448" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb20-449"><a href="#cb20-449" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb20-450"><a href="#cb20-450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb20-451"><a href="#cb20-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-454"><a href="#cb20-454" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb20-455"><a href="#cb20-455" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(num_iterations), history_train, label<span class="op">=</span><span class="st">'Training Cost'</span>, color<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb20-456"><a href="#cb20-456" aria-hidden="true" tabindex="-1"></a>    plt.plot(<span class="bu">range</span>(num_iterations), history_val, label<span class="op">=</span><span class="st">'Validation Cost'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb20-457"><a href="#cb20-457" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'Iteration'</span>)</span>
<span id="cb20-458"><a href="#cb20-458" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Cost'</span>)</span>
<span id="cb20-459"><a href="#cb20-459" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Cost vs. Iteration'</span>)</span>
<span id="cb20-460"><a href="#cb20-460" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb20-461"><a href="#cb20-461" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb20-462"><a href="#cb20-462" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2023, Danny_UDK - All rights reserved</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This website is built by Daniel Udekwe with <a href="https://quarto.org/">Quarto</a></div>
  </div>
</footer>



</body></html>