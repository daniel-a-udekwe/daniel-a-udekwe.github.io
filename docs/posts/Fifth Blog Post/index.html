<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel A. Udekwe">
<meta name="dcterms.date" content="2023-11-23">

<title>CS5805 - Machine Learning I - Probability Theory and Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS5805 - Machine Learning I</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">Blog Posts</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/daniel-udekwe" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/daniel-a-udekwe-19a2bbb2" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">Probability Theory and Random Variables</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">data</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Daniel A. Udekwe </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 23, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-probability-theory" id="toc-introduction-to-probability-theory" class="nav-link active" data-scroll-target="#introduction-to-probability-theory">Introduction to Probability Theory</a>
  <ul class="collapse">
  <li><a href="#set-theory" id="toc-set-theory" class="nav-link" data-scroll-target="#set-theory">Set Theory</a></li>
  <li><a href="#events" id="toc-events" class="nav-link" data-scroll-target="#events">Events</a></li>
  <li><a href="#probability" id="toc-probability" class="nav-link" data-scroll-target="#probability">Probability</a></li>
  </ul></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables">Random Variables</a></li>
  <li><a href="#probability-distribution" id="toc-probability-distribution" class="nav-link" data-scroll-target="#probability-distribution">Probability Distribution</a>
  <ul class="collapse">
  <li><a href="#probability-mass-function" id="toc-probability-mass-function" class="nav-link" data-scroll-target="#probability-mass-function"><strong>Probability Mass Function</strong></a>
  <ul class="collapse">
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution">Bernoulli Distribution</a></li>
  <li><a href="#binomial-distribution" id="toc-binomial-distribution" class="nav-link" data-scroll-target="#binomial-distribution">Binomial Distribution</a></li>
  <li><a href="#geometric-distribution" id="toc-geometric-distribution" class="nav-link" data-scroll-target="#geometric-distribution">Geometric Distribution</a></li>
  <li><a href="#gaussian-distribution" id="toc-gaussian-distribution" class="nav-link" data-scroll-target="#gaussian-distribution">Gaussian Distribution</a></li>
  </ul></li>
  <li><a href="#probability-density-function" id="toc-probability-density-function" class="nav-link" data-scroll-target="#probability-density-function">Probability Density Function</a></li>
  </ul></li>
  <li><a href="#applications-in-machine-learning" id="toc-applications-in-machine-learning" class="nav-link" data-scroll-target="#applications-in-machine-learning">Applications in Machine Learning</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference">Bayesian Inference</a>
  <ul class="collapse">
  <li><a href="#the-likelihood" id="toc-the-likelihood" class="nav-link" data-scroll-target="#the-likelihood"><strong>The likelihood</strong></a></li>
  <li><a href="#the-prior" id="toc-the-prior" class="nav-link" data-scroll-target="#the-prior"><strong>The prior</strong></a></li>
  <li><a href="#the-posterior" id="toc-the-posterior" class="nav-link" data-scroll-target="#the-posterior"><strong>The posterior</strong></a></li>
  </ul></li>
  <li><a href="#markov-chain-monte-carlo-mcmc" id="toc-markov-chain-monte-carlo-mcmc" class="nav-link" data-scroll-target="#markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<p>The subject of probability theory is the foundation upon which all of statistics is built, providing a means for modelling population, experiments or almost anything else that could be considered a random phenomenon. Through these models, statisticians are able to draw inferences about populations, inferences based on examination of only a part of the whole.</p>
<section id="introduction-to-probability-theory" class="level1">
<h1>Introduction to Probability Theory</h1>
<p>Probability theory is a branch of mathematics concerned with the study of random phenomena and is often considered one of the fundamental pillars of machine learning. It is however a huge field to cover and very easy to get lost in, especially when being self-taught.</p>
<p>In the following sections, we are going to cover some fundamental aspects of probability before delving into aspects that are especially relevant to machine learning — the random variable and the probability distribution.</p>
<section id="set-theory" class="level2">
<h2 class="anchored" data-anchor-id="set-theory">Set Theory</h2>
<p>The set <span class="math inline">\(S\)</span> of all possible outcomes of a particular experiment is called the sample space for the experiment. If the experiment consists of tossing a coin, the sample space contains two outcomes, heads and tails: thus,</p>
<p><span class="math display">\[
S = \{H, T\}
\]</span></p>
<p>If, on the other hand, the experiment consists of observing the reported SAT scores of randomly selected students at a certain university, the sample space would be the set of positive integers between 200 and 800 that are multiples of ten — that is, <span class="math inline">\(S = \{200, 210, 220, ..., 780, 790, 800 \}\)</span></p>
<p>We can classify sample spaces into two types according to the number of elements they contain. Sample spaces can be either <strong>countable</strong> or <strong>uncountable</strong>; If the elements of a sample space can be put into 1-1 correspondence with a subset of the integers, the sample space is countable. Of course, if the sample space contains only a finite number of elements, it is countable. Thus, the coin toss and SAT score sample is uncountable, since the positive real numbers cannot be put into 1-1 correspondence with the integers. If, however, we measured the reaction time to the nearest second, then the sample space would be (in seconds) <span class="math inline">\(S = \{0, 1, 2, 3, ... \}\)</span>, which is then countable.</p>
</section>
<section id="events" class="level2">
<h2 class="anchored" data-anchor-id="events">Events</h2>
<p>An event is any collection of possible outcomes of an experiment that is, any subset of <span class="math inline">\(S\)</span> (including <span class="math inline">\(S\)</span> itself)</p>
<p>Let A be an event, a subset of S. We say the event A occurs if the outcome of the experiment is the set A. When speaking of probabilities, we generally speak of the probability of an event, rather than a set. But we may use the terms interchangeably.</p>
<p>Given any two events (or sets) A and B, we have the following elementary set operations.</p>
<p><strong>Union</strong>: The union of A and B, written <span class="math inline">\(A\cup B\)</span> is the set of elements that belong to either A or B or both</p>
<p><span class="math display">\[
A \cup B = \{x:x \in A \ or \ x \in B \}
\]</span></p>
<p><strong>Intersection:</strong> The intersection of A and B, written <span class="math inline">\(A\cap B\)</span> is the set of elements that belong to both A and B</p>
<p><span class="math display">\[
A \cap B = \{x:x \in A \ and \ x \in B\}
\]</span></p>
<p><strong>Complementation:</strong> The complement of A, written <span class="math inline">\(A^c\)</span>, is the set of all elements that are not in A</p>
<p><span class="math display">\[
A^c = \{x:x \not\in A\}
\]</span></p>
<p>Elementary set operations can be combined, somewhat akin to the way addition and multiplication can be combined. As long as we are careful, we can treat sets as if they were numbers. We can now state the following useful properties of set operations.</p>
<ol type="1">
<li><p>Commutativity.</p>
<p>a. <span class="math inline">\(A\cup B = B \cup B\)</span>,</p>
<p>b. <span class="math inline">\(A \cap B = B \cap A\)</span></p></li>
<li><p>Assocaiativity</p>
<p>a. <span class="math inline">\(A \cup (B\cup C) = (A \cup B) \cup C\)</span></p>
<p>b. <span class="math inline">\(A \cap (B\cap C) = (A \cap B) \cap C\)</span></p></li>
<li><p>Distribution Laws</p>
<p>a. <span class="math inline">\(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\)</span></p>
<p>b. <span class="math inline">\(A \cup (B\cap C) = (A \cup B) \cap (A \cup C)\)</span></p></li>
<li><p>DeMorgan’s Laws</p>
<p>a. <span class="math inline">\((A \cup B )^C = A^C \cap B^C\)</span></p>
<p>b. <span class="math inline">\((A \cap B )^C = A^C \cup B^C\)</span></p></li>
</ol>
</section>
<section id="probability" class="level2">
<h2 class="anchored" data-anchor-id="probability">Probability</h2>
<p>Consider the simple experiment of tossing a fair coin, so <span class="math inline">\(S = \{H, T\}\)</span>. By a “fair” coin we mean a balanced coin that is equally as likely to land heads up as tails up, and hence the reasonable probability function is the one that assigns equal probabilities to heads and tails, that is</p>
<p><span class="math display">\[
P(\{H\}) = P(\{T\})
\]</span></p>
<p>Since <span class="math inline">\(S = \{H\} \cup \{T\}\)</span>, we have <span class="math inline">\(P(\{H\} \cup \{T\}) = P(\{H\}) + P(\{T\})\)</span> and</p>
<p><span class="math display">\[
P(\{H\}) + P(\{T\}) = 1
\]</span></p>
<p>Solving the equations simultaneously shows that <span class="math inline">\(P(\{H\}) = P(\{T\}) = 1/2\)</span></p>
<p>In machine learning, we often deal with uncertainty and stochastic quantities, due to one of the reasons being incomplete observability — therefore, we most likely work with sampled data.</p>
<p>Now, suppose we want to draw reliable conclusions about the behavior of a random variable, despite the fact that we only have limited data and we simply do not know the entire population.</p>
<p>Hence, we need some kind of way to generalize from the sampled data to the population, or in other words — we need to estimate the true data-generating process.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/data-generation.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Estimating the data-generating process</figcaption>
</figure>
</div>
<p>Understanding the probability distribution, allows us to compute the probability of a certain outcome by also accounting for the variability in the results. Thus, it enables us to generalize from the sample to the population, estimate the data-generating function and predict the behavior of a random variable more accurately.</p>
</section>
</section>
<section id="random-variables" class="level1">
<h1>Random Variables</h1>
<p>Loosely speaking, the random variable is a variable whose value depends on the outcome of a random event. We can also describe it as a function that maps from the sample space to a measurable space (e.g.&nbsp;a real number).</p>
<p>Let’s assume, we have a sample space containing 4 students <code>{A, B, C, D}</code>. If we now randomly pick <code>student A</code> and measure the height in centimeters, we can think of the <code>random variable (H)</code>as the function with the input of <code>student</code>and the output of <code>height</code>as a real number.</p>
<p><span class="math display">\[
H(student) = height
\]</span></p>
<p>We can visualize this small example like the following</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/random-variable.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a random variable</figcaption>
</figure>
</div>
<p>Depending on the outcome — which student is randomly picked — our random variable (H) can take on different states or different values in terms of height in centimeters.</p>
<p>A random variable can be either discrete or continuous.</p>
<p>If our random variable can take only a finite or countably infinite number of distinct values, then it is discrete. Examples of a discrete random variable include the number of students in a class, test questions answered correctly, the number of children in a family, etc.</p>
<p>Our random variable, however, is continuous if between any two values of our variable are an infinite number of other valid values. We can think of quantities such as pressure, height, mass, and distance as examples of continuous random variables.</p>
<p>When we couple our random variable with a probability distribution we can answer the following question: How likely is it for our random variable to take a specific state? Which is basically the same as asking for the probability.</p>
<p>Now, we are left with one question that remains— what is a probability distribution?</p>
</section>
<section id="probability-distribution" class="level1">
<h1>Probability Distribution</h1>
<p>The description of how likely a random variable takes one of its possible states can be given by a probability distribution. Thus, the probability distribution is a mathematical function that gives the probabilities of different outcomes for an experiment.</p>
<p>More generally it can be described as the function</p>
<p><span class="math display">\[
P:A \rightarrow R
\]</span></p>
<p>which maps an input space A — related to the sample space — to a real number, namely the probability.</p>
<p>For the above function to characterize a probability distribution, it must follow all of the <strong>Kolmogorov axioms:</strong></p>
<ol type="1">
<li><p><strong>Non</strong>-negativity</p></li>
<li><p>No probability exceeds 1</p></li>
<li><p>Additivity of any countable disjoint (mutually exclusive) events</p></li>
</ol>
<p>The way we describe a probability distribution depends on whether the random variable is discrete or continuous, which will result in a probability mass or density function respectively.</p>
<section id="probability-mass-function" class="level2">
<h2 class="anchored" data-anchor-id="probability-mass-function"><strong>Probability Mass Function</strong></h2>
<p>The probability mass function (PMF) describes the probability distribution over a discrete random variable. In other terms, it is a function that returns the probability of a random variable being exactly equal to a specific value.</p>
<p>The returned probability lies in the range [0, 1] and the sum of all probabilities for every state equals one.</p>
<p>Let’s imagine a plot where the x-axis describes the states and the y-axis shows the probability of a certain state. Thinking this way allows us to envision the probability or the PMF as a barplot sitting on top of a state.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/probability-mass-function.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a uniform PMF</figcaption>
</figure>
</div>
<p>In the following, we will learn about three common discrete probability distributions: The Bernoulli, binomial and geometric distribution.</p>
<section id="bernoulli-distribution" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>Named after the Swiss mathematician Jacob Bernoulli, the Bernoulli distribution is a discrete probability distribution of a single binary random variable, which either takes the value 1 or 0.</p>
<p>Loosely speaking, we can think of the Bernoulli distribution as a model giving the set of possible outcomes for a single experiment, that can be answered with a simple yes-no question.</p>
<p>More formally the function can be stated as the following equation</p>
<p><span class="math display">\[
f(k;p) =     \begin{dcases}
        q = 1-p &amp; if \ k = 0 \\
        q &amp; if \ k = 1 \\
    \end{dcases}
\]</span></p>
<p><span class="math display">\[
f(k;p) = p^k(1-p)^{1-k} \ for \ k \in \{0,1\}
\]</span></p>
<p>which basically evaluates to <code>p if k=1</code> or to <code>(1-p) if k=0</code>. Thus, the Bernoulli distribution is parametrized by just a <code>single parameter p</code>.</p>
<p>Suppose, we toss a fair coin once. The probability of obtaining heads is <code>P(Heads) = 0.5</code>. Visualizing the PMF we get the following plot:</p>
<p><img src="images/bernoulli-distribution.webp" class="img-fluid"></p>
<p>Since the Bernoulli Distribution models only a single trial, it can also be viewed as a special case of the binomial distribution</p>
</section>
<section id="binomial-distribution" class="level3">
<h3 class="anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h3>
<p>The binomial distribution describes the discrete probability distribution of the number of successes in a sequence of <em>n</em> independent trials, each with a binary outcome. The success or failure is given by the probability <em>p</em> or <em>(1-p)</em> respectively<em>.</em></p>
<p>Thus, the binomial distribution is parametrized by the parameters</p>
<p><span class="math display">\[
n \in N, \ p \in [0,1]
\]</span></p>
<p>More formally the binomial distribution can be expressed with the following equation:</p>
<p><span class="math display">\[
f(k;n,p) = {n \choose k} p^k(1-p)^{n-k}
\]</span></p>
<p>The success of <em>k</em> is given by the probability <em>p</em> to the power of <em>k,</em> whereas the probability of failure is defined by <em>(1-p)</em> to the power of <em>n</em> minus <em>k</em>, which is basically the number of trials minus the one trial where we get <em>k</em>.</p>
<p>Since the event of success <em>k</em> can occur anywhere in <em>n</em> trials, we have <strong><em>“n choose k”</em></strong> ways to distribute the success.</p>
<p>Let’s pick up our coin-tossing example from before and build on it.</p>
<p>Now, we are going to flip the fair coin three times, while being interested in the random variable describing the number of heads obtained.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/binomial-distribution.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Number of heads in three coin flips</figcaption>
</figure>
</div>
<p>If we want to compute the probability of the coin coming up as heads two times, we can simply use the equation from before and pluck in the values.</p>
<p><span class="math display">\[
P(2) = {3 \choose 2} p^2 (1-p)^{3-2}
\]</span></p>
<p><span class="math display">\[
P(2) = 3(0.5)^2(0.5)^1
\]</span></p>
<p><span class="math display">\[
P(2) = 0.375
\]</span></p>
<p>which results in a probability <code>P(2) = 0.375</code>. If we proceed in the same way for the remaining probabilities, we get the following distribution:</p>
<p><img src="images/binomial-distribution-2.webp" class="img-fluid"></p>
</section>
<section id="geometric-distribution" class="level3">
<h3 class="anchored" data-anchor-id="geometric-distribution">Geometric Distribution</h3>
<p>Suppose, we are interested in the number of times we have to flip a coin until it comes up heads for the first time.</p>
<p>The geometric distribution gives the probability of the first success occurrence, requiring <em>n</em> independent trials, with a success probability of <em>p</em>.</p>
<p>More formally it can be stated as</p>
<p>which computes the probability of the number of trials needed up to and including the success event.</p>
<p>The following assumptions need to be true, in order to calculate the geometric distribution:</p>
<ol type="1">
<li><p>Independence</p></li>
<li><p>For each trial, there are only two possible outcomes</p></li>
<li><p>The probability of success is the same for every trial</p></li>
</ol>
<p>Let’s visualize the geometric distribution by answering the question for the probability of the number of trials needed for the coin to come up heads for the first time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/geometric.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The geometric distribution until first head</figcaption>
</figure>
</div>
</section>
<section id="gaussian-distribution" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-distribution">Gaussian Distribution</h3>
<p>The Gaussian distribution is often considered a sensible choice to represent a real-valued random variable, whose distribution is unknown.</p>
<p>This is mainly due to the central limit theorem, which, loosely speaking, states that the average of many independent random variables with finite mean and variance is itself a random variable — which is normally distributed as the number of observations increases.</p>
<p>This is especially useful since it allows us to model complicated systems as Gaussian distributed, even if the individual parts follow a more complicated structure or distribution.</p>
<p>Another reason it is a common choice for modeling a distribution over a continuous variable is the fact that it inserts the least amount of prior knowledge.</p>
<p>More formally, the Gaussian distribution can be stated as</p>
<p><span class="math display">\[
N (x: \mu, \sigma^2) = \sqrt\frac{1}{2\pi\sigma^2}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
\]</span></p>
<p>where the parameter <em>µ</em> is the mean and <em>σ²</em> describes the variance.</p>
<p>In simple terms, the mean will be responsible for defining the central peak of the bell-shaped distribution, whereas the variance or the standard deviation defines its width.</p>
<p>We can visualize the normal distribution as the following:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/gaussian-distribution.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a Gaussian distribution</figcaption>
</figure>
</div>
</section>
</section>
<section id="probability-density-function" class="level2">
<h2 class="anchored" data-anchor-id="probability-density-function">Probability Density Function</h2>
<p>In the earlier sections, we learned that a random variable can either be discrete or continuous. If it is discrete, we can describe the probability distribution with a probability mass function.</p>
<p>Now, we are dealing with continuous variables — hence, we need to describe the probability distribution with a probability density function (PDF).</p>
<p>The PDF, contrary to the PMF, does not give the probability of a random variable taking a specific state directly. Instead, it describes the probability of landing inside an infinitesimal region. In other terms, the PDF describes the probability of a random variable lying between a particular range of values.</p>
<p>In order to find the actual probability mass, we need to integrate, which yields the area under the density function but above the x-axis.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/density-function.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a probability density function</figcaption>
</figure>
</div>
<p>The probability density function must be non-negative and its integral needs to be 1.</p>
<p><span class="math display">\[
(1) \ \ p(x) \ge 0
\]</span></p>
<p><span class="math display">\[
(2) \ \ \int p(x) \delta x = 1
\]</span></p>
<p>One of the most common continuous probability distributions is the gaussian or normal distribution.</p>
</section>
</section>
<section id="applications-in-machine-learning" class="level1">
<h1>Applications in Machine Learning</h1>
<section id="bayesian-inference" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-inference">Bayesian Inference</h2>
<p>Bayesian inference is a way of making statistical inferences in which the statistician assigns subjective probabilities to the distributions that could generate the data. These subjective probabilities form the so-called prior distribution.</p>
<p>After the data is observed, <a href="https://www.statlect.com/fundamentals-of-probability/Bayes-rule">Bayes’ rule</a> is used to update the prior, that is, to revise the probabilities assigned to the possible data generating distributions. These revised probabilities form the so-called posterior distribution.</p>
<p>This lecture provides an introduction to Bayesian inference and discusses a simple example of inference about the mean of a normal distribution.</p>
<section id="the-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="the-likelihood"><strong>The likelihood</strong></h3>
<p>The first building block of a parametric Bayesian model is the likelihood</p>
<p>The likelihood is equal to the probability density of x when the parameter of the data generating distribution is equal to <span class="math inline">\(\theta\)</span></p>
<p>For the time being, we assume that and are <a href="https://www.statlect.com/glossary/absolutely-continuous-random-variable">continuous</a>. Later, we will discuss how to relax this assumption.</p>
</section>
<section id="the-prior" class="level3">
<h3 class="anchored" data-anchor-id="the-prior"><strong>The prior</strong></h3>
<p>The second building block of a Bayesian model is the prior</p>
<p><span class="math display">\[
p(\theta)
\]</span></p>
<p>The prior is the subjective probability density assigned to the parameter</p>
</section>
<section id="the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="the-posterior"><strong>The posterior</strong></h3>
<p>After observing the data , we use Bayes’ rule to update the prior about the parameter :</p>
<p>The conditional density is called posterior distribution of the parameter.</p>
<p>By using the formula for the marginal density derived above, we obtain</p>
<p>Thus, the posterior depends on the two distributions specified by the statistician, the prior and the likelihood .</p>
<p>Example</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta, binom</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up prior distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># alpha parameter</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># beta parameter</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> beta(a, b)  <span class="co"># create Beta distribution object</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some fake data</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span>  <span class="co"># number of trials</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">40</span>   <span class="co"># number of successes</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> binom.rvs(<span class="dv">1</span>, k<span class="op">/</span>n, size<span class="op">=</span>n)  <span class="co"># generate binary data from binomial distribution</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute posterior distribution</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>a_post <span class="op">=</span> a <span class="op">+</span> np.<span class="bu">sum</span>(data)  <span class="co"># update alpha parameter</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>b_post <span class="op">=</span> b <span class="op">+</span> n <span class="op">-</span> np.<span class="bu">sum</span>(data)  <span class="co"># update beta parameter</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> beta(a_post, b_post)  <span class="co"># create updated Beta distribution object</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute credible intervals</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>CI_95 <span class="op">=</span> beta.ppf([<span class="fl">0.025</span>, <span class="fl">0.975</span>], a_post, b_post)  <span class="co"># compute 95% credible interval</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior and posterior distributions</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>prior_pdf <span class="op">=</span> prior.pdf(x)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>posterior_pdf <span class="op">=</span> posterior.pdf(x)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.plot(x, prior_pdf, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.plot(x, posterior_pdf, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="dv">0</span>, <span class="bu">max</span>(posterior_pdf)<span class="op">*</span><span class="fl">1.1</span>])</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Prior'</span>, <span class="st">'Posterior'</span>])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p/(1-p)'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bayesian Analysis of p/(1-p)'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="576" height="449"></p>
</div>
</div>
</section>
</section>
<section id="markov-chain-monte-carlo-mcmc" class="level2">
<h2 class="anchored" data-anchor-id="markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</h2>
<p>There are several Bayesian models that allow us to compute the posterior distribution of the parameters analytically. However, this is often not possible.</p>
<p>When an analytical solution is not available, Markov Chain Monte Carlo (MCMC) methods are commonly employed to derive the posterior distribution numerically.</p>
<p>MCMC methods are <a href="https://www.statlect.com/asymptotic-theory/Monte-Carlo-method">Monte Carlo methods</a> that allow us to generate large samples of correlated draws from the posterior distribution of the parameter vector by simply using the proportionality</p>
<p>The <a href="https://www.statlect.com/asymptotic-theory/empirical-distribution">empirical distribution</a> of the generated sample can then be used to produce <a href="https://www.statlect.com/asymptotic-theory/plug-in-principle">plug-in estimates</a> of the quantities of interest.</p>
<p>See the lecture on <a href="https://www.statlect.com/fundamentals-of-statistics/Markov-Chain-Monte-Carlo">MCMC methods</a> for more details.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the target distribution</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_dist(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the proposal distribution (normal distribution)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prop_dist(x, sigma):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(x, sigma)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the initial state and other parameters</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>burn_in <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Markov chain</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x0</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples using the Metropolis-Hastings algorithm</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.zeros(num_samples)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_samples <span class="op">+</span> burn_in):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a proposal</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    x_prop <span class="op">=</span> prop_dist(x, sigma)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the acceptance probability</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, target_dist(x_prop) <span class="op">/</span> target_dist(x))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decide whether to accept the proposal</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> alpha:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x_prop</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the sample after the burn-in period</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> burn_in:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        samples[i <span class="op">-</span> burn_in] <span class="op">=</span> x</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the histogram of the samples and the target distribution</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, target_dist(x_vals), <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Samples'</span>, <span class="st">'Target distribution'</span>])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="597" height="429"></p>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2023, Danny_UDK - All rights reserved</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This website is built by Daniel Udekwe with <a href="https://quarto.org/">Quarto</a></div>
  </div>
</footer>



</body></html>