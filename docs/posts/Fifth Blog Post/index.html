<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel A. Udekwe">
<meta name="dcterms.date" content="2023-11-23">

<title>CS5805 - Machine Learning I - Probability Theory and Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">CS5805 - Machine Learning I</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html" rel="" target="">
 <span class="menu-text">Blog Posts</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/daniel-udekwe" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/daniel-a-udekwe-19a2bbb2" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <div class="quarto-title-block"><div><h1 class="title">Probability Theory and Random Variables</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">data</div>
                <div class="quarto-category">code</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Daniel A. Udekwe </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 23, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-probability-theory" id="toc-introduction-to-probability-theory" class="nav-link active" data-scroll-target="#introduction-to-probability-theory">Introduction to Probability Theory</a>
  <ul class="collapse">
  <li><a href="#set-theory" id="toc-set-theory" class="nav-link" data-scroll-target="#set-theory">Set Theory</a></li>
  <li><a href="#events" id="toc-events" class="nav-link" data-scroll-target="#events">Events</a></li>
  <li><a href="#probability" id="toc-probability" class="nav-link" data-scroll-target="#probability">Probability</a></li>
  </ul></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables">Random Variables</a></li>
  <li><a href="#probability-distribution" id="toc-probability-distribution" class="nav-link" data-scroll-target="#probability-distribution">Probability Distribution</a>
  <ul class="collapse">
  <li><a href="#probability-mass-function" id="toc-probability-mass-function" class="nav-link" data-scroll-target="#probability-mass-function"><strong>Probability Mass Function</strong></a>
  <ul class="collapse">
  <li><a href="#bernoulli-distribution" id="toc-bernoulli-distribution" class="nav-link" data-scroll-target="#bernoulli-distribution">Bernoulli Distribution</a></li>
  <li><a href="#binomial-distribution" id="toc-binomial-distribution" class="nav-link" data-scroll-target="#binomial-distribution">Binomial Distribution</a></li>
  <li><a href="#geometric-distribution" id="toc-geometric-distribution" class="nav-link" data-scroll-target="#geometric-distribution">Geometric Distribution</a></li>
  <li><a href="#gaussian-distribution" id="toc-gaussian-distribution" class="nav-link" data-scroll-target="#gaussian-distribution">Gaussian Distribution</a></li>
  </ul></li>
  <li><a href="#probability-density-function" id="toc-probability-density-function" class="nav-link" data-scroll-target="#probability-density-function">Probability Density Function</a></li>
  </ul></li>
  <li><a href="#applications-in-machine-learning" id="toc-applications-in-machine-learning" class="nav-link" data-scroll-target="#applications-in-machine-learning">Applications in Machine Learning</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference">Bayesian Inference</a>
  <ul class="collapse">
  <li><a href="#the-likelihood" id="toc-the-likelihood" class="nav-link" data-scroll-target="#the-likelihood"><strong>The likelihood</strong></a></li>
  <li><a href="#the-prior" id="toc-the-prior" class="nav-link" data-scroll-target="#the-prior"><strong>The prior</strong></a></li>
  <li><a href="#the-posterior" id="toc-the-posterior" class="nav-link" data-scroll-target="#the-posterior"><strong>The posterior</strong></a></li>
  </ul></li>
  <li><a href="#markov-chain-monte-carlo-mcmc" id="toc-markov-chain-monte-carlo-mcmc" class="nav-link" data-scroll-target="#markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/daniel-a-udekwe/daniel-a-udekwe.github.io/blob/main/posts/Fifth Blog Post/index.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">




<p>The subject of probability theory is the foundation upon which all of statistics is built, providing a means for modelling population, experiments or almost anything else that could be considered a random phenomenon. Through these models, statisticians are able to draw inferences about populations, inferences based on examination of only a part of the whole.</p>
<section id="introduction-to-probability-theory" class="level1">
<h1>Introduction to Probability Theory</h1>
<p>Probability theory is a branch of mathematics concerned with the study of random phenomena and is often considered one of the fundamental pillars of machine learning. It is however a huge field to cover and very easy to get lost in, especially when being self-taught.</p>
<p>In the following sections, we are going to cover some fundamental aspects of probability before delving into aspects that are especially relevant to machine learning — the random variable and the probability distribution.</p>
<section id="set-theory" class="level2">
<h2 class="anchored" data-anchor-id="set-theory">Set Theory</h2>
<p>The set <span class="math inline">\(S\)</span> of all possible outcomes of a particular experiment is called the sample space for the experiment. If the experiment consists of tossing a coin, the sample space contains two outcomes, heads and tails: thus,</p>
<p><span class="math display">\[
S = \{H, T\}
\]</span></p>
<p>If, on the other hand, the experiment consists of observing the reported SAT scores of randomly selected students at a certain university, the sample space would be the set of positive integers between 200 and 800 that are multiples of ten — that is, <span class="math inline">\(S = \{200, 210, 220, ..., 780, 790, 800 \}\)</span></p>
<p>We can classify sample spaces into two types according to the number of elements they contain. Sample spaces can be either <strong>countable</strong> or <strong>uncountable</strong>; If the elements of a sample space can be put into 1-1 correspondence with a subset of the integers, the sample space is countable. Of course, if the sample space contains only a finite number of elements, it is countable. Thus, the coin toss and SAT score sample is uncountable, since the positive real numbers cannot be put into 1-1 correspondence with the integers. If, however, we measured the reaction time to the nearest second, then the sample space would be (in seconds) <span class="math inline">\(S = \{0, 1, 2, 3, ... \}\)</span>, which is then countable.</p>
</section>
<section id="events" class="level2">
<h2 class="anchored" data-anchor-id="events">Events</h2>
<p>An event is any collection of possible outcomes of an experiment that is, any subset of <span class="math inline">\(S\)</span> (including <span class="math inline">\(S\)</span> itself)</p>
<p>Let A be an event, a subset of S. We say the event A occurs if the outcome of the experiment is the set A. When speaking of probabilities, we generally speak of the probability of an event, rather than a set. But we may use the terms interchangeably.</p>
<p>Given any two events (or sets) A and B, we have the following elementary set operations.</p>
<p><strong>Union</strong>: The union of A and B, written <span class="math inline">\(A\cup B\)</span> is the set of elements that belong to either A or B or both</p>
<p><span class="math display">\[
A \cup B = \{x:x \in A \ or \ x \in B \}
\]</span></p>
<p><strong>Intersection:</strong> The intersection of A and B, written <span class="math inline">\(A\cap B\)</span> is the set of elements that belong to both A and B</p>
<p><span class="math display">\[
A \cap B = \{x:x \in A \ and \ x \in B\}
\]</span></p>
<p><strong>Complementation:</strong> The complement of A, written <span class="math inline">\(A^c\)</span>, is the set of all elements that are not in A</p>
<p><span class="math display">\[
A^c = \{x:x \not\in A\}
\]</span></p>
<p>Elementary set operations can be combined, somewhat akin to the way addition and multiplication can be combined. As long as we are careful, we can treat sets as if they were numbers. We can now state the following useful properties of set operations.</p>
<ol type="1">
<li><p>Commutativity.</p>
<p>a. <span class="math inline">\(A\cup B = B \cup B\)</span>,</p>
<p>b. <span class="math inline">\(A \cap B = B \cap A\)</span></p></li>
<li><p>Assocaiativity</p>
<p>a. <span class="math inline">\(A \cup (B\cup C) = (A \cup B) \cup C\)</span></p>
<p>b. <span class="math inline">\(A \cap (B\cap C) = (A \cap B) \cap C\)</span></p></li>
<li><p>Distribution Laws</p>
<p>a. <span class="math inline">\(A \cap (B \cup C) = (A \cap B) \cup (A \cap C)\)</span></p>
<p>b. <span class="math inline">\(A \cup (B\cap C) = (A \cup B) \cap (A \cup C)\)</span></p></li>
<li><p>DeMorgan’s Laws</p>
<p>a. <span class="math inline">\((A \cup B )^C = A^C \cap B^C\)</span></p>
<p>b. <span class="math inline">\((A \cap B )^C = A^C \cup B^C\)</span></p></li>
</ol>
</section>
<section id="probability" class="level2">
<h2 class="anchored" data-anchor-id="probability">Probability</h2>
<p>Consider the simple experiment of tossing a fair coin, so <span class="math inline">\(S = \{H, T\}\)</span>. By a “fair” coin we mean a balanced coin that is equally as likely to land heads up as tails up, and hence the reasonable probability function is the one that assigns equal probabilities to heads and tails, that is</p>
<p><span class="math display">\[
P(\{H\}) = P(\{T\})
\]</span></p>
<p>Since <span class="math inline">\(S = \{H\} \cup \{T\}\)</span>, we have <span class="math inline">\(P(\{H\} \cup \{T\}) = P(\{H\}) + P(\{T\})\)</span> and</p>
<p><span class="math display">\[
P(\{H\}) + P(\{T\}) = 1
\]</span></p>
<p>Solving the equations simultaneously shows that <span class="math inline">\(P(\{H\}) = P(\{T\}) = 1/2\)</span></p>
<p>In machine learning, we often deal with uncertainty and stochastic quantities, due to one of the reasons being incomplete observability — therefore, we most likely work with sampled data.</p>
<p>Now, suppose we want to draw reliable conclusions about the behavior of a random variable, despite the fact that we only have limited data and we simply do not know the entire population.</p>
<p>Hence, we need some kind of way to generalize from the sampled data to the population, or in other words — we need to estimate the true data-generating process.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/data-generation.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Estimating the data-generating process</figcaption>
</figure>
</div>
<p>Understanding the probability distribution, allows us to compute the probability of a certain outcome by also accounting for the variability in the results. Thus, it enables us to generalize from the sample to the population, estimate the data-generating function and predict the behavior of a random variable more accurately.</p>
</section>
</section>
<section id="random-variables" class="level1">
<h1>Random Variables</h1>
<p>Loosely speaking, the random variable is a variable whose value depends on the outcome of a random event. We can also describe it as a function that maps from the sample space to a measurable space (e.g.&nbsp;a real number).</p>
<p>Let’s assume, we have a sample space containing 4 students <code>{A, B, C, D}</code>. If we now randomly pick <code>student A</code> and measure the height in centimeters, we can think of the <code>random variable (H)</code>as the function with the input of <code>student</code>and the output of <code>height</code>as a real number.</p>
<p><span class="math display">\[
H(student) = height
\]</span></p>
<p>We can visualize this small example like the following</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/random-variable.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a random variable</figcaption>
</figure>
</div>
<p>Depending on the outcome — which student is randomly picked — our random variable (H) can take on different states or different values in terms of height in centimeters.</p>
<p>A random variable can be either discrete or continuous.</p>
<p>If our random variable can take only a finite or countably infinite number of distinct values, then it is discrete. Examples of a discrete random variable include the number of students in a class, test questions answered correctly, the number of children in a family, etc.</p>
<p>Our random variable, however, is continuous if between any two values of our variable are an infinite number of other valid values. We can think of quantities such as pressure, height, mass, and distance as examples of continuous random variables.</p>
<p>When we couple our random variable with a probability distribution we can answer the following question: How likely is it for our random variable to take a specific state? Which is basically the same as asking for the probability.</p>
<p>Now, we are left with one question that remains— what is a probability distribution?</p>
</section>
<section id="probability-distribution" class="level1">
<h1>Probability Distribution</h1>
<p>The description of how likely a random variable takes one of its possible states can be given by a probability distribution. Thus, the probability distribution is a mathematical function that gives the probabilities of different outcomes for an experiment.</p>
<p>More generally it can be described as the function</p>
<p><span class="math display">\[
P:A \rightarrow R
\]</span></p>
<p>which maps an input space A — related to the sample space — to a real number, namely the probability.</p>
<p>For the above function to characterize a probability distribution, it must follow all of the <strong>Kolmogorov axioms:</strong></p>
<ol type="1">
<li><p><strong>Non</strong>-negativity</p></li>
<li><p>No probability exceeds 1</p></li>
<li><p>Additivity of any countable disjoint (mutually exclusive) events</p></li>
</ol>
<p>The way we describe a probability distribution depends on whether the random variable is discrete or continuous, which will result in a probability mass or density function respectively.</p>
<section id="probability-mass-function" class="level2">
<h2 class="anchored" data-anchor-id="probability-mass-function"><strong>Probability Mass Function</strong></h2>
<p>The probability mass function (PMF) describes the probability distribution over a discrete random variable. In other terms, it is a function that returns the probability of a random variable being exactly equal to a specific value.</p>
<p>The returned probability lies in the range [0, 1] and the sum of all probabilities for every state equals one.</p>
<p>Let’s imagine a plot where the x-axis describes the states and the y-axis shows the probability of a certain state. Thinking this way allows us to envision the probability or the PMF as a barplot sitting on top of a state.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/probability-mass-function.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a uniform PMF</figcaption>
</figure>
</div>
<p>In the following, we will learn about three common discrete probability distributions: The Bernoulli, binomial and geometric distribution.</p>
<section id="bernoulli-distribution" class="level3">
<h3 class="anchored" data-anchor-id="bernoulli-distribution">Bernoulli Distribution</h3>
<p>Named after the Swiss mathematician Jacob Bernoulli, the Bernoulli distribution is a discrete probability distribution of a single binary random variable, which either takes the value 1 or 0.</p>
<p>Loosely speaking, we can think of the Bernoulli distribution as a model giving the set of possible outcomes for a single experiment, that can be answered with a simple yes-no question.</p>
<p>More formally the function can be stated as the following equation</p>
<p><span class="math display">\[
f(k;p) =     \begin{dcases}
        q = 1-p &amp; if \ k = 0 \\
        q &amp; if \ k = 1 \\
    \end{dcases}
\]</span></p>
<p><span class="math display">\[
f(k;p) = p^k(1-p)^{1-k} \ for \ k \in \{0,1\}
\]</span></p>
<p>which basically evaluates to <code>p if k=1</code> or to <code>(1-p) if k=0</code>. Thus, the Bernoulli distribution is parametrized by just a <code>single parameter p</code>.</p>
<p>Suppose, we toss a fair coin once. The probability of obtaining heads is <code>P(Heads) = 0.5</code>. Visualizing the PMF we get the following plot:</p>
<p><img src="images/bernoulli-distribution.webp" class="img-fluid"></p>
<p>Since the Bernoulli Distribution models only a single trial, it can also be viewed as a special case of the binomial distribution</p>
</section>
<section id="binomial-distribution" class="level3">
<h3 class="anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h3>
<p>The binomial distribution describes the discrete probability distribution of the number of successes in a sequence of <em>n</em> independent trials, each with a binary outcome. The success or failure is given by the probability <em>p</em> or <em>(1-p)</em> respectively<em>.</em></p>
<p>Thus, the binomial distribution is parametrized by the parameters</p>
<p><span class="math display">\[
n \in N, \ p \in [0,1]
\]</span></p>
<p>More formally the binomial distribution can be expressed with the following equation:</p>
<p><span class="math display">\[
f(k;n,p) = {n \choose k} p^k(1-p)^{n-k}
\]</span></p>
<p>The success of <em>k</em> is given by the probability <em>p</em> to the power of <em>k,</em> whereas the probability of failure is defined by <em>(1-p)</em> to the power of <em>n</em> minus <em>k</em>, which is basically the number of trials minus the one trial where we get <em>k</em>.</p>
<p>Since the event of success <em>k</em> can occur anywhere in <em>n</em> trials, we have <strong><em>“n choose k”</em></strong> ways to distribute the success.</p>
<p>Let’s pick up our coin-tossing example from before and build on it.</p>
<p>Now, we are going to flip the fair coin three times, while being interested in the random variable describing the number of heads obtained.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/binomial-distribution.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Number of heads in three coin flips</figcaption>
</figure>
</div>
<p>If we want to compute the probability of the coin coming up as heads two times, we can simply use the equation from before and pluck in the values.</p>
<p><span class="math display">\[
P(2) = {3 \choose 2} p^2 (1-p)^{3-2}
\]</span></p>
<p><span class="math display">\[
P(2) = 3(0.5)^2(0.5)^1
\]</span></p>
<p><span class="math display">\[
P(2) = 0.375
\]</span></p>
<p>which results in a probability <code>P(2) = 0.375</code>. If we proceed in the same way for the remaining probabilities, we get the following distribution:</p>
<p><img src="images/binomial-distribution-2.webp" class="img-fluid"></p>
</section>
<section id="geometric-distribution" class="level3">
<h3 class="anchored" data-anchor-id="geometric-distribution">Geometric Distribution</h3>
<p>Suppose, we are interested in the number of times we have to flip a coin until it comes up heads for the first time.</p>
<p>The geometric distribution gives the probability of the first success occurrence, requiring <em>n</em> independent trials, with a success probability of <em>p</em>.</p>
<p>More formally it can be stated as</p>
<p>which computes the probability of the number of trials needed up to and including the success event.</p>
<p>The following assumptions need to be true, in order to calculate the geometric distribution:</p>
<ol type="1">
<li><p>Independence</p></li>
<li><p>For each trial, there are only two possible outcomes</p></li>
<li><p>The probability of success is the same for every trial</p></li>
</ol>
<p>Let’s visualize the geometric distribution by answering the question for the probability of the number of trials needed for the coin to come up heads for the first time.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/geometric.jpg" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The geometric distribution until first head</figcaption>
</figure>
</div>
</section>
<section id="gaussian-distribution" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-distribution">Gaussian Distribution</h3>
<p>The Gaussian distribution is often considered a sensible choice to represent a real-valued random variable, whose distribution is unknown.</p>
<p>This is mainly due to the central limit theorem, which, loosely speaking, states that the average of many independent random variables with finite mean and variance is itself a random variable — which is normally distributed as the number of observations increases.</p>
<p>This is especially useful since it allows us to model complicated systems as Gaussian distributed, even if the individual parts follow a more complicated structure or distribution.</p>
<p>Another reason it is a common choice for modeling a distribution over a continuous variable is the fact that it inserts the least amount of prior knowledge.</p>
<p>More formally, the Gaussian distribution can be stated as</p>
<p><span class="math display">\[
N (x: \mu, \sigma^2) = \sqrt\frac{1}{2\pi\sigma^2}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
\]</span></p>
<p>where the parameter <em>µ</em> is the mean and <em>σ²</em> describes the variance.</p>
<p>In simple terms, the mean will be responsible for defining the central peak of the bell-shaped distribution, whereas the variance or the standard deviation defines its width.</p>
<p>We can visualize the normal distribution as the following:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/gaussian-distribution.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a Gaussian distribution</figcaption>
</figure>
</div>
</section>
</section>
<section id="probability-density-function" class="level2">
<h2 class="anchored" data-anchor-id="probability-density-function">Probability Density Function</h2>
<p>In the earlier sections, we learned that a random variable can either be discrete or continuous. If it is discrete, we can describe the probability distribution with a probability mass function.</p>
<p>Now, we are dealing with continuous variables — hence, we need to describe the probability distribution with a probability density function (PDF).</p>
<p>The PDF, contrary to the PMF, does not give the probability of a random variable taking a specific state directly. Instead, it describes the probability of landing inside an infinitesimal region. In other terms, the PDF describes the probability of a random variable lying between a particular range of values.</p>
<p>In order to find the actual probability mass, we need to integrate, which yields the area under the density function but above the x-axis.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/density-function.webp" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">An example of a probability density function</figcaption>
</figure>
</div>
<p>The probability density function must be non-negative and its integral needs to be 1.</p>
<p><span class="math display">\[
(1) \ \ p(x) \ge 0
\]</span></p>
<p><span class="math display">\[
(2) \ \ \int p(x) \delta x = 1
\]</span></p>
<p>One of the most common continuous probability distributions is the gaussian or normal distribution.</p>
</section>
</section>
<section id="applications-in-machine-learning" class="level1">
<h1>Applications in Machine Learning</h1>
<section id="bayesian-inference" class="level2">
<h2 class="anchored" data-anchor-id="bayesian-inference">Bayesian Inference</h2>
<p>Bayesian inference is a way of making statistical inferences in which the statistician assigns subjective probabilities to the distributions that could generate the data. These subjective probabilities form the so-called prior distribution.</p>
<p>After the data is observed, <a href="https://www.statlect.com/fundamentals-of-probability/Bayes-rule">Bayes’ rule</a> is used to update the prior, that is, to revise the probabilities assigned to the possible data generating distributions. These revised probabilities form the so-called posterior distribution.</p>
<p>This lecture provides an introduction to Bayesian inference and discusses a simple example of inference about the mean of a normal distribution.</p>
<section id="the-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="the-likelihood"><strong>The likelihood</strong></h3>
<p>The first building block of a parametric Bayesian model is the likelihood</p>
<p>The likelihood is equal to the probability density of x when the parameter of the data generating distribution is equal to <span class="math inline">\(\theta\)</span></p>
<p>For the time being, we assume that and are <a href="https://www.statlect.com/glossary/absolutely-continuous-random-variable">continuous</a>. Later, we will discuss how to relax this assumption.</p>
</section>
<section id="the-prior" class="level3">
<h3 class="anchored" data-anchor-id="the-prior"><strong>The prior</strong></h3>
<p>The second building block of a Bayesian model is the prior</p>
<p><span class="math display">\[
p(\theta)
\]</span></p>
<p>The prior is the subjective probability density assigned to the parameter</p>
</section>
<section id="the-posterior" class="level3">
<h3 class="anchored" data-anchor-id="the-posterior"><strong>The posterior</strong></h3>
<p>After observing the data , we use Bayes’ rule to update the prior about the parameter :</p>
<p>The conditional density is called posterior distribution of the parameter.</p>
<p>By using the formula for the marginal density derived above, we obtain</p>
<p>Thus, the posterior depends on the two distributions specified by the statistician, the prior and the likelihood .</p>
<p>Example</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta, binom</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up prior distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># alpha parameter</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># beta parameter</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> beta(a, b)  <span class="co"># create Beta distribution object</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some fake data</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span>  <span class="co"># number of trials</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">40</span>   <span class="co"># number of successes</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> binom.rvs(<span class="dv">1</span>, k<span class="op">/</span>n, size<span class="op">=</span>n)  <span class="co"># generate binary data from binomial distribution</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute posterior distribution</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>a_post <span class="op">=</span> a <span class="op">+</span> np.<span class="bu">sum</span>(data)  <span class="co"># update alpha parameter</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>b_post <span class="op">=</span> b <span class="op">+</span> n <span class="op">-</span> np.<span class="bu">sum</span>(data)  <span class="co"># update beta parameter</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> beta(a_post, b_post)  <span class="co"># create updated Beta distribution object</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute credible intervals</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>CI_95 <span class="op">=</span> beta.ppf([<span class="fl">0.025</span>, <span class="fl">0.975</span>], a_post, b_post)  <span class="co"># compute 95% credible interval</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior and posterior distributions</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>prior_pdf <span class="op">=</span> prior.pdf(x)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>posterior_pdf <span class="op">=</span> posterior.pdf(x)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>plt.plot(x, prior_pdf, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.plot(x, posterior_pdf, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="dv">0</span>, <span class="bu">max</span>(posterior_pdf)<span class="op">*</span><span class="fl">1.1</span>])</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Prior'</span>, <span class="st">'Posterior'</span>])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p/(1-p)'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bayesian Analysis of p/(1-p)'</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="576" height="449"></p>
</div>
</div>
</section>
</section>
<section id="markov-chain-monte-carlo-mcmc" class="level2">
<h2 class="anchored" data-anchor-id="markov-chain-monte-carlo-mcmc">Markov Chain Monte Carlo (MCMC)</h2>
<p>There are several Bayesian models that allow us to compute the posterior distribution of the parameters analytically. However, this is often not possible.</p>
<p>When an analytical solution is not available, Markov Chain Monte Carlo (MCMC) methods are commonly employed to derive the posterior distribution numerically.</p>
<p>MCMC methods are <a href="https://www.statlect.com/asymptotic-theory/Monte-Carlo-method">Monte Carlo methods</a> that allow us to generate large samples of correlated draws from the posterior distribution of the parameter vector by simply using the proportionality</p>
<p>The <a href="https://www.statlect.com/asymptotic-theory/empirical-distribution">empirical distribution</a> of the generated sample can then be used to produce <a href="https://www.statlect.com/asymptotic-theory/plug-in-principle">plug-in estimates</a> of the quantities of interest.</p>
<p>See the lecture on <a href="https://www.statlect.com/fundamentals-of-statistics/Markov-Chain-Monte-Carlo">MCMC methods</a> for more details.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the target distribution</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_dist(x):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the proposal distribution (normal distribution)</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prop_dist(x, sigma):</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(x, sigma)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the initial state and other parameters</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>burn_in <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Markov chain</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x0</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples using the Metropolis-Hastings algorithm</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.zeros(num_samples)</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_samples <span class="op">+</span> burn_in):</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a proposal</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    x_prop <span class="op">=</span> prop_dist(x, sigma)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the acceptance probability</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, target_dist(x_prop) <span class="op">/</span> target_dist(x))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decide whether to accept the proposal</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> alpha:</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x_prop</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the sample after the burn-in period</span></span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> burn_in:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        samples[i <span class="op">-</span> burn_in] <span class="op">=</span> x</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the histogram of the samples and the target distribution</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, target_dist(x_vals), <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Samples'</span>, <span class="st">'Target distribution'</span>])</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="597" height="429"></p>
</div>
</div>
<p>Illustrations are obtained from here<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>


<!-- -->

</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><a href="https://towardsdatascience.com/" class="uri">https://towardsdatascience.com/</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb3" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Probability Theory and Random Variables"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Daniel A. Udekwe"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-11-23"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [data, code, analysis]</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "probability.jpg"</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>The subject of probability theory is the foundation upon which all of statistics is built, providing a means for modelling population, experiments or almost anything else that could be considered a random phenomenon. Through these models, statisticians are able to draw inferences about populations, inferences based on examination of only a part of the whole.</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="fu"># Introduction to Probability Theory</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>Probability theory is a branch of mathematics concerned with the study of random phenomena and is often considered one of the fundamental pillars of machine learning. It is however a huge field to cover and very easy to get lost in, especially when being self-taught.</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>In the following sections, we are going to cover some fundamental aspects of probability before delving into aspects that are especially relevant to machine learning --- the random variable and the probability distribution.</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Set Theory</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>The set $S$ of all possible outcomes of a particular experiment is called the sample space for the experiment. If the experiment consists of tossing a coin, the sample space contains two outcomes, heads and tails: thus,</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>S = <span class="sc">\{</span>H, T<span class="sc">\}</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>If, on the other hand, the experiment consists of observing the reported SAT scores of randomly selected students at a certain university, the sample space would be the set of positive integers between 200 and 800 that are multiples of ten --- that is, $S = <span class="sc">\{</span>200, 210, 220, ..., 780, 790, 800 <span class="sc">\}</span>$</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>We can classify sample spaces into two types according to the number of elements they contain. Sample spaces can be either **countable** or **uncountable**; If the elements of a sample space can be put into 1-1 correspondence with a subset of the integers, the sample space is countable. Of course, if the sample space contains only a finite number of elements, it is countable. Thus, the coin toss and SAT score sample is uncountable, since the positive real numbers cannot be put into 1-1 correspondence with the integers. If, however, we measured the reaction time to the nearest second, then the sample space would be (in seconds) $S = <span class="sc">\{</span>0, 1, 2, 3, ... <span class="sc">\}</span>$, which is then countable.</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="fu">## Events</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>An event is any collection of possible outcomes of an experiment that is, any subset of $S$ (including $S$ itself)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>Let A be an event, a subset of S. We say the event A occurs if the outcome of the experiment is the set A. When speaking of probabilities, we generally speak of the probability of an event, rather than a set. But we may use the terms interchangeably.</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>Given any two events (or sets) A and B, we have the following elementary set operations.</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>**Union**: The union of A and B, written $A\cup B$ is the set of elements that belong to either A or B or both</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>A \cup B = <span class="sc">\{</span>x:x \in A \ or \ x \in B <span class="sc">\}</span></span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>**Intersection:** The intersection of A and B, written $A\cap B$ is the set of elements that belong to both A and B</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>A \cap B = <span class="sc">\{</span>x:x \in A \ and \ x \in B<span class="sc">\}</span></span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>**Complementation:** The complement of A, written $A^c$, is the set of all elements that are not in A</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>A^c = <span class="sc">\{</span>x:x \not\in A<span class="sc">\}</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>Elementary set operations can be combined, somewhat akin to the way addition and multiplication can be combined. As long as we are careful, we can treat sets as if they were numbers. We can now state the following useful properties of set operations.</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Commutativity.</span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>    a<span class="sc">\.</span> $A\cup B = B \cup B$,</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    b<span class="sc">\.</span> $A \cap B = B \cap A$</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Assocaiativity</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a>    a<span class="sc">\.</span> $A \cup (B\cup C) = (A \cup B) \cup C$</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>    b<span class="sc">\.</span> $A \cap (B\cap C) = (A \cap B) \cap C$</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Distribution Laws</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>    a<span class="sc">\.</span> $A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    b<span class="sc">\.</span> $A \cup (B\cap C) = (A \cup B) \cap (A \cup C)$</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>DeMorgan's Laws</span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    a<span class="sc">\.</span> $(A \cup B )^C = A^C \cap B^C$</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>    b<span class="sc">\.</span> $(A \cap B )^C = A^C \cup B^C$</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability</span></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>Consider the simple experiment of tossing a fair coin, so $S = <span class="sc">\{</span>H, T<span class="sc">\}</span>$. By a "fair" coin we mean a balanced coin that is equally as likely to land heads up as tails up, and hence the reasonable probability function is the one that assigns equal probabilities to heads and tails, that is</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>P(<span class="sc">\{</span>H<span class="sc">\}</span>) = P(<span class="sc">\{</span>T<span class="sc">\}</span>)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>Since $S = <span class="sc">\{</span>H<span class="sc">\}</span> \cup <span class="sc">\{</span>T<span class="sc">\}</span>$, we have $P(<span class="sc">\{</span>H<span class="sc">\}</span> \cup <span class="sc">\{</span>T<span class="sc">\}</span>) = P(<span class="sc">\{</span>H<span class="sc">\}</span>) + P(<span class="sc">\{</span>T<span class="sc">\}</span>)$ and</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>P(<span class="sc">\{</span>H<span class="sc">\}</span>) + P(<span class="sc">\{</span>T<span class="sc">\}</span>) = 1</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>Solving the equations simultaneously shows that $P(<span class="sc">\{</span>H<span class="sc">\}</span>) = P(<span class="sc">\{</span>T<span class="sc">\}</span>) = 1/2$</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>In machine learning, we often deal with uncertainty and stochastic quantities, due to one of the reasons being incomplete observability --- therefore, we most likely work with sampled data.</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>Now, suppose we want to draw reliable conclusions about the behavior of a random variable, despite the fact that we only have limited data and we simply do not know the entire population.</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>Hence, we need some kind of way to generalize from the sampled data to the population, or in other words --- we need to estimate the true data-generating process.</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a><span class="al">![ Estimating the data-generating process](images/data-generation.webp)</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>Understanding the probability distribution, allows us to compute the probability of a certain outcome by also accounting for the variability in the results. Thus, it enables us to generalize from the sample to the population, estimate the data-generating function and predict the behavior of a random variable more accurately.</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="fu"># Random Variables</span></span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>Loosely speaking, the random variable is a variable whose value depends on the outcome of a random event. We can also describe it as a function that maps from the sample space to a measurable space (e.g. a real number).</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>Let's assume, we have a sample space containing 4 students <span class="in">`{A, B, C, D}`</span>. If we now randomly pick <span class="in">`student A`</span> and measure the height in centimeters, we can think of the <span class="in">`random variable (H)`</span>as the function with the input of <span class="in">`student`</span>and the output of <span class="in">`height`</span>as a real number.</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>H(student) = height</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>We can visualize this small example like the following</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a><span class="al">![An example of a random variable](images/random-variable.webp)</span></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>Depending on the outcome --- which student is randomly picked --- our random variable (H) can take on different states or different values in terms of height in centimeters.</span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a>A random variable can be either discrete or continuous.</span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a>If our random variable can take only a finite or countably infinite number of distinct values, then it is discrete. Examples of a discrete random variable include the number of students in a class, test questions answered correctly, the number of children in a family, etc.</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a>Our random variable, however, is continuous if between any two values of our variable are an infinite number of other valid values. We can think of quantities such as pressure, height, mass, and distance as examples of continuous random variables.</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a>When we couple our random variable with a probability distribution we can answer the following question: How likely is it for our random variable to take a specific state? Which is basically the same as asking for the probability.</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a>Now, we are left with one question that remains--- what is a probability distribution?</span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="fu"># Probability Distribution</span></span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a>The description of how likely a random variable takes one of its possible states can be given by a probability distribution. Thus, the probability distribution is a mathematical function that gives the probabilities of different outcomes for an experiment.</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a>More generally it can be described as the function</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a>P:A \rightarrow R</span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a>which maps an input space A --- related to the sample space --- to a real number, namely the probability.</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a>For the above function to characterize a probability distribution, it must follow all of the **Kolmogorov axioms:**</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>**Non**-negativity</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>No probability exceeds 1</span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Additivity of any countable disjoint (mutually exclusive) events</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a>The way we describe a probability distribution depends on whether the random variable is discrete or continuous, which will result in a probability mass or density function respectively.</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a><span class="fu">## **Probability Mass Function**</span></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a>The probability mass function (PMF) describes the probability distribution over a discrete random variable. In other terms, it is a function that returns the probability of a random variable being exactly equal to a specific value.</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a>The returned probability lies in the range <span class="sc">\[</span>0, 1<span class="sc">\]</span> and the sum of all probabilities for every state equals one.</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a>Let's imagine a plot where the x-axis describes the states and the y-axis shows the probability of a certain state. Thinking this way allows us to envision the probability or the PMF as a barplot sitting on top of a state.</span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a><span class="al">![An example of a uniform PMF](images/probability-mass-function.webp)</span></span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a>In the following, we will learn about three common discrete probability distributions: The Bernoulli, binomial and geometric distribution.</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bernoulli Distribution</span></span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a>Named after the Swiss mathematician Jacob Bernoulli, the Bernoulli distribution is a discrete probability distribution of a single binary random variable, which either takes the value 1 or 0.</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a>Loosely speaking, we can think of the Bernoulli distribution as a model giving the set of possible outcomes for a single experiment, that can be answered with a simple yes-no question.</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a>More formally the function can be stated as the following equation</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a>f(k;p) =     \begin{dcases}</span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a>        q = 1-p &amp; if \ k = 0 <span class="sc">\\</span></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a>        q &amp; if \ k = 1 <span class="sc">\\</span></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a>    \end{dcases}</span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a>f(k;p) = p^k(1-p)^{1-k} \ for \ k \in <span class="sc">\{</span>0,1<span class="sc">\}</span></span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>which basically evaluates to <span class="in">`p if k=1`</span> or to <span class="in">`(1-p) if k=0`</span>. Thus, the Bernoulli distribution is parametrized by just a <span class="in">`single parameter p`</span>.</span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a>Suppose, we toss a fair coin once. The probability of obtaining heads is <span class="in">`P(Heads) = 0.5`</span>. Visualizing the PMF we get the following plot:</span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/bernoulli-distribution.webp)</span></span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a>Since the Bernoulli Distribution models only a single trial, it can also be viewed as a special case of the binomial distribution</span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a><span class="fu">### Binomial Distribution</span></span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a>The binomial distribution describes the discrete probability distribution of the number of successes in a sequence of *n* independent trials, each with a binary outcome. The success or failure is given by the probability *p* or *(1-p)* respectively*.*</span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a>Thus, the binomial distribution is parametrized by the parameters</span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a>n \in N, \ p \in <span class="co">[</span><span class="ot">0,1</span><span class="co">]</span></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a>More formally the binomial distribution can be expressed with the following equation:</span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a>f(k;n,p) = {n \choose k} p^k(1-p)^{n-k}</span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a>The success of *k* is given by the probability *p* to the power of *k,* whereas the probability of failure is defined by *(1-p)* to the power of *n* minus *k*, which is basically the number of trials minus the one trial where we get *k*.</span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a>Since the event of success *k* can occur anywhere in *n* trials, we have ***"n choose k"*** ways to distribute the success.</span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a>Let's pick up our coin-tossing example from before and build on it.</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a>Now, we are going to flip the fair coin three times, while being interested in the random variable describing the number of heads obtained.</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a><span class="al">![Number of heads in three coin flips](images/binomial-distribution.webp)</span></span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a>If we want to compute the probability of the coin coming up as heads two times, we can simply use the equation from before and pluck in the values.</span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a>P(2) = {3 \choose 2} p^2 (1-p)^{3-2}</span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a>P(2) = 3(0.5)^2(0.5)^1</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a>P(2) = 0.375</span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a>which results in a probability <span class="in">`P(2) = 0.375`</span>. If we proceed in the same way for the remaining probabilities, we get the following distribution:</span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a><span class="al">![](images/binomial-distribution-2.webp)</span></span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### Geometric Distribution</span></span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a>Suppose, we are interested in the number of times we have to flip a coin until it comes up heads for the first time.</span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a>The geometric distribution gives the probability of the first success occurrence, requiring *n* independent trials, with a success probability of *p*.</span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a>More formally it can be stated as</span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a>which computes the probability of the number of trials needed up to and including the success event.</span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a>The following assumptions need to be true, in order to calculate the geometric distribution:</span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Independence</span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>For each trial, there are only two possible outcomes</span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>The probability of success is the same for every trial</span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a>Let's visualize the geometric distribution by answering the question for the probability of the number of trials needed for the coin to come up heads for the first time.</span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a><span class="al">![The geometric distribution until first head](images/geometric.jpg)</span></span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gaussian Distribution</span></span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a>The Gaussian distribution is often considered a sensible choice to represent a real-valued random variable, whose distribution is unknown.</span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a>This is mainly due to the central limit theorem, which, loosely speaking, states that the average of many independent random variables with finite mean and variance is itself a random variable --- which is normally distributed as the number of observations increases.</span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a>This is especially useful since it allows us to model complicated systems as Gaussian distributed, even if the individual parts follow a more complicated structure or distribution.</span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a>Another reason it is a common choice for modeling a distribution over a continuous variable is the fact that it inserts the least amount of prior knowledge.</span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a>More formally, the Gaussian distribution can be stated as</span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a>N (x: \mu, \sigma^2) = \sqrt\frac{1}{2\pi\sigma^2}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)</span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a>where the parameter *µ* is the mean and *σ²* describes the variance.</span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a>In simple terms, the mean will be responsible for defining the central peak of the bell-shaped distribution, whereas the variance or the standard deviation defines its width.</span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a>We can visualize the normal distribution as the following:</span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a><span class="al">![An example of a Gaussian distribution](images/gaussian-distribution.webp)</span></span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a><span class="fu">## Probability Density Function</span></span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a>In the earlier sections, we learned that a random variable can either be discrete or continuous. If it is discrete, we can describe the probability distribution with a probability mass function.</span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a>Now, we are dealing with continuous variables --- hence, we need to describe the probability distribution with a probability density function (PDF).</span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a>The PDF, contrary to the PMF, does not give the probability of a random variable taking a specific state directly. Instead, it describes the probability of landing inside an infinitesimal region. In other terms, the PDF describes the probability of a random variable lying between a particular range of values.</span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a>In order to find the actual probability mass, we need to integrate, which yields the area under the density function but above the x-axis.</span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a><span class="al">![An example of a probability density function](images/density-function.webp)</span></span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a>The probability density function must be non-negative and its integral needs to be 1.</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a>(1) \ \ p(x) \ge 0</span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a>(2) \ \ \int p(x) \delta x = 1</span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a>One of the most common continuous probability distributions is the gaussian or normal distribution.</span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a><span class="fu"># Applications in Machine Learning</span></span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a><span class="fu">## Bayesian Inference</span></span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a>Bayesian inference is a way of making statistical inferences in which the statistician assigns subjective probabilities to the distributions that could generate the data. These subjective probabilities form the so-called prior distribution.</span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a>After the data is observed, <span class="co">[</span><span class="ot">Bayes' rule</span><span class="co">](https://www.statlect.com/fundamentals-of-probability/Bayes-rule)</span> is used to update the prior, that is, to revise the probabilities assigned to the possible data generating distributions. These revised probabilities form the so-called posterior distribution.</span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a>This lecture provides an introduction to Bayesian inference and discusses a simple example of inference about the mean of a normal distribution.</span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a><span class="fu">### **The likelihood**</span></span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a>The first building block of a parametric Bayesian model is the likelihood</span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a>The likelihood is equal to the probability density of x when the parameter of the data generating distribution is equal to $\theta$</span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a>For the time being, we assume that and are <span class="co">[</span><span class="ot">continuous</span><span class="co">](https://www.statlect.com/glossary/absolutely-continuous-random-variable)</span>. Later, we will discuss how to relax this assumption.</span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a><span class="fu">### **The prior**</span></span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a>The second building block of a Bayesian model is the prior</span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a>p(\theta)</span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a>The prior is the subjective probability density assigned to the parameter</span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a><span class="fu">### **The posterior**</span></span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a>After observing the data , we use Bayes' rule to update the prior about the parameter :</span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a>The conditional density is called posterior distribution of the parameter.</span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a>By using the formula for the marginal density derived above, we obtain</span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a>Thus, the posterior depends on the two distributions specified by the statistician, the prior and the likelihood .</span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a>Example</span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta, binom</span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up prior distribution</span></span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># alpha parameter</span></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># beta parameter</span></span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> beta(a, b)  <span class="co"># create Beta distribution object</span></span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate some fake data</span></span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">100</span>  <span class="co"># number of trials</span></span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">40</span>   <span class="co"># number of successes</span></span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> binom.rvs(<span class="dv">1</span>, k<span class="op">/</span>n, size<span class="op">=</span>n)  <span class="co"># generate binary data from binomial distribution</span></span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute posterior distribution</span></span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a>a_post <span class="op">=</span> a <span class="op">+</span> np.<span class="bu">sum</span>(data)  <span class="co"># update alpha parameter</span></span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a>b_post <span class="op">=</span> b <span class="op">+</span> n <span class="op">-</span> np.<span class="bu">sum</span>(data)  <span class="co"># update beta parameter</span></span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> beta(a_post, b_post)  <span class="co"># create updated Beta distribution object</span></span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute credible intervals</span></span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a>CI_95 <span class="op">=</span> beta.ppf([<span class="fl">0.025</span>, <span class="fl">0.975</span>], a_post, b_post)  <span class="co"># compute 95% credible interval</span></span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot prior and posterior distributions</span></span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1000</span>)</span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a>prior_pdf <span class="op">=</span> prior.pdf(x)</span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a>posterior_pdf <span class="op">=</span> posterior.pdf(x)</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a>plt.plot(x, prior_pdf, <span class="st">'b--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a>plt.plot(x, posterior_pdf, <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a>plt.ylim([<span class="dv">0</span>, <span class="bu">max</span>(posterior_pdf)<span class="op">*</span><span class="fl">1.1</span>])</span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Prior'</span>, <span class="st">'Posterior'</span>])</span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'p/(1-p)'</span>)</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bayesian Analysis of p/(1-p)'</span>)</span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a><span class="fu">## Markov Chain Monte Carlo (MCMC)</span></span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a>There are several Bayesian models that allow us to compute the posterior distribution of the parameters analytically. However, this is often not possible.</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a>When an analytical solution is not available, Markov Chain Monte Carlo (MCMC) methods are commonly employed to derive the posterior distribution numerically.</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a>MCMC methods are <span class="co">[</span><span class="ot">Monte Carlo methods</span><span class="co">](https://www.statlect.com/asymptotic-theory/Monte-Carlo-method)</span> that allow us to generate large samples of correlated draws from the posterior distribution of the parameter vector by simply using the proportionality</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">empirical distribution</span><span class="co">](https://www.statlect.com/asymptotic-theory/empirical-distribution)</span> of the generated sample can then be used to produce <span class="co">[</span><span class="ot">plug-in estimates</span><span class="co">](https://www.statlect.com/asymptotic-theory/plug-in-principle)</span> of the quantities of interest.</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a>See the lecture on <span class="co">[</span><span class="ot">MCMC methods</span><span class="co">](https://www.statlect.com/fundamentals-of-statistics/Markov-Chain-Monte-Carlo)</span> for more details.</span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb3-404"><a href="#cb3-404" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-405"><a href="#cb3-405" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb3-406"><a href="#cb3-406" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb3-407"><a href="#cb3-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-408"><a href="#cb3-408" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the target distribution</span></span>
<span id="cb3-409"><a href="#cb3-409" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_dist(x):</span>
<span id="cb3-410"><a href="#cb3-410" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(<span class="op">-</span>x<span class="op">**</span><span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span>) <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi)</span>
<span id="cb3-411"><a href="#cb3-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-412"><a href="#cb3-412" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the proposal distribution (normal distribution)</span></span>
<span id="cb3-413"><a href="#cb3-413" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prop_dist(x, sigma):</span>
<span id="cb3-414"><a href="#cb3-414" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(x, sigma)</span>
<span id="cb3-415"><a href="#cb3-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-416"><a href="#cb3-416" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the initial state and other parameters</span></span>
<span id="cb3-417"><a href="#cb3-417" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-418"><a href="#cb3-418" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb3-419"><a href="#cb3-419" aria-hidden="true" tabindex="-1"></a>burn_in <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-420"><a href="#cb3-420" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-421"><a href="#cb3-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-422"><a href="#cb3-422" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the Markov chain</span></span>
<span id="cb3-423"><a href="#cb3-423" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> x0</span>
<span id="cb3-424"><a href="#cb3-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-425"><a href="#cb3-425" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples using the Metropolis-Hastings algorithm</span></span>
<span id="cb3-426"><a href="#cb3-426" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> np.zeros(num_samples)</span>
<span id="cb3-427"><a href="#cb3-427" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_samples <span class="op">+</span> burn_in):</span>
<span id="cb3-428"><a href="#cb3-428" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate a proposal</span></span>
<span id="cb3-429"><a href="#cb3-429" aria-hidden="true" tabindex="-1"></a>    x_prop <span class="op">=</span> prop_dist(x, sigma)</span>
<span id="cb3-430"><a href="#cb3-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-431"><a href="#cb3-431" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute the acceptance probability</span></span>
<span id="cb3-432"><a href="#cb3-432" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> <span class="bu">min</span>(<span class="dv">1</span>, target_dist(x_prop) <span class="op">/</span> target_dist(x))</span>
<span id="cb3-433"><a href="#cb3-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-434"><a href="#cb3-434" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Decide whether to accept the proposal</span></span>
<span id="cb3-435"><a href="#cb3-435" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> alpha:</span>
<span id="cb3-436"><a href="#cb3-436" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x_prop</span>
<span id="cb3-437"><a href="#cb3-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-438"><a href="#cb3-438" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save the sample after the burn-in period</span></span>
<span id="cb3-439"><a href="#cb3-439" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&gt;</span> burn_in:</span>
<span id="cb3-440"><a href="#cb3-440" aria-hidden="true" tabindex="-1"></a>        samples[i <span class="op">-</span> burn_in] <span class="op">=</span> x</span>
<span id="cb3-441"><a href="#cb3-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-442"><a href="#cb3-442" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the histogram of the samples and the target distribution</span></span>
<span id="cb3-443"><a href="#cb3-443" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb3-444"><a href="#cb3-444" aria-hidden="true" tabindex="-1"></a>x_vals <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb3-445"><a href="#cb3-445" aria-hidden="true" tabindex="-1"></a>plt.plot(x_vals, target_dist(x_vals), <span class="st">'r-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb3-446"><a href="#cb3-446" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb3-447"><a href="#cb3-447" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability density'</span>)</span>
<span id="cb3-448"><a href="#cb3-448" aria-hidden="true" tabindex="-1"></a>plt.legend([<span class="st">'Samples'</span>, <span class="st">'Target distribution'</span>])</span>
<span id="cb3-449"><a href="#cb3-449" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb3-450"><a href="#cb3-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-451"><a href="#cb3-451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-452"><a href="#cb3-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-453"><a href="#cb3-453" aria-hidden="true" tabindex="-1"></a>Illustrations are obtained from here<span class="ot">[^1]</span></span>
<span id="cb3-454"><a href="#cb3-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-455"><a href="#cb3-455" aria-hidden="true" tabindex="-1"></a><span class="ot">[^1]: &lt;https://towardsdatascience.com/&gt;</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Copyright 2023, Danny_UDK - All rights reserved</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">This website is built by Daniel Udekwe with <a href="https://quarto.org/">Quarto</a></div>
  </div>
</footer>



</body></html>