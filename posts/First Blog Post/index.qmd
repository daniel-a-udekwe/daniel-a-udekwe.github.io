---
title: "Regression (Linear and Nonlinear)"
author: "Daniel A. Udekwe"
date: "2023-10-17"
categories: [news, code, analysis]
image: "regression.png"
---

Regression is one of the most common algorithms in machine learning. In this post we will explore this algorithm and we will implement it using Python from scratch.

As the name suggests this algorithm is applicable for Regression problems. Linear Regression is a **Linear Model**. Which means, we will establish a linear relationship between the input variables(**X**) and single output variable(**Y**). When the input(**X**) is a single variable this model is called **Simple Linear Regression** and when there are mutiple input variables(**X**), it is called **Multiple Linear Regression**. Additionally, non-linear regression seeks to establish a non-linear relationship between the input variable (x) and the output (Y)

## Simple Linear Regression

We discussed that Linear Regression is a simple model. Simple Linear Regression is the simplest model in machine learning.

### Model Representation

In this problem we have an input variable - **X** and one output variable - **Y**. And we want to build linear relationship between these variables. Here the input variable is called **Independent Variable** and the output variable is called **Dependent Variable**. We can define this linear relationship as follows:$$Y = \beta_0 + \beta_1 X$$

The $\beta_1$ is called a scale factor or **coefficient** and $\beta_0$ is called **bias coefficient**. The bias coeffient gives an extra degree of freedom to this model. This equation is similar to the line equation $y = mx +b$ with $m = \beta_1$(Slope) and $b = \beta_0$(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.

But how do we find these coefficients? That's the learning procedure. We can find these using different approaches. One is called **Ordinary Least Square Method** and other one is called **Gradient Descent Approach**. We will use Ordinary Least Square Method in Simple Linear Regression and Gradient Descent Approach in Multiple Linear Regression in post.

### Ordinary Least Square Method

Earlier in this post we discussed that we are going to approximate the relationship between X and Y to a line. Let's say we have few inputs and outputs. And we plot these scatter points in 2D space, we will get something like the following image.

![](regression_1.png)

And you can see a line in the image. That's what we are going to accomplish. And we want to minimize the error of out model. A good model will always have least error. We can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.

![](regression_2.jpeg)

And total error of this model is the sum of all errors of each point. ie.

$$
D = \sum_{i=1}^{m} d_i^2
$$

$d_i$Distance between line and i^th^ point.

$m$- Total number of points

You might have noticed that we are squaring each of the distances. This is because, some points will be above the line and some points will be below the line. We can minimize the error in the model by minimizing $D$ And after the mathematics of minimizing $D_i$, we will get;

$$
\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}
$$

$$
\beta_0 = \bar{y} - \beta_1\bar{x}
$$

In these equations �¯ is the mean value of input variable **X** and �¯ is the mean value of output variable **Y**.

Now we have the model. This method is called [**Ordinary Least Square Method**](https://www.wikiwand.com/en/Ordinary_least_squares). Now we will implement this model in Python.

$$
Y = \beta_0 + \beta_1X
$$

$$
\beta_1 = \frac{\sum_{i=1}^{m} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{m} (x_i - \bar{x})^2}
$$

$$
\beta_0 = \bar{y} - \beta_1\bar{x}
$$

### Implementation

We are going to use a dataset containing the price of houses as a function of size. This data is split into 3 for training, validation and finally testing. Let's start off by importing and viewing the data.

```{python}
#Import the needed libraries
import pandas as pd
import matplotlib.pyplot as plt
import json
import numpy as np

with open('assignment1.json', 'r') as json_file:
    data = json.load(json_file)
    
X_train = np.array(data['X_train']).reshape(-1, 1)
y_train = np.array(data['Y_train']).reshape(-1, 1)
X_val = np.array(data['X_val']).reshape(-1, 1)
y_val = np.array(data['Y_val']).reshape(-1, 1)
X_test = np.array(data['X_test']).reshape(-1, 1)
y_test = np.array(data['Y_test']).reshape(-1, 1)


array1 = data['X_train']
array2 = data['Y_train']

array3 = data['X_val']
array4 = data['Y_val']

array5 = data['X_test']
array6 = data['Y_test']



print("Training Data")
tableData = {
  'X_train': array1,
  'Y_train': array2,
}
table = pd.DataFrame(tableData)
print(table.head(3))
print(f"number of rows and colums: {table.shape}")
print()

print("Validation Data")
tableData2 = {
  'X_Val': array3,
  'Y_Val': array4
}
table2 = pd.DataFrame(tableData2)
print(table2.head(3))
print(f"number of rows and colums: {table2.shape}")
print()

print("Test Data")
tableData3 = {
  'X_test': array5,
  'Y_test': array6
}
table3 = pd.DataFrame(tableData3)
print(table3.head(3))
print(f"number of rows and colums: {table3.shape}")

```

As we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.

we need to implement feature scaling

```{python}
# Step 2: Preprocess the data (feature scaling)
mean = np.mean(X_train)
std = np.std(X_train)
X_train = (X_train - mean) / std
X_val = (X_val - mean) / std
X_test = (X_test - mean) / std

```

Next, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first

```{python}
plt.scatter(array1, array2, label='training data')
plt.scatter(array3, array4, label='validation data')
plt.scatter(array5, array6, label='testing data')

plt.ylabel('House Price')
plt.xlabel('House size')
plt.legend()
plt.title('Training, Validation and Testing Data')
```

Lets create a function to implement linear regression with regularization.

$$
J(\theta) = \frac{1}{2m} (\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2) + \frac{\lambda}{2m}(\sum_{j=1}^n \theta_j^2)
$$

```{python}
# Step 3: Implement regularized linear regression with gradient descent
def ridge_regression(X, y, alpha, num_iterations, learning_rate):
    m, n = X.shape
    # Initialize theta with the correct shape
    theta = np.zeros((n, 1))
    history = []

    for _ in range(num_iterations):
        gradient = (X.T @ (X @ theta - y) + alpha * theta) / m
        theta -= learning_rate * gradient
        cost = np.sum((X @ theta - y) ** 2) / (2 * m) + (alpha / (2 * m)) * np.sum(theta[1:] ** 2)
        history.append(cost)

    return theta, history, cost
```

In the equation above, $\lambda$ is the regularization parameter which ensures ... This is implemented with the gradient descent algorithm.

### Gradient Descent

Gradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.

##### Step 1

Initialize values $\theta_0, \theta_1, ..., \theta_n$ with some value. In this case we will initialize with 0.

#### Step 2

Iteratively update,

$$
\theta_j : \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta)
$$

until it converges.

where:

$$
\frac{\partial J(\theta)}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \ j = 0
$$

$$
\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j \ j\ge 1 
$$

This is the procedure. Here $\alpha$ is the learning rate. This operation $\frac{\partial}{\partial \theta_j} J(\theta)$ means we are finding partial derivate of cost with respect to each $\theta_j$. This is called Gradient.

Read [this](https://math.stackexchange.com/questions/174270/what-exactly-is-the-difference-between-a-derivative-and-a-total-derivative) if you are unfamiliar with partial derivatives.

In step 2 we are changing the values of $\theta_j$. in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don't want to change values of $\theta_j$. drastically, because we might miss the minima. That's why we need learning rate.

![Animation illustrating the gradient descent method](regression%20_4.gif){fig-align="center"}

The above animation illustrates the Gradient Descent method.

But we still didn't find the value of $\frac{\partial J(\theta)}{\partial \theta_j}$ . After we applying the mathematics. The step 2 becomes.

$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \frac{\lambda}{m}\theta_j
$$

We iteratively change values of $\theta_j$ according to above equation. This particular method is called **Batch Gradient Descent**.

Then we need to implement a function to plot the line of best fit

```{python}
def plot_data_and_fit(X_train, y_train, theta):
    plt.scatter(X_train, y_train, color='blue', label='Training data')
    
    x_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)
    x_values_extended = np.column_stack((np.ones_like(x_values), x_values))
    y_values = x_values_extended @ theta
    
    plt.plot(x_values, y_values, color='red', label='Line of best fit')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Training Data and Line of Best Fit')
    plt.legend()
    plt.show()
```

Next we will train the model using the training data and return the cost using the equations below:

```{python}
# Step 4: Train the model using the training data
alpha = 0 # Regularization strength (adjust as needed)
num_iterations = 1000
learning_rate = 0.1
X_train_extended = np.column_stack((np.ones_like(X_train), X_train))
theta, history, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)
print(f"the cost iss: {cost}")
```

Then tuning the regularization parameter

```{python}
# Step 5: Tune the regularization parameter using the validation data
alphas = [0.01, 0.1, 1, 10, 100]
mse_val = []

for alpha in alphas:
    X_val_extended = np.column_stack((np.ones_like(X_val), X_val))
    theta_val, history_, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)
    y_val_pred = X_val_extended @ theta_val
    mse = np.mean((y_val - y_val_pred) ** 2)
    mse_val.append(mse)

best_alpha = alphas[np.argmin(mse_val)]
print(f"Best regularization parameter (alpha): {best_alpha}")

```

Evaluating the Model on the Testing data using the root mean square of errors.

Root Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,

$$
RMSE = \sqrt{\sum_{i=1}^m \frac{1}{m}(\hat{y_1}-y_i)^2)}
$$

Here $\hat{y_i}$ is the $i^{th}$ predicted output values.

```{python}
# Step 6: Evaluate the model using the testing data
X_test_extended = np.column_stack((np.ones_like(X_test), X_test))
theta_test, _, cost = ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)
y_test_pred = X_test_extended @ theta_test
mse_test = np.mean((y_test - y_test_pred) ** 2)
print(f"Mean Squared Error on Test Data: {mse_test}")
print(cost)
```

```{python}
# Plot the cost history during training
plt.plot(range(num_iterations), history)
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Cost vs. Iteration')
plt.show()
```

Plotting the training data and line of best fit

```{python}
# Plot training data and line of best fit
plot_data_and_fit(X_train, y_train, theta)
print(theta)

```

$$
House Price = 22.108 + 20.736 \times House size
$$

------------------------------------------------------------------------

Nonlinear Regression

```{python}
# Step 4: Train the model using the training data
alpha = 10  # Regularization strength (adjust as needed)
num_iterations = 1000
learning_rate = 0.1
degree = 3  # Degree of the polynomial features

# Create polynomial features
X_train_poly = np.column_stack([X_train ** i for i in range(1, degree + 1)])

# Train the model using polynomial features
X_train_poly = np.column_stack((np.ones_like(X_train_poly), X_train_poly))
theta, history_train, cost_train = ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)
print(f"Training cost: {cost_train}")
print(theta)

# Step 5: Tune the regularization parameter using the validation data
alphas = [0.01, 0.1, 1, 10, 100]
mse_val = []

for alpha in alphas:
    # Create polynomial features for validation data
    X_val_poly = np.column_stack([X_val ** i for i in range(1, degree + 1)])
    X_val_poly = np.column_stack((np.ones_like(X_val_poly), X_val_poly))
    
    theta_val, history_val, cost_val = ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)
    y_val_pred = X_val_poly @ theta_val
    mse = np.mean((y_val - y_val_pred) ** 2)
    mse_val.append(mse)
"""
    # Plot the cost history for both training and validation
    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')
    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.title('Cost vs. Iteration')
    plt.legend()
    plt.show()
"""
best_alpha = alphas[np.argmin(mse_val)]
print(f"Best regularization parameter (alpha): {best_alpha}")

# Step 6: Evaluate the model using the testing data
# Create polynomial features for test data
X_test_poly = np.column_stack([X_test ** i for i in range(1, degree + 1)])
X_test_poly = np.column_stack((np.ones_like(X_test_poly), X_test_poly))

theta_test, _, cost_test = ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)
y_test_pred = X_test_poly @ theta_test
mse_test = np.mean((y_test - y_test_pred) ** 2)
print(f"Mean Squared Error on Test Data: {mse_test}")

# Plot training data and the polynomial fit
plt.scatter(X_train, y_train, color='blue', label='Training data')
x_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)
x_values_poly = np.column_stack([x_values ** i for i in range(1, degree + 1)])
x_values_poly = np.column_stack((np.ones_like(x_values_poly), x_values_poly))
y_values = x_values_poly @ theta
plt.plot(x_values, y_values, color='red', label=f'Polynomial Fit (Degree {degree})')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Training Data and Polynomial Fit')
plt.legend()
plt.show()
```

```{python}
    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')
    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')
    plt.xlabel('Iteration')
    plt.ylabel('Cost')
    plt.title('Cost vs. Iteration')
    plt.legend()
    plt.show()
```
