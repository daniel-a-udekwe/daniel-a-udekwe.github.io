{
  "hash": "ef22082301c434a909eb71707bda3886",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-14\"\ncategories: [news]\nimage: \"avknn.png\"\n---\n\nAnomaly detection is one of the most common use cases of machine learning. Finding and identifying outliers helps to prevent fraud, adversary attacks, and network intrusions that can compromise your company's future.\n\nIn this post, we will talk about how anomaly detection works, what machine learning techniques you can use for it, and what benefits anomaly detection with ML brings to a business.\n\n## **What is an anomaly?**\n\nBefore talking about anomaly detection, we need to understand what an **anomaly** is.\n\nGenerally speaking, an anomaly is something that differs from a norm: a deviation, an exception. In software engineering, by anomaly we understand a rare occurrence or event that doesn't fit into the pattern, and, therefore, seems suspicious. Some examples are:\n\n-   sudden burst or decrease in activity;\n\n-   error in the text;\n\n-   sudden rapid drop or increase in temperature.\n\nCommon reasons for outliers are:\n\n-   data preprocessing errors;\n\n-   noise;\n\n-   fraud;\n\n-   attacks.\n\nNormally, you want to catch them all; a software program must run smoothly and be predictable so every outlier is a potential threat to its robustness and security. Catching and identifying anomalies is what we call **anomaly or outlier detection**.\n\nFor example, if large sums of money are spent one after another within one day and it is not your typical behavior, a bank can block your card. They will see an unusual pattern in your daily transactions. This anomaly can typically be connected to fraud since identity thieves try to steal as much money as they can while they can. Once an anomaly is detected, it needs to be investigated, or problems may follow.\n\n## **Why do you need machine learning for anomaly detection?**\n\nThis is a process that is usually conducted with the help of statistics and machine learning tools.\n\nThe reason is that the majority of companies today that require outlier detection work with huge amounts of data: transactions, text, image, and video content, etc. You would have to spend days going through all the transitions that happen inside a bank every hour, and more and more are generated every second. It is simply impossible to drive any meaningful insights from this amount of data manually.\n\nMoreover, another difficulty is that the data is often unstructured, which means that the information wasn't arranged in any specific way for the data analysis. For example, business documents, emails, or images are examples of unstructured data.\n\nTo be able to collect, clean, structure, analyze, and store data, you need to use tools that aren't scared of big volumes of data. Machine learning techniques, in fact, show the best results when large data sets are involved. Machine learning algorithms are able to process most types of data. Moreover, you can choose the algorithm based on your problem and even combine different techniques for the best results.\n\nMachine learning used for real-world applications helps to streamline the process of anomaly detection and save the resources. It can happen not only post-factum but also in real time. Real-time anomaly detection is applied to improve security and robustness, for instance, in fraud discovery and cybersecurity.\n\n**What are anomaly detection methods?**\n\n![](anomaly.png){fig-align=\"center\"}\n\nThere are different kinds of anomaly detection methods with machine learning.\n\n### **Supervised**\n\nIn supervised anomaly detection, an ML engineer needs a training dataset. Items in the dataset are labeled into two categories: normal and abnormal. The model will use these examples to extract patterns and be able to detect abnormal patterns in the previously unseen data.\n\nIn supervised learning, the quality of the training dataset is very important. There is a lot of manual work involved since somebody needs to collect and label examples.\n\n**Note:** While you can label some anomalies and try to classify them (hence it's a classification task), the underlying goal of anomaly detection is defining \"normal data points\" rather than \"abnormal data points\". So in real world applications with very few anomaly samples labelled, it's almost never regarded as a supervised task.\n\n### **Unsupervised**\n\nThis type of anomaly detection is the most common type, and the most well-known representative of unsupervised algorithms are neural networks.\n\nArtificial neural networks allow to decrease the amount of manual work needed to pre-process examples: no manual labeling is needed. Neural networks can even be applied to unstructured data. NNs can detect anomalies in unlabeled data and use what they have learned when working with new data.\n\nThe advantage of this method is that it allows you to decrease the manual work in anomaly detection. Moreover, quite often it's impossible to predict all the anomalies that can occur in the dataset. Think of self-driving cars, for example. They can face a situation on the road that has never happened before. Putting all road situations into a finite number of classes would be impossible. That is why neural networks are priceless when working with real-life data in real-time.\n\nHowever, ANNs almost rocket science level of complexity. So before you try out those, you might want to experiment with more conventional algorithms like DBSCAN, especially if your project is not that big.\n\nMoreover, the architecture of neural networks is a black box. We often don't know what kinds of events neural networks will label as anomalies, moreover, it can easily learn wrong rules that are not so easy to fix. That is why unsupervised anomaly detection techniques are often less trustworthy than supervised ones.\n\n### **Semi-supervised**\n\nSemi-supervised anomaly detection methods combine the benefits of the previous two methods. Engineers can apply unsupervised learning methods to automate feature learning and work with unstructured data. However, by combining it with human supervision, they have an opportunity to monitor and control what kind of patterns the model learns. This usually helps to make the model's predictions more accurate.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n<Axes: xlabel='Item_MRP', ylabel='Item_Outlet_Sales'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=610 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Item_MRP</th>\n      <th>Item_Outlet_Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.927507</td>\n      <td>0.283587</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.072068</td>\n      <td>0.031419</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.468288</td>\n      <td>0.158115</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.640093</td>\n      <td>0.053555</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.095805</td>\n      <td>0.073651</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(base_estimator=KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Fit the models and predict outliers\nfor clf_name, clf in classifiers.items():\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores > threshold).astype(int)\n        y_test_pred = (y_test_scores > threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Test Set Predictions')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Training Set Predictions')\n    \n    plt.show()\n\n    # Evaluate the model performance\n    print(f\"------ {clf_name} ------\")\n    print(f\"Train Accuracy: {np.sum(y_train_pred == 0) / len(y_train_pred):.2%}\")\n    print(f\"Test Accuracy: {np.sum(y_test_pred == 0) / len(y_test_pred):.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test_pred, np.zeros_like(y_test_pred), zero_division=1))  # Set zero_division to 1\n    print(\"\\n\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=991 height=505}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705\n\n\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=1033 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-5.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-6.png){width=1036 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-7.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-8.png){width=948 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-9.png){width=941 height=505}\n:::\n:::\n\n\nOption 2\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Set up subplots for visualization\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\naxs = axs.flatten()\n\n# Fit the models and predict outliers\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores > threshold).astype(int)\n        y_test_pred = (y_test_scores > threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    inliers_train = X_train[y_train_pred == 0]\n    outliers_train = X_train[y_train_pred == 1]\n    inliers_test = X_test[y_test_pred == 0]\n    outliers_test = X_test[y_test_pred == 1]\n\n    axs[i].scatter(inliers_train[:, 0], inliers_train[:, 1], color='blue', label='Inliers (Train)')\n    axs[i].scatter(outliers_train[:, 0], outliers_train[:, 1], color='red', label='Outliers (Train)')\n    axs[i].scatter(inliers_test[:, 0], inliers_test[:, 1], color='green', label='Inliers (Test)')\n    axs[i].scatter(outliers_test[:, 0], outliers_test[:, 1], color='orange', label='Outliers (Test)')\n    axs[i].set_title(clf_name)\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=1430 height=1430}\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(base_estimator=KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Fit the models and predict outliers\nfor clf_name, clf in classifiers.items():\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores > threshold).astype(int)\n        y_test_pred = (y_test_scores > threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Test Set Predictions')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Training Set Predictions')\n    \n    plt.show()\n\n    # Evaluate the model performance\n    print(f\"------ {clf_name} ------\")\n    print(f\"Train Accuracy: {np.sum(y_train_pred == 0) / len(y_train_pred):.2%}\")\n    print(f\"Test Accuracy: {np.sum(y_test_pred == 0) / len(y_test_pred):.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test_pred, np.zeros_like(y_test_pred), zero_division=1))  # Set zero_division to 1\n    print(\"\\n\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=991 height=505}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705\n\n\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-4.png){width=1033 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-5.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-6.png){width=1036 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-7.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-8.png){width=948 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-9.png){width=941 height=505}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}