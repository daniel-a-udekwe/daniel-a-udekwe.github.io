{
  "hash": "1bb736208d2c2167239e3c86bfdee34e",
  "result": {
    "markdown": "---\ntitle: \"Anomaly/Outlier Detection\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-14\"\ncategories: [news]\nimage: \"avknn.png\"\n---\n\nAnomaly detection is the process of identifying data points, entities or events that fall outside the normal range. An anomaly is anything that deviates from what is standard or expected. Humans and animals do this habitually when they spot a ripe fruit in a tree or a rustle in the grass that stands out from the background and could represent an opportunity or threat. Thus, the concept is sometimes framed as *outlier detection* or *novelty detection*.\n\nAnomaly detection has a long history in statistics, driven by analysts and scientists who pored over charts to find elements that stood out. Over the last several decades, researchers have started automating this process using [machine learning](https://www.techtarget.com/searchenterpriseai/definition/machine-learning-ML) training techniques designed to find more efficient ways to detect different types of outliers.\n\nIn practice, anomaly detection is often used to detect suspicious events, unexpected opportunities or bad data buried in [time series](https://www.techtarget.com/whatis/definition/time-series-forecasting) data. A suspicious event might indicate a network breach, fraud, crime, disease or faulty equipment. An unexpected opportunity could involve finding a store, product or salesperson that's performing much better than others and should be investigated for insight into improving the business.\n\n**What is an anomaly?**\n\nBefore talking about anomaly detection, we need to understand what an **anomaly** is.\n\nGenerally speaking, an anomaly is something that differs from a norm: a deviation, an exception. In software engineering, by anomaly we understand a rare occurrence or event that doesn't fit into the pattern, and, therefore, seems suspicious. Some examples are:\n\n-   sudden burst or decrease in activity;\n\n-   error in the text;\n\n-   sudden rapid drop or increase in temperature.\n\nCommon reasons for outliers are:\n\n-   data preprocessing errors;\n\n-   noise;\n\n-   fraud;\n\n-   attacks.\n\nNormally, you want to catch them all; a software program must run smoothly and be predictable so every outlier is a potential threat to its robustness and security. Catching and identifying anomalies is what we call **anomaly or outlier detection**.\n\nFor example, if large sums of money are spent one after another within one day and it is not your typical behavior, a bank can block your card. They will see an unusual pattern in your daily transactions. This anomaly can typically be connected to fraud since identity thieves try to steal as much money as they can while they can. Once an anomaly is detected, it needs to be investigated, or problems may follow.\n\n## How does anomaly detection work?\n\nThere are several ways of training [machine learning algorithms](https://www.techtarget.com/whatis/definition/machine-learning-algorithm) to detect anomalies. [Supervised machine learning techniques](https://www.techtarget.com/searchenterpriseai/definition/supervised-learning) are used when you have a labeled data set indicating normal vs. abnormal conditions. For example, a bank or credit card company can develop a process for labeling fraudulent credit card transactions after those transactions have been reported. Medical researchers might similarly label images or data sets indicative of future disease diagnosis. In such instances, supervised machine learning models can be trained to detect these known anomalies.\n\nResearchers might start with some previously discovered outliers but suspect that other anomalies also exist. In the scenario of fraudulent credit card transactions, consumers might fail to report suspicious transactions with innocuous-sounding names and of a small value. A data scientist might use reports that include these types of fraudulent transactions to automatically label other like transactions as fraud, using semi-supervised machine learning techniques.\n\n### Supervised vs. unsupervised anomaly detection techniques\n\nThe supervised and semi-supervised techniques can only detect known anomalies. However, the vast majority of data is unlabeled. In these cases, data scientists might use unsupervised anomaly detection techniques, which can automatically identify exceptional or rare events.\n\nFor example, a cloud cost estimator might look for unusual upticks in data egress charges or processing costs that could be caused by a poorly written algorithm. Similarly, an intrusion detection algorithm might look for novel network traffic patterns or a rise in authentication requests. In both cases, [unsupervised machine learning techniques](https://www.techtarget.com/searchenterpriseai/definition/unsupervised-learning) might be used to identify data points indicating things that are well outside the range of normal behavior. In contrast, supervised techniques would have to be explicitly trained using examples of previously known deviant behavior.\n\n## **Different types of anomalies**\n\nBroadly speaking, there are three different types of anomalies.\n\n-   **Global outliers,** or point anomalies, occur far outside the range of the rest of a data set.\n\n-   **Contextual outliers** deviate from other points in the same context, e.g., holiday or weekend sales.\n\n-   **Collective outliers** occur when a range of different types of data vary when considered together, for example, ice cream sales and temperature spikes.\n\n## **Anomaly detection techniques**\n\nMany different kinds of machine learning algorithms can be trained to detect anomalies. Some of the most popular anomaly detection methods include the following:\n\n-   Density-based algorithms determine when an outlier differs from a larger, hence denser normal data set, using algorithms like K-nearest neighbor and Isolation Forest.\n\n-   Cluster-based algorithms evaluate how any point differs from [clusters of related data](https://www.techtarget.com/searchenterpriseai/definition/clustering-in-machine-learning) using techniques like K-means cluster analysis.\n\n-   [Bayesian-network](https://www.techtarget.com/searchenterpriseai/feature/Bayesian-networks-applications-are-fueling-enterprise-support) algorithms develop models for estimating the probability that events will occur based on related data and then identifying significant deviations from these predictions.\n\n-   [Neural network](https://www.techtarget.com/searchenterpriseai/definition/neural-network) algorithms train a neural network to predict an expected time series and then flag deviations.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import models\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.iforest import IForest\nfrom pyod.models.knn import KNN\nfrom pyod.models.lof import LOF\n# reading the big mart sales training data\ndf = pd.read_csv(\"Train.csv\")\ndf.plot.scatter('Item_MRP','Item_Outlet_Sales')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<Axes: xlabel='Item_MRP', ylabel='Item_Outlet_Sales'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=610 height=429}\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\ndf[['Item_MRP','Item_Outlet_Sales']] = scaler.fit_transform(df[['Item_MRP','Item_Outlet_Sales']])\ndf[['Item_MRP','Item_Outlet_Sales']].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Item_MRP</th>\n      <th>Item_Outlet_Sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.927507</td>\n      <td>0.283587</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.072068</td>\n      <td>0.031419</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.468288</td>\n      <td>0.158115</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.640093</td>\n      <td>0.053555</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.095805</td>\n      <td>0.073651</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nOption 2\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = {\n    'Angle-based Outlier Detector (ABOD)': ABOD(contamination=outliers_fraction),\n    'Cluster-based Local Outlier Factor (CBLOF)': CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Feature Bagging': FeatureBagging(KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state),\n    'Histogram-based Outlier Detection (HBOS)': HBOS(contamination=outliers_fraction),\n    'Isolation Forest': IsolationForest(contamination=outliers_fraction, random_state=random_state),\n    'K Nearest Neighbors (KNN)': KNN(contamination=outliers_fraction),\n    'Average KNN': KNN(method='mean', contamination=outliers_fraction)\n}\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Set up subplots for visualization\nfig, axs = plt.subplots(nrows=3, ncols=3, figsize=(15, 15))\naxs = axs.flatten()\n\n# Fit the models and predict outliers\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores > threshold).astype(int)\n        y_test_pred = (y_test_scores > threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    inliers_train = X_train[y_train_pred == 0]\n    outliers_train = X_train[y_train_pred == 1]\n    inliers_test = X_test[y_test_pred == 0]\n    outliers_test = X_test[y_test_pred == 1]\n\n    axs[i].scatter(inliers_train[:, 0], inliers_train[:, 1], color='blue', label='Inliers (Train)')\n    axs[i].scatter(outliers_train[:, 0], outliers_train[:, 1], color='red', label='Outliers (Train)')\n    axs[i].scatter(inliers_test[:, 0], inliers_test[:, 1], color='green', label='Inliers (Test)')\n    axs[i].scatter(outliers_test[:, 0], outliers_test[:, 1], color='orange', label='Outliers (Test)')\n    axs[i].set_title(clf_name)\n    axs[i].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=1430 height=1430}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pyod.models.abod import ABOD\nfrom pyod.models.cblof import CBLOF\nfrom pyod.models.feature_bagging import FeatureBagging\nfrom pyod.models.hbos import HBOS\nfrom pyod.models.knn import KNN\n\n# Load the dataset\ndf = pd.read_csv('train.csv')\n\n# Select relevant features for outlier detection\nfeatures = ['Item_MRP', 'Item_Outlet_Sales']\nX = df[features].values\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Set up the outlier detection algorithms\nrandom_state = np.random.RandomState(42)\noutliers_fraction = 0.05\n\nclassifiers = [\n    ('Angle-based Outlier Detector (ABOD)', ABOD(contamination=outliers_fraction)),\n    ('Cluster-based Local Outlier Factor (CBLOF)', CBLOF(contamination=outliers_fraction, check_estimator=False, random_state=random_state)),\n    ('Feature Bagging', FeatureBagging(base_estimator=KNN(n_neighbors=35), contamination=outliers_fraction, check_estimator=False, random_state=random_state)),\n    ('Histogram-based Outlier Detection (HBOS)', HBOS(contamination=outliers_fraction)),\n    ('Isolation Forest', IsolationForest(contamination=outliers_fraction, random_state=random_state)),\n    ('K Nearest Neighbors (KNN)', KNN(contamination=outliers_fraction)),\n    ('Average KNN', KNN(method='mean', contamination=outliers_fraction))\n]\n\n# Split the data into training and testing sets\nX_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n\n# Fit the models and predict outliers\nindex = 0\nwhile index < len(classifiers):\n    clf_name, clf = classifiers[index]\n    clf.fit(X_train)\n    \n    if clf_name == 'Isolation Forest':\n        # Use decision function to get anomaly scores\n        y_train_scores = clf.decision_function(X_train)\n        y_test_scores = clf.decision_function(X_test)\n        \n        # Set a threshold to classify samples as outliers (you may need to adjust this threshold)\n        threshold = np.percentile(y_train_scores, 100 * outliers_fraction)\n        \n        # Convert continuous scores to binary labels\n        y_train_pred = (y_train_scores > threshold).astype(int)\n        y_test_pred = (y_test_scores > threshold).astype(int)\n    else:\n        y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n        y_test_pred = clf.predict(X_test)\n\n    # Visualize the results\n    plt.figure(figsize=(12, 6))\n    \n    plt.subplot(1, 2, 1)\n    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Test Set Predictions')\n    \n    plt.subplot(1, 2, 2)\n    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_pred, cmap='coolwarm', edgecolors='k', marker='o')\n    plt.title(f'{clf_name} - Training Set Predictions')\n\n    # Evaluate the model performance\n    print(f\"------ {clf_name} ------\")\n    print(f\"Train Accuracy: {np.sum(y_train_pred == 0) / len(y_train_pred):.2%}\")\n    print(f\"Test Accuracy: {np.sum(y_test_pred == 0) / len(y_test_pred):.2%}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test_pred, np.zeros_like(y_test_pred), zero_division=1))  # Set zero_division to 1\n    plt.show()\n    \n    # Increment the index for the next iteration\n    index += 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n------ Angle-based Outlier Detector (ABOD) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.13%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1622\n           1       1.00      0.00      0.00        83\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.95      0.95      0.93      1705\n\n------ Cluster-based Local Outlier Factor (CBLOF) ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.95%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1636\n           1       1.00      0.00      0.00        69\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n------ Feature Bagging ------\nTrain Accuracy: 95.00%\nTest Accuracy: 95.60%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1630\n           1       1.00      0.00      0.00        75\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.93      1705\n\n------ Histogram-based Outlier Detection (HBOS) ------\nTrain Accuracy: 95.28%\nTest Accuracy: 96.01%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.96      1.00      0.98      1637\n           1       1.00      0.00      0.00        68\n\n    accuracy                           0.96      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.96      0.94      1705\n\n------ Isolation Forest ------\nTrain Accuracy: 5.00%\nTest Accuracy: 4.34%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.04      1.00      0.08        74\n           1       1.00      0.00      0.00      1631\n\n    accuracy                           0.04      1705\n   macro avg       0.52      0.50      0.04      1705\nweighted avg       0.96      0.04      0.00      1705\n\n------ K Nearest Neighbors (KNN) ------\nTrain Accuracy: 95.01%\nTest Accuracy: 95.37%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.98      1626\n           1       1.00      0.00      0.00        79\n\n    accuracy                           0.95      1705\n   macro avg       0.98      0.50      0.49      1705\nweighted avg       0.96      0.95      0.93      1705\n\n------ Average KNN ------\nTrain Accuracy: 95.00%\nTest Accuracy: 94.96%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      1.00      0.97      1619\n           1       1.00      0.00      0.00        86\n\n    accuracy                           0.95      1705\n   macro avg       0.97      0.50      0.49      1705\nweighted avg       0.95      0.95      0.92      1705\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=991 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){width=1033 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-4.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-5.png){width=1036 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-6.png){width=941 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-7.png){width=948 height=505}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-8.png){width=941 height=505}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}