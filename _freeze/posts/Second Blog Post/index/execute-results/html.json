{
  "hash": "bb32b0a1cc3c9a2e9e798e39a03c5959",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-14\"\ncategories: [news]\nimage: \"classification.png\"\n---\n\nIn machine learning, classification is a type of supervised learning where the algorithm is trained to categorize input data into predefined classes or categories. The goal is to learn a mapping between the input features and the corresponding class labels based on a set of labeled training data. Essentially, the algorithm learns to generalize from the provided examples and then applies this knowledge to classify new, unseen instances.\n\nThe process involves training a model on a labeled dataset, where each data point has input features and an associated class label. The model learns to recognize patterns and relationships within the input data that are indicative of the different classes. Once trained, the model can predict the class labels for new, unseen data.\n\nSome types of Classification challenges are :\n\n-   Classifying emails as spam or not\n\n-   Classifying a given handwritten character to be either a known character or not\n\n-   Classifying recent user behaviour as churn or not\n\nThere are various classification algorithms, each with its strengths and weaknesses, suited for different types of data and problem domains. Common algorithms include:\n\n-   K-Nearest Neighbours,\n\n-   Logistic Regression\n\n-   Support Vector Machine\n\n-   Naive Bayes\n\n-   Neural Networks\n\n-   Decision Trees\n\nThe choice of algorithm often depends on factors such as the nature of the data, the size of the dataset, and the desired interpretability of the model.\n\nWe will go over them one by one.\n\n# **Binary Classification** \n\nA binary classification refers to those tasks which can give either of any two class labels as the output. Generally, one is considered as the normal state and the other is considered to be the abnormal state.Â  The following examples will help you to understand them better.\n\n-   Email Spam detection:\n\n    Normal State -- Not Spam, Abnormal State -- Spam\n\n-   Conversion prediction:\n\n    Normal State -- Not churned, Abnormal State -- Churn\n\n## K-Nearest Neighbours\n\nK-Nearest Neighbors (KNN) is a simple and intuitive supervised machine learning algorithm used for classification and regression tasks. It is a type of instance-based learning, also known as lazy learning, where the algorithm makes predictions based on the entire training dataset rather than learning a specific model during the training phase.\n\nHere's a basic overview of how the KNN algorithm works:\n\n1.  **Training Phase:**\n\n    -   The algorithm stores all the training examples in memory.\n\n    -   Each example in the training set consists of a set of features and a corresponding class label.\n\n2.  **Prediction Phase:**\n\n    -   When a prediction is needed for a new, unseen data point, the algorithm calculates the distances between that point and all the points in the training set. Common distance metrics include Euclidean distance, Manhattan distance, or others depending on the problem.\n\n    -   The \"k\" nearest neighbors to the new data point are identified based on the calculated distances. \"K\" is a user-defined parameter representing the number of neighbors to consider.\n\n    -   For a classification task, the algorithm assigns the class label that is most frequent among the k neighbors. In regression tasks, the algorithm may return the average or weighted average of the target values of the k neighbors.\n\nKNN is a versatile algorithm with some key characteristics:\n\n-   **Non-parametric:** KNN doesn't make any assumptions about the underlying data distribution. It adapts to the data during the training phase.\n\n-   **Instance-based:** Instead of building an explicit model during training, KNN stores the entire dataset and makes predictions based on the similarities between instances.\n\n-   **Simple and interpretable:** KNN is easy to understand and implement, making it a good choice for quick prototyping and baseline models.\n\nHowever, KNN has some limitations, such as being sensitive to irrelevant or redundant features, computation complexity (especially for large datasets), and a lack of interpretability for the decision-making process.\n\nChoosing the appropriate value for \"k\" is crucial, as a small k may lead to overfitting, and a large k may introduce bias. The optimal value of \"k\" often depends on the specific dataset and problem at hand.\n\n### Implementation\n\nNow, let's implement K-Nearest neighbours on the scikit learn breast cancer dataset to classify malignant and benign cancers\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the KNN classifier\nk = 5  # You can choose your desired value for k\nknn_classifier = KNeighborsClassifier(n_neighbors=k)\nknn_classifier.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = knn_classifier.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries on the training set\nplot_decision_boundary(X_train_2d, y_train, knn_classifier, 'KNN Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries on the test set\nplot_decision_boundary(X_test_2d, y_test, knn_classifier, 'KNN Classification (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 97.37%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=553 height=492}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-4.png){width=596 height=449}\n:::\n:::\n\n\n## Logistic Regression\n\nLogistic Regression is a statistical method and a popular machine learning algorithm used for binary classification problems. Despite its name, it is used for classification rather than regression. Logistic Regression models the probability that a given input belongs to a particular category. It's widely employed in various fields, such as medicine (disease prediction), marketing (customer churn analysis), and finance (credit scoring).\n\nHere's a brief overview of how logistic regression works:\n\n1.  **Sigmoid Function:**\n\n    -   Logistic Regression uses the logistic function, also called the sigmoid function, to model the probability.\n\n    -   The sigmoid function has an S-shaped curve and maps any real-valued number to the range \\[0, 1\\]. The formula for the sigmoid function is:\n        $\\sigma(z) = \\frac{1}{1+ e^{-z}}$\n        where $\\sigma(z)$ is the linear combination of input features and weights.\n\n2.  **Linear Combination:**\n\n    -   Logistic Regression establishes a linear relationship between the input features and the log-odds (logit) of the probability of belonging to the positive class.\n\n    -   The linear combination is given by:\n        $z = b_0 + b_1\\times x_1 + b_2\\times x_2 + ... + b_n \\times x_n$\n        where $b_0, b_1, ... , b_n$ are the coefficients (weights) and $x_1, x_2, ... , x_n$ are the input features.\n\n3.  **Probability Prediction:**\n\n    -   The output of the sigmoid function is interpreted as the probability that the given input belongs to the positive class. If $\\sigma(z)$ is close to 1, the model predicts a high probability of belonging to the positive class; if close to 0, it predicts a low probability.\n\n4.  **Decision Boundary:**\n\n    -   A decision boundary is established by the model based on a threshold probability (commonly 0.5). If the predicted probability is above the threshold, the instance is classified as the positive class; otherwise, it's classified as the negative class.\n\nTraining a logistic regression model involves finding the optimal weights that maximize the likelihood of the observed data given the model. This is typically done using optimization algorithms like gradient descent.\n\nLogistic Regression is advantageous for its simplicity, interpretability, and efficiency. However, it assumes a linear relationship between the features and the log-odds, which may not hold in all situations. Extensions like polynomial logistic regression can be used to capture non-linear relationships.\n\n### Implementation\n\nNow, let's implement logistic regression on the breast cancer dataset to classify malignant and benign cancers\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\nlogreg_model.fit(X_train_2d, y_train)  # Use the reduced dimensionality for training\n\n# Make predictions on the test set\ny_pred = logreg_model.predict(X_test_2d)  # Use the same dimensionality for testing\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\n\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for logistic regression on the training set\nplot_decision_boundary(X_train_2d, y_train, logreg_model, 'Logistic Regression (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for logistic regression on the test set\nplot_decision_boundary(X_test_2d, y_test, logreg_model, 'Logistic Regression (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 99.12%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=553 height=492}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-3.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-4.png){width=596 height=449}\n:::\n:::\n\n\n## Support Vector Machine\n\nA Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its primary objective is to find a hyperplane in a high-dimensional space that best separates data points into different classes. In the context of classification, the SVM aims to create a decision boundary that maximizes the margin between classes.\n\nHere are key concepts and features of Support Vector Machines:\n\n1.  **Hyperplane:**\n\n    -   In a two-dimensional space, a hyperplane is a simple line. In higher dimensions, it becomes a hyperplane, which is a subspace of one dimension less than the ambient space. For a binary classification problem, the SVM seeks the hyperplane that best separates data points of different classes.\n\n2.  **Margin:**\n\n    -   The margin is the distance between the hyperplane and the nearest data point from either class. SVMs strive to maximize this margin because a larger margin generally leads to better generalization performance on unseen data.\n\n3.  **Support Vectors:**\n\n    -   Support vectors are the data points that are closest to the hyperplane and have the most influence on determining its position. These are the critical elements for defining the margin and decision boundary.\n\n4.  **Kernel Trick:**\n\n    -   SVMs can handle non-linear decision boundaries by using a kernel trick. The kernel function transforms the input features into a higher-dimensional space, making it possible to find a hyperplane in this transformed space. Common kernels include polynomial kernels and radial basis function (RBF) kernels.\n\n5.  **C Parameter:**\n\n    -   The C parameter in SVM is a regularization parameter that controls the trade-off between achieving a smooth decision boundary and classifying the training points correctly. A smaller C value allows for a more flexible decision boundary (potentially with some misclassifications), while a larger C value enforces a stricter boundary.\n\nSVMs have several advantages:\n\n-   Effective in high-dimensional spaces.\n\n-   Versatile due to the kernel trick, enabling them to handle complex relationships in the data.\n\n-   Resistant to overfitting, especially in high-dimensional spaces.\n\n### Implementation\n\nNow, lets use support vector machines to classify malignant and benign cancers\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction separately on the training and testing sets\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the SVM model\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = svm_model.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n#print('Confusion Matrix:')\n#print(conf_matrix)\n\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for SVM on the training set\nplot_decision_boundary(X_train_2d, y_train, svm_model, 'SVM Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for SVM on the test set\nplot_decision_boundary(X_test_2d, y_test, svm_model, 'SVM Classification (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 99.12%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=553 height=492}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-4.png){width=596 height=449}\n:::\n:::\n\n\n## Naive Bayes\n\nNaive Bayes is a family of probabilistic algorithms used for classification and, in some cases, regression tasks. It is based on Bayes' theorem, which is a mathematical formula that describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n\nHere are the key concepts of Naive Bayes:\n\n1.  **Bayes' Theorem:**\n\n    -   Bayes' theorem relates the conditional and marginal probabilities of random events. For a classification problem, it can be expressed as:\n        $$P(y|X) = \\frac{P(X|y) \\times P(y)}{P(X)}$$\n        where:\n\n        -    $P(y|X)$ is the posterior probability of class $y$ given the features $X$,\n\n        -    $P(X|y)$ is the likelihood of the features given the class,\n\n        -    $P(y)$ is the prior probability of class $y$,\n\n        -    $P(X)$ is the probability of the features.\n\n2.  **Naive Assumption:**\n\n    -   The \"naive\" in Naive Bayes comes from the assumption that features are conditionally independent given the class label. This means that the presence of one feature is considered independent of the presence of any other feature, given the class label. While this assumption simplifies the model, it may not always hold in real-world scenarios.\n\n3.  **Types of Naive Bayes:**\n\n    -   There are different variants of Naive Bayes, depending on the distributional assumptions made about the data. The three most common types are:\n\n        -   **Gaussian Naive Bayes:** Assumes that the features follow a normal distribution.\n\n        -   **Multinomial Naive Bayes:** Used for discrete data, often for text classification with word frequencies.\n\n        -   **Bernoulli Naive Bayes:** Assumes binary (0 or 1) features, often used for text classification with binary term presence/absence.\n\n4.  **Text Classification:**\n\n    -   Naive Bayes is particularly popular in text classification tasks, such as spam filtering and sentiment analysis. It works well with high-dimensional data like word counts in documents.\n\n5.  **Training and Prediction:**\n\n    -   During training, Naive Bayes estimates the parameters (probabilities) from the training dataset.\n\n    -   During prediction, it calculates the posterior probability for each class and assigns the class with the highest probability to the input instance.\n\nNaive Bayes is computationally efficient, simple to implement, and often performs surprisingly well, especially in text and document classification tasks. However, its performance may degrade when the independence assumption is strongly violated or when dealing with highly correlated features.\n\n### Implementation\n\nNow, lets implement Naive Bayes to classify malignant and benign cancers using Gaussian Naive Bayes\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train the Naive Bayes model\nnb_model = GaussianNB()\nnb_model.fit(X_train_scaled, y_train)\n\n# Make predictions on the test set\ny_pred = nb_model.predict(X_test_scaled)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\n\n# Plot Confusion Matrix\nplt.figure(figsize=(8, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_matrix[i, j]), ha='center', va='center')\nplt.show()\n\n# Plot Classification Results\nplt.figure(figsize=(8, 6))\ncorrectly_classified = (y_test == y_pred)\nincorrectly_classified = (y_test != y_pred)\n\n# Plot correctly classified points\nsns.scatterplot(x=X_test_scaled[correctly_classified, 0], y=X_test_scaled[correctly_classified, 1], color='green', label='Correctly Classified', marker='o')\n\n# Plot incorrectly classified points\nsns.scatterplot(x=X_test_scaled[incorrectly_classified, 0], y=X_test_scaled[incorrectly_classified, 1], color='red', label='Incorrectly Classified', marker='x')\n\nplt.title('Classification Results')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 96.49%\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=634 height=566}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-3.png){width=662 height=523}\n:::\n:::\n\n\n## Neural Networks\n\nNeural networks, or artificial neural networks (ANNs), are a class of machine learning models inspired by the structure and functioning of the human brain. They consist of interconnected nodes, known as neurons or artificial neurons, organized into layers. Neural networks are a fundamental component of deep learning, a subfield of machine learning that focuses on models with multiple layers, also known as deep neural networks.\n\nHere are the key components and concepts related to neural networks:\n\n1.  **Neurons:**\n\n    -   Neurons are the basic units of a neural network. They receive inputs, perform a weighted sum of those inputs, apply an activation function, and produce an output. The output is then passed to the next layer of neurons.\n\n2.  **Layers:**\n\n    -   Neural networks are organized into layers, typically divided into three types:\n\n        -   **Input Layer:** Neurons that receive the initial input data.\n\n        -   **Hidden Layers:** Neurons that process the input data. Deep neural networks have multiple hidden layers.\n\n        -   **Output Layer:** Neurons that produce the final output of the network.\n\n3.  **Connections and Weights:**\n\n    -   Neurons in one layer are connected to neurons in the next layer by connections. Each connection has an associated weight that determines the strength of the connection. During training, these weights are adjusted to optimize the network's performance.\n\n4.  **Activation Function:**\n\n    -   The activation function introduces non-linearity to the network, allowing it to learn complex relationships in the data. Common activation functions include sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).\n\n5.  **Feedforward and Backpropagation:**\n\n    -   During the feedforward phase, input data is passed through the network to generate predictions. The predictions are compared to the actual targets, and the error is calculated.\n\n    -   Backpropagation is the process of iteratively adjusting the weights of the connections based on the calculated error. This is done using optimization algorithms like gradient descent to minimize the error and improve the model's performance.\n\n6.  **Deep Learning:**\n\n    -   Neural networks with multiple hidden layers are referred to as deep neural networks. The depth of the network allows it to learn hierarchical features and representations, making it capable of handling complex tasks.\n\n7.  **Types of Neural Networks:**\n\n    -   Different types of neural networks are designed for specific tasks. For example:\n\n        -   **Feedforward Neural Networks (FNN):** Standard neural networks where information flows in one direction, from input to output.\n\n        -   **Convolutional Neural Networks (CNN):** Effective for image-related tasks, with specialized layers for feature extraction.\n\n        -   **Recurrent Neural Networks (RNN):** Suitable for sequential data, with connections that form cycles to capture temporal dependencies.\n\nNeural networks have achieved remarkable success in various domains, including image and speech recognition, natural language processing, and playing games. Their power lies in their ability to automatically learn complex patterns and representations from data, enabling them to excel in tasks that traditional algorithms may struggle with.\n\n### Implementation\n\nNow, let's use a neural network to classify malignant or benign cancers\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_breast_cancer\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the neural network model using Keras\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification, so use 'sigmoid' activation\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Visualize the model architecture\nplot_model(model, to_file='neural_network.png', show_shapes=True, show_layer_names=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot training history\nplt.figure(figsize=(12, 5))\n\n# Plot training & validation accuracy values\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n\r1/4 [======>.......................] - ETA: 0s - loss: 0.1449 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 0s 960us/step - loss: 0.0669 - accuracy: 0.9825\nTest Loss: 0.0669\nTest Accuracy: 0.9825\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=1142 height=470}\n:::\n:::\n\n\n## Decision Trees\n\nA Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It's a tree-like structure where each internal node represents a decision based on the value of a particular feature, each branch represents the outcome of that decision, and each leaf node represents the final decision or prediction.\n\nHere are the key concepts of decision trees:\n\n1.  **Node Types:**\n\n    -   **Root Node:** The topmost node that makes the initial decision.\n\n    -   **Internal Nodes:** Nodes that represent decisions based on feature values.\n\n    -   **Leaf Nodes:** Terminal nodes that provide the final prediction or decision.\n\n2.  **Decision Criteria:**\n\n    -   At each internal node, a decision is made based on the value of a specific feature. The goal is to make decisions that result in the most accurate predictions.\n\n3.  **Splitting:**\n\n    -   The process of dividing a node into two or more child nodes based on a chosen feature and a threshold value. The goal is to increase the homogeneity of the target variable within each resulting node.\n\n4.  **Homogeneity and Impurity:**\n\n    -   Decision trees aim to create nodes that are as pure as possible. Impurity measures, such as Gini impurity or entropy, are used to quantify the homogeneity within a node. The goal is to minimize impurity during the tree-building process.\n\n5.  **Tree Pruning:**\n\n    -   Decision trees can become too complex and overfit the training data. Pruning involves removing some branches (subtrees) from the tree to prevent overfitting and improve generalization to new data.\n\n6.  **Categorical and Continuous Variables:**\n\n    -   Decision trees can handle both categorical and continuous features. For categorical features, the tree performs a split for each category, while for continuous features, the tree finds an optimal threshold to split the data.\n\n7.  **Ensemble Methods:**\n\n    -   Decision trees are often used in ensemble methods, such as Random Forests and Gradient Boosting, to enhance predictive performance. Ensemble methods combine the predictions of multiple decision trees to achieve more robust and accurate results.\n\n8.  **Interpretability:**\n\n    -   Decision trees are known for their interpretability. The structure of the tree provides a clear and intuitive representation of the decision-making process, making it easy to understand how the model arrives at its predictions.\n\nDecision trees are used in various applications, including finance, healthcare, and marketing. They are particularly useful when dealing with a mix of categorical and numerical features and are valued for their simplicity, interpretability, and ability to handle non-linear relationships in the data.\n\n### Implementation\n\nNow lets use a Decision Tree to classify malignant and benign cancers\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot the confusion matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nplt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_mat[i, j]), ha='center', va='center')\n\nplt.show()\n\n# Plot the decision tree\nplt.figure(figsize=(15, 10))\ntree.plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 0.9474\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=553 height=492}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-3.png){width=1135 height=758}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}