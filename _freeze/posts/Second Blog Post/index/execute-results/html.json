{
  "hash": "01165288f60f4176adcdcc1c85bafb9e",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-14\"\ncategories: [news]\nimage: \"classification1.png\"\n---\n\nClassification is a task of Machine Learning which assigns a label value to a specific class and then can identify a particular type to be of one kind or another. The most basic example can be of the mail spam filtration system where one can classify a mail as either \"spam\" or \"not spam\".\n\nClassification usually refers to any kind of problem where a specific type of class label is the result to be predicted from the given input field of data. Some types of Classification challenges are :\n\n-   Classifying emails as spam or not\n\n-   Classify a given handwritten character to be either a known character or not\n\n-   Classify recent user behaviour as churn or not\n\nFor any model, you will require a training dataset with many examples of inputs and outputs from which the model will train itself. The training data must include all the possible scenarios of the problem and must have sufficient data for each label for the model to be trained correctly. Class labels are often returned as string values and hence needs to be encoded into an integer like either representing 0 for \"spam\" and 1 for \"no-spam\".\n\nThere are mainly 4 different types of classification tasks that you might encounter in your day to day challenges. Generally, the different types of predictive models in machine learning are as follows :\n\n-   Binary classification\n\n-   Multi-Label Classification\n\n-   Multi-Class Classification\n\n-   Imbalanced Classification\n\nWe will go over them one by one.\n\n## **Binary Classification for Machine Learning**\n\nA binary classification refers to those tasks which can give either of any two class labels as the output. Generally, one is considered as the normal state and the other is considered to be the abnormal state.Â  The following examples will help you to understand them better.\n\n-   Email Spam detection: Normal State -- Not Spam, Abnormal State -- Spam\n\n-   Conversion prediction: Normal State -- Not churned, Abnormal State -- Churn\n\n-   Conversion Prediction: Normal State -- Bought an item, Abnormal State -- Not bought an item\n\nYou can also add the example of that \" No cancer detected\" to be a normal state and \" Cancer detected\" to be the abnormal state. The notation mostly followed is that the normal state gets assigned the value of 0 and the class with the abnormal state gets assigned the value of 1. For each example, one can also create a model which predicts the Bernoulli probability for the output. You can read more about the probability [here](https://en.wikipedia.org/wiki/Bernoulli_distribution). In short, it returns a discrete value that covers all cases and will give the output as either the outcome will have a value of 1 or 0. Hence after the association to two different states, the model can give an output for either of the values present.\n\nThe most popular algorithms which are used for binary classification are :\n\n-   K-Nearest Neighbours\n\n-   Logistic Regression\n\n-   Support Vector Machine\n\n-   Decision Trees\n\n-   Naive Bayes\n\n-   Neural Networks\n\n-   etc.\n\n# K-Nearest Neighbours\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the KNN classifier\nk = 5  # You can choose your desired value for k\nknn_classifier = KNeighborsClassifier(n_neighbors=k)\nknn_classifier.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = knn_classifier.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries on the training set\nplot_decision_boundary(X_train_2d, y_train, knn_classifier, 'KNN Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries on the test set\nplot_decision_boundary(X_test_2d, y_test, knn_classifier, 'KNN Classification (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 97.37%\nConfusion Matrix:\n[[42  1]\n [ 2 69]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=596 height=449}\n:::\n:::\n\n\n# Logistic Regression\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the Logistic Regression model\nlogreg_model = LogisticRegression(random_state=42)\nlogreg_model.fit(X_train_2d, y_train)  # Use the reduced dimensionality for training\n\n# Make predictions on the test set\ny_pred = logreg_model.predict(X_test_2d)  # Use the same dimensionality for testing\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for logistic regression on the training set\nplot_decision_boundary(X_train_2d, y_train, logreg_model, 'Logistic Regression (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for logistic regression on the test set\nplot_decision_boundary(X_test_2d, y_test, logreg_model, 'Logistic Regression (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 99.12%\nConfusion Matrix:\n[[42  1]\n [ 0 71]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-3.png){width=596 height=449}\n:::\n:::\n\n\n# Support Vector Machine\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Apply PCA for dimensionality reduction separately on the training and testing sets\npca = PCA(n_components=2)\nX_train_2d = pca.fit_transform(X_train_scaled)\nX_test_2d = pca.transform(X_test_scaled)\n\n# Train the SVM model\nsvm_model = SVC(kernel='linear', random_state=42)\nsvm_model.fit(X_train_2d, y_train)\n\n# Make predictions on the test set\ny_pred = svm_model.predict(X_test_2d)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Plot the decision boundaries\ndef plot_decision_boundary(X, y, classifier, title):\n    h = 0.02  # step size in the mesh\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    \n    plt.title(title)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n\n# Plot the decision boundaries for SVM on the training set\nplot_decision_boundary(X_train_2d, y_train, svm_model, 'SVM Classification (Training Set)')\nplt.show()\n\n# Plot the decision boundaries for SVM on the test set\nplot_decision_boundary(X_test_2d, y_test, svm_model, 'SVM Classification (Test Set)')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 99.12%\nConfusion Matrix:\n[[42  1]\n [ 0 71]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=600 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-3.png){width=596 height=449}\n:::\n:::\n\n\n# Naive Bayes\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train the Naive Bayes model\nnb_model = GaussianNB()\nnb_model.fit(X_train_scaled, y_train)\n\n# Make predictions on the test set\ny_pred = nb_model.predict(X_test_scaled)\n\n# Calculate accuracy and display confusion matrix\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy * 100:.2f}%')\nprint('Confusion Matrix:')\nprint(conf_matrix)\n\n# Concatenate the training features and labels for pair plot\ntrain_data = np.column_stack((X_train_scaled, y_train))\n\n# Convert to DataFrame for easier plotting with seaborn\ntrain_df = pd.DataFrame(data=train_data, columns=data.feature_names.tolist() + ['target'])\n\n# Select a subset of features for the pair plot\nselected_features = ['mean radius', 'mean texture', 'mean smoothness', 'target']\n\n# Create a pair plot\nsns.pairplot(train_df[selected_features], hue='target', palette='viridis', markers=['o', 's'])\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 96.49%\nConfusion Matrix:\n[[40  3]\n [ 1 70]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=787 height=711}\n:::\n:::\n\n\n# Neural Networks\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import load_breast_cancer\nfrom tensorflow.keras.utils import plot_model\nimport matplotlib.pyplot as plt\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Build the neural network model using Keras\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification, so use 'sigmoid' activation\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Visualize the model architecture\nplot_model(model, to_file='neural_network.png', show_shapes=True, show_layer_names=True)\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f\"Test Loss: {loss:.4f}\")\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot training history\nplt.figure(figsize=(12, 5))\n\n# Plot training & validation accuracy values\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'], loc='upper left')\n\n# Plot training & validation loss values\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'], loc='upper left')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYou must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n\r1/4 [======>.......................] - ETA: 0s - loss: 0.1544 - accuracy: 0.9688\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r4/4 [==============================] - 0s 759us/step - loss: 0.0572 - accuracy: 0.9912\nTest Loss: 0.0572\nTest Accuracy: 0.9912\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-2.png){width=1142 height=470}\n:::\n:::\n\n\n# Decision Trees\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Load Breast Cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a decision tree classifier\nclf = DecisionTreeClassifier(random_state=42)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Plot the confusion matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nplt.imshow(conf_mat, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = ['Benign', 'Malignant']\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes, rotation=45)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, str(conf_mat[i, j]), ha='center', va='center')\n\nplt.show()\n\n# Plot the decision tree\nplt.figure(figsize=(15, 10))\ntree.plot_tree(clf, feature_names=data.feature_names, class_names=data.target_names, filled=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 0.9474\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=553 height=492}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-3.png){width=1135 height=758}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}