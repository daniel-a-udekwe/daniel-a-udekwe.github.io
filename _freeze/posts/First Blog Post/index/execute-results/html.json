{
  "hash": "c5f60b0983089fa80b3eef7569295704",
  "result": {
    "markdown": "---\ntitle: \"Regression (Linear and Nonlinear)\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-17\"\ncategories: [data, code, analysis]\nimage: \"regression.png\"\n---\n\nRegression is a statistical technique in machine learning and statistics that aims to establish a relationship between a **dependent variable (target)** and one or more **independent variables (features or predictors)**. The primary goal of regression analysis is to model the relationship between these variables, enabling the prediction or estimation of the dependent variable based on the values of the independent variables.\n\nIn simpler terms, regression helps us understand how the changes in one or more variables are associated with changes in another variable. It is widely used for prediction and forecasting, making it a fundamental tool in various fields, including finance, economics, biology, and social sciences.\n\nThere are several types of regression models, with linear regression being one of the most common. In linear regression, the relationship between the variables is modeled as a linear equation, representing a straight line on a graph. However, regression is not limited to linear relationships; non-linear regression models can capture more complex patterns, accommodating scenarios where the relationship between variables is curved or follows a different pattern.\n\nThe training process in regression involves fitting the model to historical data, allowing the algorithm to learn the underlying patterns. Once trained, the regression model can be used to make predictions on new, unseen data, providing valuable insights and aiding decision-making processes.\n\nIn this post, we will consider the implementation of linear and nonlinear regression for predicting house prices based on size\n\n## Simple Linear Regression\n\nSimple Linear Regression is a fundamental and straightforward form of regression analysis where the relationship between two variables is modeled using a linear equation. In this case, there are two variables: one is considered the independent variable (often denoted as $X$), and the other is the dependent variable (often denoted as $Y$).\n\n### Model Representation\n\nThe equation for simple linear regression is represented as:\n\n$$Y = \\beta_0 + \\beta_1 X + \\epsilon$$\n\nThe $\\beta_1$ is called a scale factor or **coefficient** and $\\beta_0$ is called **bias coefficient**. The bias coefficient gives an extra degree of freedom to this model and $\\epsilon$ is the error term accounting for unobserved factors that affect Y but are not accounted for by the model\n\nThis equation is similar to the line equation $y = mx +b$ with $m = \\beta_1$(Slope) and $b = \\beta_0$(Intercept). So in this Simple Linear Regression model we want to draw a line between X and Y which estimates the relationship between X and Y.\n\nBut how do we find these coefficients? That's the learning procedure. We can find these using different approaches. One is called **Ordinary Least Square Method** and other one is called **Gradient Descent Approach**.\n\n### Ordinary Least Square Method\n\nThe Ordinary Least Squares (OLS) method is a common approach used in linear regression to estimate the parameters of the linear equation by minimizing the sum of the squared differences between the observed and predicted values of the dependent variable. In simple terms, OLS aims to find the best-fitting line through the data points.\n\nLet's say we have few inputs and outputs plotted in a 2D space with a scatter plot to yield the following image:\n\n![](regression_1.png){fig-align=\"center\"}\n\nFor a simple linear regression model with the equation given above, the OLS method seeks to find the values $\\beta_0$, $\\beta_1$ of the linear model that minimize the sum of squared residuals.\n\nA good model will always have least error and we can find this line by reducing the error. The error of each point is the distance between line and that point. This is illustrated as follows.\n\n![](regression_2.jpeg){fig-align=\"center\"}\n\nAnd total error of this model is the sum of all errors of each point. ie.\n\n$$\nD = \\sum_{i=1}^{m} d_i^2\n$$\n\n$d_i$ - Distance between line and i^th^ point.\n\n$m$ - Total number of points\n\nYou may have observed that we are taking the square of each distance. This is done because certain points lie above the line while others lie below it. By minimizing D, we aim to reduce the error in the model.\n\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n$$\n\n$$\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n$$\n\nIn these equations $\\bar{x}$ is the mean value of input variable $x$ and $\\bar{y}$ is the mean value of output variable $y$\n\nNow we have the *Ordinary Least Square Method* which is described with the following equations\n\n$$\nY = \\beta_0 + \\beta_1X\n$$\n\n$$\n\\beta_1 = \\frac{\\sum_{i=1}^{m} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{m} (x_i - \\bar{x})^2}\n$$\n\n$$\n\\beta_0 = \\bar{y} - \\beta_1\\bar{x}\n$$\n\n### Implementation\n\nWe are going to use a dataset containing the price of houses as a function of size to implement regression. This data is split into 3 for training, validation and finally testing. Let's start off by importing and viewing the data.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#Import the needed libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport numpy as np\n\nwith open('assignment1.json', 'r') as json_file:\n    data = json.load(json_file)\n    \nX_train = np.array(data['X_train']).reshape(-1, 1)\ny_train = np.array(data['Y_train']).reshape(-1, 1)\nX_val = np.array(data['X_val']).reshape(-1, 1)\ny_val = np.array(data['Y_val']).reshape(-1, 1)\nX_test = np.array(data['X_test']).reshape(-1, 1)\ny_test = np.array(data['Y_test']).reshape(-1, 1)\n\n\narray1 = data['X_train']\narray2 = data['Y_train']\n\narray3 = data['X_val']\narray4 = data['Y_val']\n\narray5 = data['X_test']\narray6 = data['Y_test']\n\n\n\nprint(\"Training Data\")\ntableData = {\n  'X_train': array1,\n  'Y_train': array2,\n}\ntable = pd.DataFrame(tableData)\nprint(table.head(3))\nprint(f\"number of rows and colums: {table.shape}\")\nprint()\n\nprint(\"Validation Data\")\ntableData2 = {\n  'X_Val': array3,\n  'Y_Val': array4\n}\ntable2 = pd.DataFrame(tableData2)\nprint(table2.head(3))\nprint(f\"number of rows and colums: {table2.shape}\")\nprint()\n\nprint(\"Test Data\")\ntableData3 = {\n  'X_test': array5,\n  'Y_test': array6\n}\ntable3 = pd.DataFrame(tableData3)\nprint(table3.head(3))\nprint(f\"number of rows and colums: {table3.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining Data\n   X_train  Y_train\n0    661.5     4.25\n1    465.0     2.30\n2   1442.0    68.00\nnumber of rows and colums: (12, 2)\n\nValidation Data\n    X_Val   Y_Val\n0   650.0   4.170\n1   682.5   4.067\n2  1417.0  31.870\nnumber of rows and colums: (21, 2)\n\nTest Data\n   X_test  Y_test\n0   405.0   3.316\n1   330.0   5.400\n2   135.0   1.300\nnumber of rows and colums: (21, 2)\n```\n:::\n:::\n\n\nAs we can see, the data is split unequally between training, validation and testing. The training data has 12 entries while the validation and testing data have 21 entries.\n\nwe need to implement feature scaling.\n\nFeature scaling is a preprocessing step in machine learning that involves adjusting the scale of the input features to a similar range. The goal is to ensure that all features contribute equally to the model training process, preventing certain features from dominating due to their larger scales.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# Step 2: Preprocess the data (feature scaling)\nmean = np.mean(X_train)\nstd = np.std(X_train)\nX_train = (X_train - mean) / std\nX_val = (X_val - mean) / std\nX_test = (X_test - mean) / std\n```\n:::\n\n\nNext, we will find a linear relationship house prices and sizes but it is important to visualize this data on a scatter plot. But first\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nplt.scatter(array1, array2, label='training data')\nplt.scatter(array3, array4, label='validation data')\nplt.scatter(array5, array6, label='testing data')\n\nplt.ylabel('House Price')\nplt.xlabel('House size')\nplt.legend()\nplt.title('Training, Validation and Testing Data')\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nText(0.5, 1.0, 'Training, Validation and Testing Data')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=585 height=449}\n:::\n:::\n\n\nLets create a function to implement linear regression with L2 regularization.\n\nLinear regression with regularization is an extension of traditional linear regression that incorporates regularization techniques to prevent overfitting and improve the model's generalization performance. The two common types of regularization used in linear regression are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization).\n\n$$\nJ(\\theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\theta_j^2)\n$$\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Step 3: Implement regularized linear regression with gradient descent\ndef ridge_regression(X, y, alpha, num_iterations, learning_rate):\n    m, n = X.shape\n    # Initialize theta with the correct shape\n    theta = np.zeros((n, 1))\n    history = []\n\n    for _ in range(num_iterations):\n        gradient = (X.T @ (X @ theta - y) + alpha * theta) / m\n        theta -= learning_rate * gradient\n        cost = np.sum((X @ theta - y) ** 2) / (2 * m) + (alpha / (2 * m)) * np.sum(theta[1:] ** 2)\n        history.append(cost)\n\n    return theta, history, cost\n```\n:::\n\n\nIn the equation above, $\\lambda$ is the regularization parameter which ensures a balance between the trade-off between fitting the training data well and keeping the model simple. This is implemented with the gradient descent algorithm.\n\n### Gradient Descent\n\nGradient Descent is an optimization algorithm. We will optimize our cost function using Gradient Descent Algorithm.\n\n#### Step 1\n\nInitialize values $\\theta_0, \\theta_1, ..., \\theta_n$ with some value. In this case we will initialize with 0.\n\n#### Step 2\n\nIteratively update,\n\n$$\n\\theta_j : \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)\n$$\n\nuntil it converges, where:\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)},  \\ j = 0\n$$\n\n$$\n\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j \\ j\\ge 1 \n$$\n\nThis is the procedure. Here $\\alpha$ is the learning rate. This operation $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ means we are finding partial derivative of cost with respect to each $\\theta_j$. This is called Gradient.\n\nIn step 2 we are changing the values of $\\theta_j$. in a direction in which it reduces our cost function. And Gradient gives the direction in which we want to move. Finally we will reach the minima of our cost function. But we don't want to change values of $\\theta_j$. drastically, because we might miss the minima. That's why we need learning rate.\n\n![ Animation illustrating the gradient descent method](regression%20_4.gif){fig-align=\"center\"}\n\nThe above animation illustrates the Gradient Descent method.\n\nAfter making substitutions, Step 2 becomes:\n\n$$\n\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}, + \\frac{\\lambda}{m}\\theta_j\n$$\n\nWe iteratively change values of $\\theta_j$ according to above equation. This particular method is called **Batch Gradient Descent**.\n\nThen we need to implement a function to plot the line of best fit\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef plot_data_and_fit(X_train, y_train, theta):\n    plt.scatter(X_train, y_train, color='blue', label='Training data')\n    \n    x_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\n    x_values_extended = np.column_stack((np.ones_like(x_values), x_values))\n    y_values = x_values_extended @ theta\n    \n    plt.plot(x_values, y_values, color='red', label='Line of best fit')\n    plt.xlabel('X')\n    plt.ylabel('y')\n    plt.title('Training Data and Line of Best Fit')\n    plt.legend()\n    plt.show()\n```\n:::\n\n\nNext we will train the model using the training data and return the cost using the equations below:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Step 4: Train the model using the training data\nalpha = 0 # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\nX_train_extended = np.column_stack((np.ones_like(X_train), X_train))\ntheta, history, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\nprint(f\"the cost iss: {cost}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthe cost iss: 86.7594520168758\n```\n:::\n:::\n\n\nThen tuning the regularization parameter\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    X_val_extended = np.column_stack((np.ones_like(X_val), X_val))\n    theta_val, history_, cost = ridge_regression(X_train_extended, y_train, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_extended @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest regularization parameter (alpha): 10\n```\n:::\n:::\n\n\nEvaluating the Model on the Testing data using the root mean square of errors.\n\nRoot Mean Squared Error is the square root of sum of all errors divided by number of values, or Mathematically,\n\n$$\nRMSE = \\sqrt{\\sum_{i=1}^m \\frac{1}{m}(\\hat{y_1}-y_i)^2)}\n$$\n\nHere $\\hat{y_i}$ is the $i^{th}$ predicted output values.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Step 6: Evaluate the model using the testing data\nX_test_extended = np.column_stack((np.ones_like(X_test), X_test))\ntheta_test, _, cost = ridge_regression(X_train_extended, y_train, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_extended @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\nprint(cost)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error on Test Data: 81.66385694629241\n234.9787954527312\n```\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Plot the cost history during training\nplt.plot(range(num_iterations), history)\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\nplt.title('Cost vs. Iteration')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=593 height=449}\n:::\n:::\n\n\nPlotting the training data and line of best fit\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Plot training data and line of best fit\nplot_data_and_fit(X_train, y_train, theta)\nprint(theta)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=596 height=449}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[[22.10833333]\n [20.73627522]]\n```\n:::\n:::\n\n\n$$\nHouse Price = 22.108 + 20.736 \\times House size\n$$\n\n------------------------------------------------------------------------\n\n## Nonlinear Regression\n\nNonlinear regression is a type of regression analysis where the relationship between the independent variable(s) and the dependent variable is modeled as a nonlinear function. In contrast to linear regression, which assumes a linear relationship between variables, nonlinear regression allows for more complex and curved relationships to be captured.\n\nThe general form of nonlinear regression is expressed as:\n\n$$\nY = f(X, \\theta) + \\epsilon\n$$\n\nWhere:\n\n-   $Y$ is the dependent variable\n\n-   $X$ is the independent variable(s)\n\n-    $\\theta$ represents the parameters of the nonlinear function $f$\n\n-    $\\epsilon$ is the error term\n\nThe goal of nonlinear regression is to estimate the parameters $(\\theta)$ of the chosen nonlinear function in a way that minimizes the sum of squared differences between the predicted values and the actual observed values. This is typically done using optimization techniques, such as gradient descent or other numerical optimization algorithms.\n\n### Implementation\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Step 4: Train the model using the training data\nalpha = 10  # Regularization strength (adjust as needed)\nnum_iterations = 1000\nlearning_rate = 0.1\ndegree = 3  # Degree of the polynomial features\n\n# Create polynomial features\nX_train_poly = np.column_stack([X_train ** i for i in range(1, degree + 1)])\n\n# Train the model using polynomial features\nX_train_poly = np.column_stack((np.ones_like(X_train_poly), X_train_poly))\ntheta, history_train, cost_train = ridge_regression(X_train_poly, y_train, alpha, num_iterations, learning_rate)\nprint(f\"Training cost: {cost_train}\")\nprint(theta)\n\n# Step 5: Tune the regularization parameter using the validation data\nalphas = [0.01, 0.1, 1, 10, 100]\nmse_val = []\n\nfor alpha in alphas:\n    # Create polynomial features for validation data\n    X_val_poly = np.column_stack([X_val ** i for i in range(1, degree + 1)])\n    X_val_poly = np.column_stack((np.ones_like(X_val_poly), X_val_poly))\n    \n    theta_val, history_val, cost_val = ridge_regression(X_val_poly, y_val, alpha, num_iterations, learning_rate)\n    y_val_pred = X_val_poly @ theta_val\n    mse = np.mean((y_val - y_val_pred) ** 2)\n    mse_val.append(mse)\n\"\"\"\n    # Plot the cost history for both training and validation\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()\n\"\"\"\nbest_alpha = alphas[np.argmin(mse_val)]\nprint(f\"Best regularization parameter (alpha): {best_alpha}\")\n\n# Step 6: Evaluate the model using the testing data\n# Create polynomial features for test data\nX_test_poly = np.column_stack([X_test ** i for i in range(1, degree + 1)])\nX_test_poly = np.column_stack((np.ones_like(X_test_poly), X_test_poly))\n\ntheta_test, _, cost_test = ridge_regression(X_test_poly, y_test, best_alpha, num_iterations, learning_rate)\ny_test_pred = X_test_poly @ theta_test\nmse_test = np.mean((y_test - y_test_pred) ** 2)\nprint(f\"Mean Squared Error on Test Data: {mse_test}\")\n\n# Plot training data and the polynomial fit\nplt.scatter(X_train, y_train, color='blue', label='Training data')\nx_values = np.linspace(min(X_train), max(X_train), 100).reshape(-1, 1)\nx_values_poly = np.column_stack([x_values ** i for i in range(1, degree + 1)])\nx_values_poly = np.column_stack((np.ones_like(x_values_poly), x_values_poly))\ny_values = x_values_poly @ theta\nplt.plot(x_values, y_values, color='red', label=f'Polynomial Fit (Degree {degree})')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Training Data and Polynomial Fit')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining cost: 104.19514186498827\n[[3.51715242]\n [3.51715242]\n [3.51715242]\n [5.00411734]\n [8.66812878]\n [6.78627004]]\nBest regularization parameter (alpha): 0.01\nMean Squared Error on Test Data: 19.60464293191099\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){width=585 height=449}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n    plt.plot(range(num_iterations), history_train, label='Training Cost', color='blue')\n    plt.plot(range(num_iterations), history_val, label='Validation Cost', color='red')\n    plt.xlabel('Iteration')\n    plt.ylabel('Cost')\n    plt.title('Cost vs. Iteration')\n    plt.legend()\n    plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=593 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}