{
  "hash": "23ccc3175878300482a3dead8b49123d",
  "result": {
    "markdown": "---\ntitle: \"Clustering\"\nauthor: \"Daniel A. Udekwe\"\ndate: \"2023-10-14\"\ncategories: [news]\nimage: \"clustering.jpeg\"\n---\n\nClustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and i\n\n## **Types of clustering algorithms**\n\nThere are different types of clustering algorithms that handle all kinds of unique data.\n\n### **Density-based**\n\nIn density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points. Basically the algorithm finds the places that are dense with data points and calls those clusters.\n\nThe great thing about this is that the clusters can be any shape. You aren't constrained to expected conditions.\n\nThe clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.\n\n### **Distribution-based**\n\nWith a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to a given cluster.\n\nIt works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it being a part of that cluster decreases.\n\nIf you aren't sure of how the distribution in your data might be, you should consider a different type of algorithm.\n\n### **Centroid-based**\n\nCentroid-based clustering is the one you probably hear about the most. It's a little sensitive to the initial parameters you give it, but it's fast and efficient.\n\nThese types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a cluster based on its squared distance from the centroid. This is the most commonly used type of clustering.\n\n### **Hierarchical-based**\n\nHierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of clusters so everything is organized from the top-down.\n\nThis is more restrictive than the other clustering types, but it's perfect for specific kinds of data sets.\n\n------------------------------------------------------------------------\n\nClustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm works the best, but when you do, you'll get invaluable insight on your data. You might find connections you never would have thought of.\n\nSome real world applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems, like earthquake analysis or city planning.\n\n# Implementation\n\nWe will use the [make_classification() function](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) to create a test binary classification dataset.\n\nThe dataset will have 1,000 examples, with two input features and one cluster per class. The clusters are visually obvious in two dimensions so that we can plot the data with a scatter plot and color the points in the plot by the assigned cluster. This will help to see, at least on the test problem, how \"well\" the clusters were identified.\n\nThe clusters in this test problem are based on a multivariate Gaussian, and not all clustering algorithms will be effective at identifying these types of clusters. As such, the results in this tutorial should not be used as the basis for comparing the methods generally.\n\nAn example of creating and summarizing the synthetic clustering dataset is listed below.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# synthetic classification dataset\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# create scatter plot for samples from each class\nfor class_value in range(2):\n\t# get row indexes for samples with this class\n\trow_ix = where(y == class_value)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=569 height=411}\n:::\n:::\n\n\nRunning the example creates the synthetic clustering dataset, then creates a scatter plot of the input data with points colored by class label (idealized clusters).\n\nWe can clearly see two distinct groups of data in two dimensions and the hope would be that an automatic clustering algorithm can detect these groupings.\n\n## K-means clustering algorithm\n\nK-means clustering is the most commonly used clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm.\n\nThis algorithm tries to minimize the variance of data points within a cluster. It's also how most people are introduced to unsupervised machine learning.\n\nK-means is best used on smaller data sets because it iterates over *all* of the data points. That means it'll take more time to classify data points if there are a large amount of them in the data set.\n\nSince this is how k-means clusters data points, it doesn't scale well.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n# k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = KMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=10)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){width=569 height=411}\n:::\n:::\n\n\n### Mini-Batch K-Means\n\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# mini-batch k-means clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MiniBatchKMeans\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = MiniBatchKMeans(n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  super()._check_params_vs_input(X, default_n_init=3)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=569 height=411}\n:::\n:::\n\n\n### **DBSCAN clustering algorithm**\n\nDBSCAN stands for density-based spatial clustering of applications with noise. It's a density-based clustering algorithm, unlike k-means.\n\nThis is a good algorithm for finding outliners in a data set. It finds arbitrarily shaped clusters based on the density of data points in different regions. It separates regions by areas of low-density so that it can detect outliers between the high-density clusters.\n\nThis algorithm is better than k-means when it comes to working with oddly shaped data.\n\nDBSCAN uses two parameters to determine how clusters are defined: *minPts* (the minimum number of data points that need to be clustered together for an area to be considered high-density) and *eps* (the distance used to determine if a data point is in the same area as other data points).\n\nChoosing the right initial parameters is critical for this algorithm to work.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# dbscan clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = DBSCAN(eps=0.30, min_samples=9)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### **Agglomerative Hierarchy clustering algorithm**\n\nThis is the most common type of hierarchical clustering algorithm. It's used to group objects in clusters based on how similar they are to each other.\n\nThis is a form of bottom-up clustering, where each data point is assigned to its own cluster. Then those clusters get joined together.\n\nAt each iteration, similar clusters are merged until all of the data points are part of one big root cluster.\n\nAgglomerative clustering is best at finding small clusters. The end result looks like a dendrogram so that you can easily visualize the clusters when the algorithm finishes.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# agglomerative clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AgglomerativeClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### BIRCH\n\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# birch clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = Birch(threshold=0.01, n_clusters=2)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Affinity Propagation\n\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# affinity propagation clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = AffinityPropagation(damping=0.9)\n# fit the model\nmodel.fit(X)\n# assign a cluster to each example\nyhat = model.predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Spectral Clustering\n\nSpectral Clustering is a general class of clustering methods, drawn from [linear algebra](https://machinelearningmastery.com/linear-algebra-machine-learning-7-day-mini-course/).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# spectral clustering\nfrom numpy import unique\nfrom numpy import where\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import SpectralClustering\nfrom matplotlib import pyplot\n# define dataset\nX, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n# define the model\nmodel = SpectralClustering(n_clusters=2)\n# fit model and predict clusters\nyhat = model.fit_predict(X)\n# retrieve unique clusters\nclusters = unique(yhat)\n# create scatter plot for samples from each cluster\nfor cluster in clusters:\n\t# get row indexes for samples with this cluster\n\trow_ix = where(yhat == cluster)\n\t# create scatter of these samples\n\tpyplot.scatter(X[row_ix, 0], X[row_ix, 1])\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=569 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}